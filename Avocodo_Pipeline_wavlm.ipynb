{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline_wavlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZk8X1Cx3G3_"
      },
      "source": [
        "# 원본 코드\n",
        "\n",
        "https://github.com/ncsoft/avocodo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7waehhoSRy"
      },
      "source": [
        "# Avocodo 사전 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE6aC-JGFmw_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHeTTabtv8WJ"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlbN1YmJkwVm"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwxv46aFl6tl"
      },
      "outputs": [],
      "source": [
        "!pip install OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqwXdH8ARxSb"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "outputs": [],
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "from scipy.signal.windows import kaiser\n",
        "from omegaconf import OmegaConf\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqn-XddwDcT"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"513a1f0c050fa7f60a76b5232e904d8df397082e\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuKV0hhkiCFp"
      },
      "source": [
        "#meldataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkgki34ooeUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5): #튀는 부분 처리\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1): #작은 부분 키우기\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5): #토치버전 튀는 부분 처리\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1): #토치버전 작은 부분 키우기\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes): #토치 버전 스펙트로그램 정규화\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes): #토치버전 스펙트로그램 비정규화\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKv2Mxbvoiba"
      },
      "outputs": [],
      "source": [
        "mel_basis = {}\n",
        "hann_window = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdi0Xs8BohoR"
      },
      "outputs": [],
      "source": [
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.: #정규화 여부\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcaNp8vHokMx"
      },
      "outputs": [],
      "source": [
        "def get_dataset_filelist(\n",
        "    input_wavs_dir,\n",
        "    input_training_file,\n",
        "    input_validation_file\n",
        "):\n",
        "    with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcNWVMHiD3R"
      },
      "outputs": [],
      "source": [
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size #1개의 샘플 내에서 묶을 신호의 갯수\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split #segment_size로 자를지에 대한 여부\n",
        "        self.n_fft = n_fft #fft에서 나눌 주파수 대역의 갯수\n",
        "        self.num_mels = num_mels #멜 필터 사용 갯수\n",
        "        self.hop_size = hop_size #나눠진 각 구간에 대한\n",
        "        self.win_size = win_size #stft에서 분석할 샘플 갯수\n",
        "        self.fmin = fmin #사용할 최소 주파수\n",
        "        self.fmax = fmax #사용할 최대 주파수\n",
        "        self.fmax_loss = fmax_loss #멜 손실 최댓값\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.fine_tuning = fine_tuning #파인튜닝 여부\n",
        "        self.base_mels_path = base_mels_path# 저장된 멜 스펙트로그램 데이터경로\n",
        "\n",
        "    def __getitem__(self, index): #오디오 파일들 가져옴\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename) #[num_samples]\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning: #파인 튜닝 여부에 따라 정규화, 첫 학습인 경우 npy\n",
        "                audio = normalize(audio) * 0.95 #[num_samples]\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate: #샘플레이트 맞추기\n",
        "                raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio)#텐서화\n",
        "        audio = audio.unsqueeze(0)#[1, num_samples]\n",
        "\n",
        "        if not self.fine_tuning:\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size: #샘플 길이가 세그먼트 갯수보다 긴 경우\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                    #랜덤 시작위치에서 세그먼트 길이까지 자름,[1,segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    #짧으면 오디오에 제로 패딩 추가\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "            #num_mels랑 hop_size, win_size로 멜로 만듦, [num_mels, num_frames]\n",
        "        else:\n",
        "\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "                # [num_mels, num_frames]\n",
        "            if len(mel.shape) < 3: #3개 값 안가지면, 차원 증강\n",
        "                mel = mel.unsqueeze(0)\n",
        "                # [1, num_mels,num_frames]\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size) #프레임 갯수 계산\n",
        "                # [1, num_mels, num_frames]\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg] #[ 1,num_mels,frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                    # [1,segment_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant') #[1,num_mels,num_frames]\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    # [1,num_mels, frames_per_seg]\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "        # [1, segment_size] -> [num_mels, frames_per_seg]\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDpRRRZ8QFix"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "# # MelDataset 클래스에서 멜 스펙트로그램 생성 함수 필요\n",
        "# def generate_mel_and_save(audio_files, segment_size, n_fft, num_mels, hop_size, win_size,\n",
        "#                           sampling_rate, fmin, fmax, output_dir):\n",
        "#     for index, file_path in enumerate(audio_files):\n",
        "#         try:\n",
        "#             # 1. Load wav file\n",
        "#             audio, sampling_rate_loaded = load_wav(file_path)\n",
        "#             audio = audio / MAX_WAV_VALUE\n",
        "#             audio = normalize(audio) * 0.95  # Normalize audio\n",
        "\n",
        "#             # 2. Ensure correct sampling rate\n",
        "#             if sampling_rate != sampling_rate_loaded:\n",
        "#                 raise ValueError(f\"File {file_path} has mismatched sampling rate {sampling_rate_loaded}.\")\n",
        "\n",
        "#             # 3. Convert to PyTorch Tensor and add batch dimension\n",
        "#             audio = torch.FloatTensor(audio).unsqueeze(0)  # Shape: [1, num_samples]\n",
        "\n",
        "#             # 4. Segment audio if it's longer than the segment size\n",
        "#             if audio.size(1) > segment_size:\n",
        "#                 max_audio_start = audio.size(1) - segment_size\n",
        "#                 audio_start = random.randint(0, max_audio_start)\n",
        "#                 audio = audio[:, audio_start:audio_start + segment_size]\n",
        "#             else:\n",
        "#                 # Pad if shorter than segment size\n",
        "#                 audio = torch.nn.functional.pad(audio, (0, segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "#             # 5. Generate mel spectrogram\n",
        "#             mel = mel_spectrogram(audio, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False)\n",
        "\n",
        "#             # 6. Save mel spectrogram as .npy\n",
        "#             file_name = os.path.splitext(os.path.basename(file_path))[0]  # Get file name without extension\n",
        "#             npy_path = os.path.join(output_dir, f\"{file_name}.npy\")\n",
        "#             np.save(npy_path, mel.numpy())  # Save mel spectrogram as numpy file\n",
        "\n",
        "#             print(f\"Processed and saved: {npy_path}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "# # Example usage with configuration\n",
        "# data_config = {\n",
        "#     \"segment_size\": 8192,\n",
        "#     \"num_mels\": 80,\n",
        "#     \"n_fft\": 1024,\n",
        "#     \"hop_size\": 256,\n",
        "#     \"win_size\": 1024,\n",
        "#     \"sampling_rate\": 22050,\n",
        "#     \"fmin\": 0,\n",
        "#     \"fmax\": 8000\n",
        "# }\n",
        "\n",
        "# # File paths\n",
        "# input_wavs_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs'\n",
        "# output_mels_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram'\n",
        "\n",
        "# # Gather all wav files\n",
        "# wav_files = [os.path.join(input_wavs_dir, f) for f in os.listdir(input_wavs_dir) if f.endswith('.wav')]\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_mels_dir, exist_ok=True)\n",
        "\n",
        "# # Generate and save mel spectrograms\n",
        "# generate_mel_and_save(\n",
        "#     audio_files=wav_files,\n",
        "#     segment_size=data_config[\"segment_size\"],\n",
        "#     n_fft=data_config[\"n_fft\"],\n",
        "#     num_mels=data_config[\"num_mels\"],\n",
        "#     hop_size=data_config[\"hop_size\"],\n",
        "#     win_size=data_config[\"win_size\"],\n",
        "#     sampling_rate=data_config[\"sampling_rate\"],\n",
        "#     fmin=data_config[\"fmin\"],\n",
        "#     fmax=data_config[\"fmax\"],\n",
        "#     output_dir=output_mels_dir\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owc_vcWhiAC"
      },
      "source": [
        "# utils.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxozRiQYhj-y"
      },
      "outputs": [],
      "source": [
        "def get_padding(kernel_size, dilation=1): #제로 패딩 추가\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01): #가중치 초기화\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98DxXGMdhmT5"
      },
      "source": [
        "# losses.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN 로스"
      ],
      "metadata": {
        "id": "vYVGZsmL2EdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "c1zDjg_k147R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "sBGKhEzK17oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE MATCHING 로스"
      ],
      "metadata": {
        "id": "kl2il_762LCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ClqhniQR2PsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VpuHS2KhoVJ"
      },
      "outputs": [],
      "source": [
        "# Feature Matching Loss\n",
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    for dr, dg in zip(fmap_r, fmap_g): #각각에 대해 절댓값으로 비교\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            _loss = torch.mean(torch.abs(rl - gl)) #절댓값 평균으로 비교\n",
        "            loss += _loss\n",
        "        losses.append(_loss)\n",
        "\n",
        "    return loss*2, losses\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):# 분류자 로스\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs): #실제 값과 생성된 값을 각각 비교\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())# 각 데이터들의 로스를 추가함\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs): #생성자 로스값\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcMzi-hiajZ"
      },
      "source": [
        "# pqmf.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SINC 함수"
      ],
      "metadata": {
        "id": "YylQbwRR6N4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "F-XNbpVO6M3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAzKgcDTpCEh"
      },
      "outputs": [],
      "source": [
        "def design_prototype_filter(taps=62, cutoff_ratio=0.142, beta=9.0): #프로토타입 필터 예시\n",
        "    \"\"\"Design prototype filter for PQMF.\n",
        "    This method is based on `A Kaiser window approach for the design of prototype\n",
        "    filters of cosine modulated filterbanks`_.\n",
        "    Args:\n",
        "        taps (int): The number of filter taps.\n",
        "        cutoff_ratio (float): Cut-off frequency ratio.\n",
        "        beta (float): Beta coefficient for kaiser window.\n",
        "    Returns:\n",
        "        ndarray: Impluse response of prototype filter (taps + 1,).\n",
        "    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n",
        "        https://ieeexplore.ieee.org/abstract/document/681427\n",
        "    \"\"\"\n",
        "    # check the arguments are valid, 제약조건\n",
        "    assert taps % 2 == 0, \"The number of taps mush be even number.\" #소수 지정\n",
        "    assert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\" #컷오프 비율\n",
        "\n",
        "    # make initial filter\n",
        "    omega_c = np.pi * cutoff_ratio #라디안 단위로 차단 주파수 계산\n",
        "    with np.errstate(invalid=\"ignore\"): #차단 주파수 기준으로 sinc함수로 필터 설계\n",
        "        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n",
        "            np.pi * (np.arange(taps + 1) - 0.5 * taps)\n",
        "        )\n",
        "    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # 완전 중심값의 경우, 분모가 0이므로 에러 발생하므로 예외처리\n",
        "\n",
        "    # apply kaiser window, 양 끝의 스무딩 처리\n",
        "    w = kaiser(taps + 1, beta)# 양 옆으로 있고, 중간에 1개만 있으므로 +1, 베타를 가지는 카이저 윈도우\n",
        "    h = h_i * w # 초기 필터 값에 카이저 윈도우를 곱해서 최종 필터 생성\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MdT1otiM6MXI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVkne6sHDnXu"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kan-bayashi/ParallelWaveGAN/blob/86740373ec609cb9fb192d472d2aea125041491a/parallel_wavegan/layers/pqmf.py#L51"
      ],
      "metadata": {
        "id": "Vgw2SUoI3Ljc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gob2R_Voicxx"
      },
      "outputs": [],
      "source": [
        "class PQMF(torch.nn.Module):\n",
        "    \"\"\"PQMF module.\n",
        "    This module is based on `Near-perfect-reconstruction pseudo-QMF banks`_.\n",
        "    .. _`Near-perfect-reconstruction pseudo-QMF banks`:\n",
        "        https://ieeexplore.ieee.org/document/258122\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "        \"\"\"Initilize PQMF module.\n",
        "        The cutoff_ratio and beta parameters are optimized for #subbands = 4.\n",
        "        See dicussion in https://github.com/kan-bayashi/ParallelWaveGAN/issues/195.\n",
        "        Args:\n",
        "            subbands (int): The number of subbands.\n",
        "            taps (int): The number of filter taps.\n",
        "            cutoff_ratio (float): Cut-off frequency ratio.\n",
        "            beta (float): Beta coefficient for kaiser window.\n",
        "        \"\"\"\n",
        "        super(PQMF, self).__init__() #원본 PQMF를 기반으로 업그레이드\n",
        "\n",
        "        # build analysis & synthesis filter coefficients, 분석 필터, 합성필터 설계\n",
        "        h_proto = design_prototype_filter(taps, cutoff_ratio, beta) #필터 생성\n",
        "        h_analysis = np.zeros((subbands, len(h_proto))) #서브 밴드 대역만큼 0으로 된 배열 생성\n",
        "        h_synthesis = np.zeros((subbands, len(h_proto))) #합성도 동일한 크기의 배열 생성\n",
        "        for k in range(subbands): #각 대역별로 분석, 합성을 진행\n",
        "            h_analysis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    + (-1) ** k * np.pi / 4 #여기만 다름\n",
        "                )\n",
        "            )\n",
        "            h_synthesis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    - (-1) ** k * np.pi / 4 #여기만 다름\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # convert to tensor\n",
        "        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1) #텐서로 변경\n",
        "        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n",
        "\n",
        "        # register coefficients as beffer\n",
        "        self.register_buffer(\"analysis_filter\", analysis_filter) #모델 버퍼로 등록해서, 학습중에 업데이트 안되게 서렂ㅇ\n",
        "        self.register_buffer(\"synthesis_filter\", synthesis_filter)\n",
        "\n",
        "        # filter for downsampling & upsampling\n",
        "        updown_filter = torch.zeros((subbands, subbands, subbands)).float() #필터생성\n",
        "        for k in range(subbands):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.subbands = subbands\n",
        "\n",
        "        # keep padding info\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0) #패딩 함수 설정, 신호 길이 유지\n",
        "\n",
        "    def analysis(self, x): #다운 샘플링\n",
        "        \"\"\"Analysis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, 1, T).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, subbands, T // subbands).\n",
        "        \"\"\"\n",
        "        x = F.conv1d(self.pad_fn(x), self.analysis_filter) #패딩 추가, [batch_size, subbands, t]\n",
        "        return F.conv1d(x, self.updown_filter, stride=self.subbands)#(batch_size, sub_bands,t//sub_bands)\n",
        "        #self.subbands를 사용해 서브밴드 수만큼 나눠서 다운샘플링\n",
        "\n",
        "    def synthesis(self, x): #업 샘플링\n",
        "        \"\"\"Synthesis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, subbands, T // subbands).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, 1, T).\n",
        "        \"\"\"\n",
        "        # NOTE(kan-bayashi): Power will be dreased so here multipy by # subbands.\n",
        "        #   Not sure this is the correct way, it is better to check again.\n",
        "        # TODO(kan-bayashi): Understand the reconstruction procedure\n",
        "        x = F.conv_transpose1d( #업샘플링, 서브밴드 크기만큼 업샘플링\n",
        "            x, self.updown_filter * self.subbands, stride=self.subbands\n",
        "        )#[batch_size, subbands, t//subbands]\n",
        "        return F.conv1d(self.pad_fn(x), self.synthesis_filter)#[batch_size, 1, t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQ3Q0mqht0K"
      },
      "source": [
        "# CoMBD.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgOcC75wouAx"
      },
      "outputs": [],
      "source": [
        "class CoMBDBlock(torch.nn.Module):\n",
        "    def __init__( #CoMBD 블록 선언\n",
        "        self,\n",
        "        h_u: List[int], #각 계층별 출력 갯수(히든 유닛) 리스트\n",
        "        d_k: List[int], #각 계층의 커널 크기 리스트\n",
        "        d_s: List[int], #각 계층의 스트라이드 리스트\n",
        "        d_d: List[int], #각 계층의 dilation(확장 계수) 리스트\n",
        "        d_g: List[int], #각 계층의 그룹 수 리스트\n",
        "        d_p: List[int], #각 계층의 패딩 크기 리스트\n",
        "        op_f: int, #프로젝션 계층의 출력 채널 크기\n",
        "        op_k: int, #프로젝션 계층의 커널 크기\n",
        "        op_g: int, #프로젝션 계층의 그룹 수\n",
        "        use_spectral_norm=False #spectral normalization 적용여부\n",
        "    ):\n",
        "        super(CoMBDBlock, self).__init__() #combd 내용 기반으로 업그레이드\n",
        "        norm_f = weight_norm if use_spectral_norm is False else spectral_norm #스펙트럼 정규화, 가중치 정규화 선택\n",
        "\n",
        "        self.convs = nn.ModuleList() #다층 컨볼루션 모델 생성\n",
        "        filters = [[1, h_u[0]]] #출력 필터 지정\n",
        "        for i in range(len(h_u) - 1): #각 계층마다 필터 지정\n",
        "            filters.append([h_u[i], h_u[i + 1]]) #각 필터 시작지점, 끝지점으로 묶기\n",
        "        for _f, _k, _s, _d, _g, _p in zip(filters, d_k, d_s, d_d, d_g, d_p):#각 대역폭별로 사용될 데이터 묶기\n",
        "            self.convs.append(norm_f(\n",
        "                Conv1d( #가중치들을 활용해 conv1d 계층 생성\n",
        "                    in_channels=_f[0],\n",
        "                    out_channels=_f[1],\n",
        "                    kernel_size=_k,\n",
        "                    stride=_s,\n",
        "                    dilation=_d,\n",
        "                    groups=_g,\n",
        "                    padding=_p\n",
        "                )\n",
        "            ))\n",
        "        self.projection_conv = norm_f( #제일 마지막 프로젝션 레이어\n",
        "            Conv1d(\n",
        "                in_channels=filters[-1][1],\n",
        "                out_channels=op_f,\n",
        "                kernel_size=op_k,\n",
        "                groups=op_g\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for block in self.convs:\n",
        "            x = block(x) #블록\n",
        "            x = F.leaky_relu(x, 0.2) #활성화 함수, 비선형성 추가\n",
        "            fmap.append(x) #묶어서 추가, 각 계층의 출력에 대해 추가\n",
        "        x = self.projection_conv(x) #최종 출력 변환\n",
        "        return x, fmap    #CoMBD 블록 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-ga7kQJhs-0"
      },
      "outputs": [],
      "source": [
        "class CoMBD(torch.nn.Module): #내부 combd\n",
        "    def __init__(self, h, pqmf_list=None, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        self.h = h #하이퍼 파라미터 설정 객체, h는 combd, combd_block에 대한 구조 정의\n",
        "        if pqmf_list is not None: #대역 분할 있으면, 지정\n",
        "            self.pqmf = pqmf_list\n",
        "        else: #대역 분할이 없으면 LV1, LV2로 분할\n",
        "            self.pqmf = [\n",
        "                PQMF(*h.pqmf_config[\"lv2\"]),\n",
        "                PQMF(*h.pqmf_config[\"lv1\"])\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.ModuleList() #CoMBD 블럭의 데이터 가져옴\n",
        "        for _h_u, _d_k, _d_s, _d_d, _d_g, _d_p, _op_f, _op_k, _op_g in zip(\n",
        "            h.combd_h_u,\n",
        "            h.combd_d_k,\n",
        "            h.combd_d_s,\n",
        "            h.combd_d_d,\n",
        "            h.combd_d_g,\n",
        "            h.combd_d_p,\n",
        "            h.combd_op_f,\n",
        "            h.combd_op_k,\n",
        "            h.combd_op_g,\n",
        "        ):\n",
        "            self.blocks.append(CoMBDBlock( #COMBD 블럭들 쌓기\n",
        "                _h_u,\n",
        "                _d_k,\n",
        "                _d_s,\n",
        "                _d_d,\n",
        "                _d_g,\n",
        "                _d_p,\n",
        "                _op_f,\n",
        "                _op_k,\n",
        "                _op_g,\n",
        "            ))\n",
        "\n",
        "    def _block_forward(self, input, blocks, outs, f_maps): #순차적 통과, 중간 표현 확인\n",
        "        for x, block in zip(input, blocks):\n",
        "            out, f_map = block(x) #각 계층마다의 중간맵 출력\n",
        "            outs.append(out)\n",
        "            f_maps.append(f_map)\n",
        "        return outs, f_maps             # 계층마다 intermidiate feature map 출력할 수 있도록\n",
        "\n",
        "    def _pqmf_forward(self, ys, ys_hat):\n",
        "        # ys는 실제 신호 리스트 - training_step 파트에서 만들어짐\n",
        "        # ys_hat은 생성된 신호 리스트\n",
        "        #  ys = [\n",
        "        #self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],  # PQMF Level 2\n",
        "        #self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],  # PQMF Level 1\n",
        "        #y  ] # 원래 입력 신호\n",
        "        multi_scale_inputs = [] #PQMF를 통한 신호의 대역폭 분할, analysis 함수 이용\n",
        "        multi_scale_inputs_hat = []\n",
        "        for pqmf in self.pqmf: #lv1, lv2가 들어가있는 pqmf 객체 리스트에 다운샘플링, 대역별 분할\n",
        "            multi_scale_inputs.append(  #analysis 함수는 일종의 다운샘플링\n",
        "                pqmf.to(ys[-1]).analysis(ys[-1])[:, :1, :] #batch_size, subband,t\n",
        "            )\n",
        "            multi_scale_inputs_hat.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys_hat[-1])[:, :1, :]\n",
        "            )\n",
        "\n",
        "        outs_real = []\n",
        "        f_maps_real = []\n",
        "        # real\n",
        "        # for hierarchical forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            ys, self.blocks, outs_real, f_maps_real)                # input : ys\n",
        "        # for multi_scale forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            multi_scale_inputs, self.blocks[:-1], outs_real, f_maps_real)\n",
        "        # outs_real: 실제 음성의 출력\n",
        "\t    # f_maps_real: 실제 음성의 중간 특징 맵\n",
        "\n",
        "\n",
        "        outs_fake = [] #생성 신호 처리\n",
        "        f_maps_fake = []\n",
        "        # predicted\n",
        "        # for hierarchical forward\n",
        "        outs_fake, f_maps_fake = self._block_forward( #실제 신호, 생성 신호에 대한 중간 특징맵 파악\n",
        "            ys_hat, self.blocks, outs_fake, f_maps_fake)     # input : ys_hat\n",
        "        # for multi_scale forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            multi_scale_inputs_hat, self.blocks[:-1], outs_fake, f_maps_fake)\n",
        "        # outs_fake: 생성 음성의 출력\n",
        "\t    # f_maps_fake: 생성 음성의 중간 특징 맵\n",
        "\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake\n",
        "\n",
        "    def forward(self, ys, ys_hat): #실제 신호, 생성신호에 대해 combd 모델 수행\n",
        "        outs_real, outs_fake, f_maps_real, f_maps_fake = self._pqmf_forward(\n",
        "            ys, ys_hat)\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeFVq4_hzq6"
      },
      "source": [
        "# SBD.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBMUu1GRozy6"
      },
      "outputs": [],
      "source": [
        "class MDC(torch.nn.Module): #multi dilated convolution 계층\n",
        "    # 신호의 다중 스케일(해상도)에 대한 특징 추출\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm #정규화 방식 설정\n",
        "        self.d_convs = nn.ModuleList()\n",
        "        for _k, _d in zip(kernel_size, dilations): #팽창 컨볼루션 계층\n",
        "            self.d_convs.append(\n",
        "                norm_f(Conv1d( #원본보다 넓은 범위의 정보 학습\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=_k,\n",
        "                    dilation=_d,\n",
        "                    padding=get_padding(_k, _d)\n",
        "                ))\n",
        "            )\n",
        "        self.post_conv = norm_f(Conv1d( #후처리 컨볼루션\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=strides,\n",
        "            padding=get_padding(_k, _d)\n",
        "        ))\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x): #컨볼루션 적용 [batch_size, channel, time_step ]\n",
        "        _out = None\n",
        "        for _l in self.d_convs:\n",
        "            _x = torch.unsqueeze(_l(x), -1) #팽창 컨볼루션\n",
        "            _x = F.leaky_relu(_x, 0.2)\n",
        "            if _out is None: #결과 병합\n",
        "                _out = _x\n",
        "            else:\n",
        "                _out = torch.cat([_out, _x], axis=-1) #출력 통합\n",
        "        x = torch.sum(_out, dim=-1)\n",
        "        x = self.post_conv(x)\n",
        "        x = F.leaky_relu(x, 0.2)  # @@\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2OaM69uh0_5"
      },
      "outputs": [],
      "source": [
        "class SBDBlock(torch.nn.Module):        # 각 sub-band 신호를 MDC 계층을 통해 분석함\n",
        "    def __init__(\n",
        "        self,\n",
        "        segment_dim, #입력 채널 수\n",
        "        strides,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(SBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters_in_out = [(segment_dim, filters[0])] #필터 입력, 출력 채널 정의\n",
        "        for i in range(len(filters) - 1):\n",
        "            filters_in_out.append([filters[i], filters[i + 1]])\n",
        "\n",
        "        for _s, _f, _k, _d in zip( #mdc 계층\n",
        "            strides,\n",
        "            filters_in_out,\n",
        "            kernel_size,\n",
        "            dilations\n",
        "        ):\n",
        "            self.convs.append(MDC(\n",
        "                in_channels=_f[0],\n",
        "                out_channels=_f[1],\n",
        "                strides=_s,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "        self.post_conv = norm_f(Conv1d( #후처리 컨볼루션\n",
        "            in_channels=_f[1],\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=3 // 2\n",
        "        ))  # @@\n",
        "\n",
        "    def forward(self, x): #mdc 계층 통과\n",
        "        fmap = []\n",
        "        for _l in self.convs:\n",
        "            x = _l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.post_conv(x)  # @@\n",
        "\n",
        "        return x, fmap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuGqG0y4o4oB"
      },
      "outputs": [],
      "source": [
        "class MDCDConfig: #설정\n",
        "    def __init__(self, h):\n",
        "        self.pqmf_params = h.pqmf_config[\"sbd\"]\n",
        "        self.f_pqmf_params = h.pqmf_config[\"fsbd\"]\n",
        "        self.filters = h.sbd_filters\n",
        "        self.kernel_sizes = h.sbd_kernel_sizes\n",
        "        self.dilations = h.sbd_dilations\n",
        "        self.strides = h.sbd_strides\n",
        "        self.band_ranges = h.sbd_band_ranges\n",
        "        self.transpose = h.sbd_transpose\n",
        "        self.segment_size = h.segment_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H-kRPzzo3dp"
      },
      "outputs": [],
      "source": [
        "class SBD(torch.nn.Module):\n",
        "    def __init__(self, h, use_spectral_norm=False):\n",
        "        super(SBD, self).__init__()\n",
        "        self.config = MDCDConfig(h)\n",
        "        self.pqmf = PQMF(\n",
        "            *self.config.pqmf_params\n",
        "        )\n",
        "        if True in h.sbd_transpose:\n",
        "            self.f_pqmf = PQMF(\n",
        "                *self.config.f_pqmf_params\n",
        "            )\n",
        "        else:\n",
        "            self.f_pqmf = None\n",
        "\n",
        "        self.discriminators = torch.nn.ModuleList() #discriminator 계층 생성\n",
        "\n",
        "        for _f, _k, _d, _s, _br, _tr in zip(\n",
        "            self.config.filters,\n",
        "            self.config.kernel_sizes,\n",
        "            self.config.dilations,\n",
        "            self.config.strides,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if _tr:\n",
        "                segment_dim = self.config.segment_size // _br[1] - _br[0]\n",
        "            else:\n",
        "                segment_dim = _br[1] - _br[0]\n",
        "\n",
        "            self.discriminators.append(SBDBlock(\n",
        "                segment_dim=segment_dim,\n",
        "                filters=_f,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                strides=_s,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        y_in = self.pqmf.analysis(y)        # y를 PQMF를 이용해서 sub-band로 분할\n",
        "        y_hat_in = self.pqmf.analysis(y_hat)  # y_hat을 PQMF를 이용해서 sub-band로 분할\n",
        "        if self.f_pqmf is not None: #실제 신호, 생성 신호를 통해 pqmf로 서브밴드로 분할\n",
        "            y_in_f = self.f_pqmf.analysis(y)\n",
        "            y_hat_in_f = self.f_pqmf.analysis(y_hat)\n",
        "\n",
        "        for d, br, tr in zip( #각 서브밴드에 대해 sbd 적용\n",
        "            self.discriminators,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if tr:          # transpose 혹은 일반적으로 처리 할지 결정\n",
        "                _y_in = y_in_f[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in_f[:, br[0]:br[1], :]\n",
        "                _y_in = torch.transpose(_y_in, 1, 2)\n",
        "                _y_hat_in = torch.transpose(_y_hat_in, 1, 2)\n",
        "            else:\n",
        "                _y_in = y_in[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in[:, br[0]:br[1], :]\n",
        "            y_d_r, fmap_r = d(_y_in)\n",
        "            y_d_g, fmap_g = d(_y_hat_in)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs #실제 신호와 생성 신호에 대한 판별, 중간 신호에 대한 특징맵"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttjqPrPh4sC"
      },
      "source": [
        "# generator.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHcgmHJEo90g"
      },
      "outputs": [],
      "source": [
        "class ResBlock(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([ #첫번째 컨볼루션 세트, dilation 크기를 늘림, 1,3,5로 됨\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights) #정규화\n",
        "\n",
        "        self.convs2 = nn.ModuleList([ #두번째 컨볼루션 세트, dilation 크기는 모두 동일\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x): #각 컨볼루션, 활성화함수 적용\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.2)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, 0.2)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self): #학습 이후에 가중치 정규화 제거\n",
        "        for _l in self.convs1:\n",
        "            remove_weight_norm(_l)\n",
        "        for _l in self.convs2:\n",
        "            remove_weight_norm(_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUBqwfZgh3dx"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module): #생성자 클래스\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.resblock = h.resblock\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock\n",
        "\n",
        "        self.ups = nn.ModuleList() #업샘플링 계층\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            _ups = nn.ModuleList()\n",
        "            for _i, (_u, _k) in enumerate(zip(u, k)):\n",
        "                in_channel = h.upsample_initial_channel // (2**i)\n",
        "                out_channel = h.upsample_initial_channel // (2**(i + 1))\n",
        "                _ups.append(weight_norm( #업샘플링으로 데이터 확대, 각 레이어별 dilation 확대로 넓은 범위의 특징 파악\n",
        "                    ConvTranspose1d(in_channel, out_channel, _k, _u, padding=(_k - _u) // 2)))\n",
        "            self.ups.append(_ups)\n",
        "\n",
        "        self.resblocks = nn.ModuleList() #잔차 레이어\n",
        "        self.conv_post = nn.ModuleList() #후처리 레이어\n",
        "        for i in range(self.num_upsamples): #최종 출력 채널로 변환\n",
        "            ch = h.upsample_initial_channel // (2**(i + 1))\n",
        "            temp = nn.ModuleList()\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                temp.append(resblock(h, ch, k, d))\n",
        "            self.resblocks.append(temp)\n",
        "\n",
        "            if self.h.projection_filters[i] != 0:\n",
        "                self.conv_post.append(\n",
        "                    weight_norm(\n",
        "                        Conv1d(\n",
        "                            ch, self.h.projection_filters[i],\n",
        "                            self.h.projection_kernels[i], 1, padding=self.h.projection_kernels[i] // 2\n",
        "                        )))\n",
        "            else:\n",
        "                self.conv_post.append(torch.nn.Identity())\n",
        "\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.conv_pre(x)\n",
        "        for i, (ups, resblocks, conv_post) in enumerate(zip(self.ups, self.resblocks, self.conv_post)):\n",
        "            x = F.leaky_relu(x, 0.2) #업샘플링과 잔차블럭 적용\n",
        "            for _ups in ups:\n",
        "                x = _ups(x)\n",
        "            xs = None\n",
        "            for j, resblock in enumerate(resblocks):\n",
        "                if xs is None:\n",
        "                    xs = resblock(x)\n",
        "                else:\n",
        "                    xs += resblock(x)\n",
        "            x = xs / self.num_kernels\n",
        "            if i >= (self.num_upsamples-3): #업샘플링 단계 마지막 3개 출력을 tanh 활성화 함수로 변환\n",
        "                _x = F.leaky_relu(x)\n",
        "                _x = conv_post(_x)\n",
        "                _x = torch.tanh(_x)\n",
        "                outs.append(_x)\n",
        "            else:\n",
        "                x = conv_post(x)\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def remove_weight_norm(self): #가중치 정규화 제거, 추론 속도 최적화\n",
        "        print('Removing weight norm...')\n",
        "        for ups in self.ups:\n",
        "            for _l in ups:\n",
        "                remove_weight_norm(_l)\n",
        "        for resblock in self.resblocks:\n",
        "            for _l in resblock:\n",
        "                _l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        for _l in self.conv_post:\n",
        "            if not isinstance(_l, torch.nn.Identity):\n",
        "                remove_weight_norm(_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyxi8fxiHbD"
      },
      "source": [
        "# data_module.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RVIsixRpGYI"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AvocodoDataConfig:\n",
        "    segment_size: int\n",
        "    num_mels: int\n",
        "    num_freq: int\n",
        "    sampling_rate: int\n",
        "    n_fft: int\n",
        "    hop_size: int\n",
        "    win_size: int\n",
        "    fmin: int\n",
        "    fmax: int\n",
        "    batch_size: int\n",
        "    num_workers: int\n",
        "\n",
        "    fine_tuning: bool\n",
        "    base_mels_path: str\n",
        "\n",
        "    input_wavs_dir: str\n",
        "    input_mels_dir: str\n",
        "    input_training_file: str\n",
        "    input_validation_file: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3ikwJZniJKo"
      },
      "outputs": [],
      "source": [
        "class AvocodoData(LightningDataModule):\n",
        "    def __init__(self, h: AvocodoDataConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        '''\n",
        "            download and prepare data\n",
        "        '''\n",
        "        self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "            self.hparams.input_wavs_dir,\n",
        "            self.hparams.input_training_file,\n",
        "            self.hparams.input_validation_file\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.trainset = MelDataset(\n",
        "            self.training_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(\n",
        "            self.trainset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "    @rank_zero_only\n",
        "    def val_dataloader(self):\n",
        "        validset = MelDataset(\n",
        "            self.validation_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            False,\n",
        "            False,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(validset, num_workers=max_workers, shuffle=False,\n",
        "                          sampler=None,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTk_rGDiPHB"
      },
      "source": [
        "# lightning_module.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK6AJpMZxaOC"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# WandB 프로젝트 초기화\n",
        "wandb_logger = WandbLogger(project=\"avocodo_train_wgangp\", name=\"avocodo_training_wgangp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsaB8xZGiOeh"
      },
      "outputs": [],
      "source": [
        "# # 원본\n",
        "# class Avocodo(LightningModule):\n",
        "#     def __init__(self, h):\n",
        "#         super().__init__()\n",
        "#         self.save_hyperparameters(h)\n",
        "\n",
        "#         # Model components, pqmf lv2 고해상도, lv1 저해상도 필터\n",
        "#         self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"]) # full-resolution waveform으ㄹ downsample하는 low-pass filter로 사용\n",
        "#         self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "#         self.generator = Generator(self.hparams.generator) #생성자\n",
        "#         self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])      # Collaborative Multi-Band Discriminator\n",
        "#         #다중 대역에서 생성된 신호와 실제 신호 비교\n",
        "#         self.sbd = SBD(self.hparams.sbd)\n",
        "#         # PQMF analysis를 통해 얻은 sub-band signal을 Discriminate\n",
        "\n",
        "#         # Validation outputs storage, 자동 최적화 비활성화\n",
        "#         self.validation_outputs = []\n",
        "\n",
        "#         # Manual optimization\n",
        "#         self.automatic_optimization = False\n",
        "\n",
        "#     def configure_optimizers(self): #생성자, 분별자에 대해 각각에 대한 adamw 최적화 생성\n",
        "#         h = self.hparams.optimizer\n",
        "#         opt_g = torch.optim.AdamW(\n",
        "#             self.generator.parameters(),\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         opt_d = torch.optim.AdamW(\n",
        "#             itertools.chain(self.combd.parameters(), self.sbd.parameters()),            # CoMBD와 SBD 파라미터 업데이트\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         return [opt_g, opt_d]\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         return self.generator(z)[-1]\n",
        "\n",
        "#     def training_step(self, batch, batch_idx): #학습\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y = y.unsqueeze(1) #다중 대역 신호 리스트 생성\n",
        "#         ys = [                                                                                                                      # pqmf로 다운샘플링 된 waveform 리스트\n",
        "#             self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "#             self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "#             y\n",
        "#         ]\n",
        "#         y_g_hats = self.generator(x)                ## Generator가 생성한 waveform\n",
        "\n",
        "#         # Get optimizers\n",
        "#         opt_g, opt_d = self.optimizers()\n",
        "\n",
        "#         # Train Generator\n",
        "#         opt_g.zero_grad() #가중치 초기화\n",
        "#         #feature matching loss 계산\n",
        "#         y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "#         loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)         # 실제(real) 중간 특징 맵과 생성(generate) 중간 특징 맵간의 loss 계산\n",
        "#         loss_gen_u, _ = generator_loss(y_du_hat_g)                  # 생성 된 음성 loss 계산\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "#         loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "#         loss_gen_s, _ = generator_loss(y_ds_hat_g)\n",
        "\n",
        "#         # L1 Mel-Spectrogram Loss\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hats[-1].squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "#         self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "#         loss_mel = loss_mel * self.hparams.loss_scale_mel # lambda 값\n",
        "#         # 생성자 최적화\n",
        "#         g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel\n",
        "\n",
        "#         self.manual_backward(g_loss)\n",
        "#         opt_g.step()\n",
        "#         self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         # Train Discriminator\n",
        "#         opt_d.zero_grad()\n",
        "#         detached_y_g_hats = [x.detach() for x in y_g_hats]      # generator의 출력 데이터를 분리 -> 그래디언트가 전파되지 않도록(generator의 학습에 영향을 주지 않도록)\n",
        "\n",
        "#         #손실 계산 및 최적화 수행\n",
        "#         y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "#         loss_disc_u, _, _ = discriminator_loss(y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "#         loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "#         d_loss = loss_disc_s + loss_disc_u\n",
        "#         self.manual_backward(d_loss)\n",
        "#         opt_d.step()\n",
        "#         self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y_g_hat = self(x)\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hat.squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         val_loss = F.l1_loss(y_mel, y_g_hat_mel) #l1 손실 계산\n",
        "#         self.validation_outputs.append(val_loss)\n",
        "\n",
        "#         # Log validation loss\n",
        "#         self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return val_loss\n",
        "\n",
        "#     def on_validation_epoch_end(self): #각 에폭당 평균 검증 손실 계산\n",
        "#         if self.validation_outputs:\n",
        "#             avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "#             self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "#         self.validation_outputs.clear()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wavlm"
      ],
      "metadata": {
        "id": "6U86vk23hlH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransposeLast(nn.Module):\n",
        "    def __init__(self, deconstruct_idx=None):\n",
        "        super().__init__()\n",
        "        self.deconstruct_idx = deconstruct_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deconstruct_idx is not None:\n",
        "            x = x[self.deconstruct_idx]\n",
        "        return x.transpose(-2, -1)#마지막 차원 두개 바꿈\n",
        "\n",
        "\n",
        "class Fp32LayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.layer_norm(\n",
        "            input.float(),\n",
        "            self.normalized_shape,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None #동일 형태\n",
        "\n",
        "\n",
        "class SamePad(nn.Module): #패딩 추가 또는 빼기\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x): # [batch_size, channel, time]\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x #[batch_size, channel, time-padding]\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x) # 동일 차원\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module): #레이어 정규화\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x #동일 차원\n",
        "\n",
        "\n",
        "def gelu_accurate(x): #계산식\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor: #계산식\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size): #양자화 노이즈 추가\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training: #학습하는 경우\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else: #체크하는 경우\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim, #임베딩 차원\n",
        "            num_heads, #MULTIHEAD ATTENTION 헤드 수\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise( #노이즈 양자화\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): #파라미터 리셋\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional: #blstm\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length): #바이에스 계산\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None] #query_length-1까지\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :] #key_length-1까지\n",
        "        relative_position = memory_position - context_position #길이 차이 만큼의 상대적 위치 계산 5-3=2 식으로\n",
        "        relative_position_bucket = self._relative_positions_bucket( #범주화\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device) #상대적 위치 편향값\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1]) # [num_heads, query_length, key_length]\n",
        "        return values\n",
        "\n",
        "    def forward( #[time, batch, channel]\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights: #가중치 필요한 경우 확인\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size() #입력 데이터 분리\n",
        "        src_len = tgt_len #key 길이 설정\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None: #상대적 위치 편향 계산\n",
        "            position_bias = self.compute_bias(tgt_len, src_len) #원본과 타겟의 상대적 위치차이 계산\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "            #batch*heads, target_length, source_length\n",
        "\n",
        "        if (\n",
        "                not is_tpu  # don't use PyTorch version on TPUs\n",
        "                and incremental_state is None\n",
        "                and not static_kv\n",
        "                # A workaround for quantization to work. Otherwise JIT compilation\n",
        "                # treats bias in linear module as method.\n",
        "                and not torch.jit.is_scripting()\n",
        "                and self.q_head_dim == self.head_dim\n",
        "        ):\n",
        "            assert key is not None and value is not None\n",
        "            assert attn_mask is None\n",
        "\n",
        "            attn_mask_rel_pos = None\n",
        "            if position_bias is not None:\n",
        "                attn_mask_rel_pos = position_bias\n",
        "                if self.gru_rel_pos:\n",
        "                    query_layer = query.transpose(0, 1)\n",
        "                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n",
        "                    query_layer = query_layer.view(*new_x_shape)\n",
        "                    query_layer = query_layer.permute(0, 2, 1, 3)\n",
        "                    _B, _H, _L, __ = query_layer.size()\n",
        "\n",
        "                    gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                        _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n",
        "            k_proj_bias = self.k_proj.bias\n",
        "            if k_proj_bias is None:\n",
        "                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n",
        "\n",
        "            x, attn = F.multi_head_attention_forward( #멀티헤드 어텐션 계산\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                torch.empty([0]),\n",
        "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout_module.p,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                self.training,\n",
        "                # self.training or self.dropout_module.apply_during_inference,\n",
        "                key_padding_mask,\n",
        "                need_weights,\n",
        "                attn_mask_rel_pos,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj.weight,\n",
        "                k_proj_weight=self.k_proj.weight,\n",
        "                v_proj_weight=self.v_proj.weight,\n",
        "            )\n",
        "            return x, attn, position_bias\n",
        "\n",
        "        if incremental_state is not None: #증분상태 결합\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention: #프로젝션 계산, q,k,v 생성\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None: #저장 상태\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0: #패딩 추가의 경우\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "            position_bias = position_bias.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + position_bias\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights"
      ],
      "metadata": {
        "id": "wv17sSdMlyUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import logging\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm\n",
        "# from modules import (\n",
        "#     Fp32GroupNorm,\n",
        "#     Fp32LayerNorm,\n",
        "#     GradMultiply,\n",
        "#     MultiheadAttention,\n",
        "#     SamePad,\n",
        "#     init_bert_params,\n",
        "#     get_activation_fn,\n",
        "#     TransposeLast,\n",
        "#     GLU_Linear,\n",
        "# )\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def compute_mask_indices(\n",
        "    shape: Tuple[int, int],\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    mask_prob: float,\n",
        "    mask_length: int,\n",
        "    mask_type: str = \"static\",\n",
        "    mask_other: float = 0.0,\n",
        "    min_masks: int = 0,\n",
        "    no_overlap: bool = False,\n",
        "    min_space: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes random mask spans for a given shape\n",
        "\n",
        "    Args:\n",
        "        shape: the the shape for which to compute masks.\n",
        "            should be of size 2 where first element is batch size and 2nd is timesteps\n",
        "        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements\n",
        "        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n",
        "            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n",
        "            however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n",
        "        mask_type: how to compute mask lengths\n",
        "            static = fixed size\n",
        "            uniform = sample from uniform distribution [mask_other, mask_length*2]\n",
        "            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element\n",
        "            poisson = sample from possion distribution with lambda = mask length\n",
        "        min_masks: minimum number of masked spans\n",
        "        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping\n",
        "        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans\n",
        "    \"\"\"\n",
        "\n",
        "    bsz, all_sz = shape\n",
        "    mask = np.full((bsz, all_sz), False)\n",
        "\n",
        "    all_num_mask = int(\n",
        "        # add a random number for probabilistic rounding\n",
        "        mask_prob * all_sz / float(mask_length)\n",
        "        + np.random.rand()\n",
        "    )\n",
        "\n",
        "    all_num_mask = max(min_masks, all_num_mask)\n",
        "\n",
        "    mask_idcs = []\n",
        "    for i in range(bsz):\n",
        "        if padding_mask is not None:\n",
        "            sz = all_sz - padding_mask[i].long().sum().item()\n",
        "            num_mask = int(\n",
        "                # add a random number for probabilistic rounding\n",
        "                mask_prob * sz / float(mask_length)\n",
        "                + np.random.rand()\n",
        "            )\n",
        "            num_mask = max(min_masks, num_mask)\n",
        "        else:\n",
        "            sz = all_sz\n",
        "            num_mask = all_num_mask\n",
        "\n",
        "        if mask_type == \"static\":\n",
        "            lengths = np.full(num_mask, mask_length)\n",
        "        elif mask_type == \"uniform\":\n",
        "            lengths = np.random.randint(mask_other, mask_length * 2 + 1, size=num_mask)\n",
        "        elif mask_type == \"normal\":\n",
        "            lengths = np.random.normal(mask_length, mask_other, size=num_mask)\n",
        "            lengths = [max(1, int(round(x))) for x in lengths]\n",
        "        elif mask_type == \"poisson\":\n",
        "            lengths = np.random.poisson(mask_length, size=num_mask)\n",
        "            lengths = [int(round(x)) for x in lengths]\n",
        "        else:\n",
        "            raise Exception(\"unknown mask selection \" + mask_type)\n",
        "\n",
        "        if sum(lengths) == 0:\n",
        "            lengths[0] = min(mask_length, sz - 1)\n",
        "\n",
        "        if no_overlap:\n",
        "            mask_idc = []\n",
        "\n",
        "            def arrange(s, e, length, keep_length):\n",
        "                span_start = np.random.randint(s, e - length)\n",
        "                mask_idc.extend(span_start + i for i in range(length))\n",
        "\n",
        "                new_parts = []\n",
        "                if span_start - s - min_space >= keep_length:\n",
        "                    new_parts.append((s, span_start - min_space + 1))\n",
        "                if e - span_start - keep_length - min_space > keep_length:\n",
        "                    new_parts.append((span_start + length + min_space, e))\n",
        "                return new_parts\n",
        "\n",
        "            parts = [(0, sz)]\n",
        "            min_length = min(lengths)\n",
        "            for length in sorted(lengths, reverse=True):\n",
        "                lens = np.fromiter(\n",
        "                    (e - s if e - s >= length + min_space else 0 for s, e in parts),\n",
        "                    np.int,\n",
        "                )\n",
        "                l_sum = np.sum(lens)\n",
        "                if l_sum == 0:\n",
        "                    break\n",
        "                probs = lens / np.sum(lens)\n",
        "                c = np.random.choice(len(parts), p=probs)\n",
        "                s, e = parts.pop(c)\n",
        "                parts.extend(arrange(s, e, length, min_length))\n",
        "            mask_idc = np.asarray(mask_idc)\n",
        "        else:\n",
        "            min_len = min(lengths)\n",
        "            if sz - min_len <= num_mask:\n",
        "                min_len = sz - num_mask - 1\n",
        "\n",
        "            mask_idc = np.random.choice(sz - min_len, num_mask, replace=False)\n",
        "\n",
        "            mask_idc = np.asarray(\n",
        "                [\n",
        "                    mask_idc[j] + offset\n",
        "                    for j in range(len(mask_idc))\n",
        "                    for offset in range(lengths[j])\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        mask_idcs.append(np.unique(mask_idc[mask_idc < sz]))\n",
        "\n",
        "    min_len = min([len(m) for m in mask_idcs])\n",
        "    for i, mask_idc in enumerate(mask_idcs):\n",
        "        if len(mask_idc) > min_len:\n",
        "            mask_idc = np.random.choice(mask_idc, min_len, replace=False)\n",
        "        mask[i, mask_idc] = True\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "class WavLMConfig:\n",
        "    def __init__(self, cfg=None):\n",
        "        self.extractor_mode: str = \"default\"     # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n",
        "        self.encoder_layers: int = 12     # num encoder layers in the transformer\n",
        "\n",
        "        self.encoder_embed_dim: int = 768     # encoder embedding dimension\n",
        "        self.encoder_ffn_embed_dim: int = 3072     # encoder embedding dimension for FFN\n",
        "        self.encoder_attention_heads: int = 12     # num encoder attention heads\n",
        "        self.activation_fn: str = \"gelu\"     # activation function to use\n",
        "\n",
        "        self.layer_norm_first: bool = False     # apply layernorm first in the transformer\n",
        "        self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"     # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]\n",
        "        self.conv_bias: bool = False     # include bias in conv encoder\n",
        "        self.feature_grad_mult: float = 1.0     # multiply feature extractor var grads by this\n",
        "\n",
        "        self.normalize: bool = False  # normalize input to have 0 mean and unit variance during training\n",
        "\n",
        "        # dropouts\n",
        "        self.dropout: float = 0.1     # dropout probability for the transformer\n",
        "        self.attention_dropout: float = 0.1     # dropout probability for attention weights\n",
        "        self.activation_dropout: float = 0.0     # dropout probability after activation in FFN\n",
        "        self.encoder_layerdrop: float = 0.0     # probability of dropping a tarnsformer layer\n",
        "        self.dropout_input: float = 0.0     # dropout to apply to the input (after feat extr)\n",
        "        self.dropout_features: float = 0.0     # dropout to apply to the features (after feat extr)\n",
        "\n",
        "        # masking\n",
        "        self.mask_length: int = 10     # mask length\n",
        "        self.mask_prob: float = 0.65     # probability of replacing a token with mask\n",
        "        self.mask_selection: str = \"static\"     # how to choose mask length\n",
        "        self.mask_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indicesh\n",
        "        self.no_mask_overlap: bool = False     # whether to allow masks to overlap\n",
        "        self.mask_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # channel masking\n",
        "        self.mask_channel_length: int = 10     # length of the mask for features (channels)\n",
        "        self.mask_channel_prob: float = 0.0     # probability of replacing a feature with 0\n",
        "        self.mask_channel_selection: str = \"static\"     # how to choose mask length for channel masking\n",
        "        self.mask_channel_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indices\n",
        "        self.no_mask_channel_overlap: bool = False     # whether to allow channel masks to overlap\n",
        "        self.mask_channel_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # positional embeddings\n",
        "        self.conv_pos: int = 128     # number of filters for convolutional positional embeddings\n",
        "        self.conv_pos_groups: int = 16     # number of groups for convolutional positional embedding\n",
        "\n",
        "        # relative position embedding\n",
        "        self.relative_position_embedding: bool = False     # apply relative position embedding\n",
        "        self.num_buckets: int = 320     # number of buckets for relative position embedding\n",
        "        self.max_distance: int = 1280     # maximum distance for relative position embedding\n",
        "        self.gru_rel_pos: bool = False     # apply gated relative position embedding\n",
        "\n",
        "        if cfg is not None:\n",
        "            self.update(cfg)\n",
        "\n",
        "    def update(self, cfg: dict):\n",
        "        self.__dict__.update(cfg)\n",
        "\n",
        "\n",
        "class WavLM(nn.Module):\n",
        "    def __init__( #[batch_size, time]\n",
        "        self,\n",
        "        cfg: WavLMConfig,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        logger.info(f\"WavLM Config: {cfg.__dict__}\")\n",
        "\n",
        "        self.cfg = cfg\n",
        "        feature_enc_layers = eval(cfg.conv_feature_layers)\n",
        "        self.embed = feature_enc_layers[-1][0]\n",
        "\n",
        "        self.feature_extractor = ConvFeatureExtractionModel(\n",
        "            conv_layers=feature_enc_layers,\n",
        "            dropout=0.0,\n",
        "            mode=cfg.extractor_mode,\n",
        "            conv_bias=cfg.conv_bias,\n",
        "        )\n",
        "\n",
        "        self.post_extract_proj = (\n",
        "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
        "            if self.embed != cfg.encoder_embed_dim\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.mask_prob = cfg.mask_prob\n",
        "        self.mask_selection = cfg.mask_selection\n",
        "        self.mask_other = cfg.mask_other\n",
        "        self.mask_length = cfg.mask_length\n",
        "        self.no_mask_overlap = cfg.no_mask_overlap\n",
        "        self.mask_min_space = cfg.mask_min_space\n",
        "\n",
        "        self.mask_channel_prob = cfg.mask_channel_prob\n",
        "        self.mask_channel_selection = cfg.mask_channel_selection\n",
        "        self.mask_channel_other = cfg.mask_channel_other\n",
        "        self.mask_channel_length = cfg.mask_channel_length\n",
        "        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
        "        self.mask_channel_min_space = cfg.mask_channel_min_space\n",
        "\n",
        "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
        "        self.dropout_features = nn.Dropout(cfg.dropout_features)\n",
        "\n",
        "        self.feature_grad_mult = cfg.feature_grad_mult\n",
        "\n",
        "        self.mask_emb = nn.Parameter(\n",
        "            torch.FloatTensor(cfg.encoder_embed_dim).uniform_()\n",
        "        )\n",
        "\n",
        "        self.encoder = TransformerEncoder(cfg)\n",
        "        self.layer_norm = LayerNorm(self.embed)\n",
        "\n",
        "    def apply_mask(self, x, padding_mask): #[batch_size, time, channel=feature]\n",
        "        B, T, C = x.shape\n",
        "        if self.mask_prob > 0:\n",
        "            mask_indices = compute_mask_indices( #특정 시간 단계에 마스크 적용\n",
        "                (B, T),# [batch_size, time]\n",
        "                padding_mask,\n",
        "                self.mask_prob,\n",
        "                self.mask_length,\n",
        "                self.mask_selection,\n",
        "                self.mask_other,\n",
        "                min_masks=2,\n",
        "                no_overlap=self.no_mask_overlap,\n",
        "                min_space=self.mask_min_space,\n",
        "            )\n",
        "            mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
        "            x[mask_indices] = self.mask_emb\n",
        "        else:\n",
        "            mask_indices = None\n",
        "\n",
        "        if self.mask_channel_prob > 0:\n",
        "            mask_channel_indices = compute_mask_indices(\n",
        "                (B, C),\n",
        "                None,\n",
        "                self.mask_channel_prob,\n",
        "                self.mask_channel_length,\n",
        "                self.mask_channel_selection,\n",
        "                self.mask_channel_other,\n",
        "                no_overlap=self.no_mask_channel_overlap,\n",
        "                min_space=self.mask_channel_min_space,\n",
        "            )\n",
        "            mask_channel_indices = (\n",
        "                torch.from_numpy(mask_channel_indices)\n",
        "                .to(x.device)\n",
        "                .unsqueeze(1)\n",
        "                .expand(-1, T, -1)\n",
        "            )\n",
        "            x[mask_channel_indices] = 0\n",
        "\n",
        "        return x, mask_indices # [batch_size, time, channel=feature]\n",
        "\n",
        "    def forward_padding_mask(\n",
        "            self, features: torch.Tensor, padding_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        extra = padding_mask.size(1) % features.size(1)\n",
        "        if extra > 0:\n",
        "            padding_mask = padding_mask[:, :-extra]\n",
        "        padding_mask = padding_mask.view(\n",
        "            padding_mask.size(0), features.size(1), -1\n",
        "        )\n",
        "        padding_mask = padding_mask.all(-1)\n",
        "        return padding_mask\n",
        "\n",
        "    def extract_features( #[batch_size, time, feature]\n",
        "        self,\n",
        "        source: torch.Tensor, #[batch_size, time]\n",
        "        padding_mask: Optional[torch.Tensor] = None, #[batch_size, time]\n",
        "        mask: bool = False,\n",
        "        ret_conv: bool = False,\n",
        "        output_layer: Optional[int] = None,\n",
        "        ret_layer_results: bool = False,\n",
        "    ):\n",
        "\n",
        "        if self.feature_grad_mult > 0:\n",
        "            features = self.feature_extractor(source)# [batch_size,time] -> [batch_size, time, feature]\n",
        "            if self.feature_grad_mult != 1.0:\n",
        "                features = GradMultiply.apply(features, self.feature_grad_mult) #[batch_size, time, feature]\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                features = self.feature_extractor(source)\n",
        "\n",
        "        features = features.transpose(1, 2) #[batch_size, feature, time]\n",
        "        features = self.layer_norm(features) #[batch_size, feature, time]\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = self.forward_padding_mask(features, padding_mask) #[batch_size, time]\n",
        "\n",
        "        if self.post_extract_proj is not None:\n",
        "            features = self.post_extract_proj(features) #[batch_size, feature, time]\n",
        "\n",
        "        features = self.dropout_input(features)\n",
        "\n",
        "        if mask:\n",
        "            x, mask_indices = self.apply_mask(\n",
        "                features, padding_mask\n",
        "            )\n",
        "        else:\n",
        "            x = features\n",
        "\n",
        "        # 중간단계 모양\n",
        "        # feature: (B, T, D), float\n",
        "        # target: (B, T), long\n",
        "        # x: (B, T, D), float\n",
        "        # padding_mask: (B, T), bool\n",
        "        # mask_indices: (B, T), bool\n",
        "        x, layer_results = self.encoder(\n",
        "            x,\n",
        "            padding_mask=padding_mask,\n",
        "            layer=None if output_layer is None else output_layer - 1\n",
        "        ) # [batch_size, channel, time]\n",
        "\n",
        "        res = {\"x\": x, \"padding_mask\": padding_mask, \"features\": features, \"layer_results\": layer_results}\n",
        "\n",
        "        feature = res[\"features\"] if ret_conv else res[\"x\"]\n",
        "        if ret_layer_results:\n",
        "            feature = (feature, res[\"layer_results\"])\n",
        "        return feature, res[\"padding_mask\"]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, cfg: WavLMConfig, pretrained_path: str):\n",
        "        \"\"\"\n",
        "        Load a pretrained WavLM model.\n",
        "        Args:\n",
        "            cfg (WavLMConfig): Configuration for WavLM.\n",
        "            pretrained_path (str): Path to the pretrained model weights.\n",
        "        Returns:\n",
        "            WavLM: An instance of WavLM initialized with pretrained weights.\n",
        "        \"\"\"\n",
        "        model = cls(cfg)\n",
        "        state_dict = torch.load(pretrained_path, map_location=\"cpu\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        logger.info(f\"Loaded pretrained weights from {pretrained_path}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "class ConvFeatureExtractionModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            conv_layers: List[Tuple[int, int, int]],\n",
        "            dropout: float = 0.0,\n",
        "            mode: str = \"default\",\n",
        "            conv_bias: bool = False,\n",
        "            conv_type: str = \"default\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert mode in {\"default\", \"layer_norm\"}\n",
        "\n",
        "        def block(\n",
        "                n_in,\n",
        "                n_out,\n",
        "                k,\n",
        "                stride,\n",
        "                is_layer_norm=False,\n",
        "                is_group_norm=False,\n",
        "                conv_bias=False,\n",
        "        ):\n",
        "            def make_conv():\n",
        "                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)\n",
        "                nn.init.kaiming_normal_(conv.weight)\n",
        "                return conv\n",
        "\n",
        "            assert (\n",
        "                           is_layer_norm and is_group_norm\n",
        "                   ) == False, \"layer norm and group norm are exclusive\"\n",
        "\n",
        "            if is_layer_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Sequential(\n",
        "                        TransposeLast(),\n",
        "                        Fp32LayerNorm(dim, elementwise_affine=True),\n",
        "                        TransposeLast(),\n",
        "                    ),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            elif is_group_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    Fp32GroupNorm(dim, dim, affine=True),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            else:\n",
        "                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n",
        "\n",
        "        self.conv_type = conv_type\n",
        "        if self.conv_type == \"default\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList() #모델 쌓기\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    block(\n",
        "                        in_d,\n",
        "                        dim,\n",
        "                        k,\n",
        "                        stride,\n",
        "                        is_layer_norm=mode == \"layer_norm\",\n",
        "                        is_group_norm=mode == \"default\" and i == 0,\n",
        "                        conv_bias=conv_bias,\n",
        "                    )\n",
        "                )\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"conv2d\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList() #모델 쌓기\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride)\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"custom\":\n",
        "            in_d = 1\n",
        "            idim = 80\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride, padding=1)\n",
        "                )\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.LayerNorm([dim, idim])\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "                if (i + 1) % 2 == 0:\n",
        "                    self.conv_layers.append(\n",
        "                        torch.nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "                    )\n",
        "                    idim = int(math.ceil(idim / 2))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, x, mask=None): #[batch_size, feature, time]\n",
        "\n",
        "        # BxT -> BxCxT\n",
        "        x = x.unsqueeze(1) #[batch_size, 1, feature, time] -> 1이 채널로 바뀌는듯?\n",
        "        if self.conv_type == \"custom\":\n",
        "            for conv in self.conv_layers:\n",
        "                if isinstance(conv, nn.LayerNorm):\n",
        "                    x = x.transpose(1, 2) # [batch_size,time,channel], 각 레이어별로 데이터 가져옴\n",
        "                    x = conv(x).transpose(1, 2) #[batch_size,channel,time], 데이터의 특징을 차원만큼 만듦\n",
        "                else:\n",
        "                    x = conv(x)\n",
        "            x = x.transpose(2, 3).contiguous()# [batch_size, channel, time, feature]\n",
        "            x = x.view(x.size(0), -1, x.size(-1))\n",
        "        else:\n",
        "            for conv in self.conv_layers:\n",
        "                x = conv(x)\n",
        "            if self.conv_type == \"conv2d\":\n",
        "                b, c, t, f = x.size() #[batch_size, channels, time, feature]\n",
        "                x = x.transpose(2, 3).contiguous().view(b, c * f, t)\n",
        "        return x #[batch_size, channel * conv된 feature, conv된 time]\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.embedding_dim = args.encoder_embed_dim\n",
        "\n",
        "        self.pos_conv = nn.Conv1d(\n",
        "            self.embedding_dim,\n",
        "            self.embedding_dim,\n",
        "            kernel_size=args.conv_pos,\n",
        "            padding=args.conv_pos // 2,\n",
        "            groups=args.conv_pos_groups,\n",
        "        )\n",
        "        dropout = 0\n",
        "        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n",
        "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
        "        nn.init.constant_(self.pos_conv.bias, 0)\n",
        "\n",
        "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
        "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n",
        "\n",
        "        if hasattr(args, \"relative_position_embedding\"):\n",
        "            self.relative_position_embedding = args.relative_position_embedding\n",
        "            self.num_buckets = args.num_buckets\n",
        "            self.max_distance = args.max_distance\n",
        "        else:\n",
        "            self.relative_position_embedding = False\n",
        "            self.num_buckets = 0\n",
        "            self.max_distance = 0\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerSentenceEncoderLayer(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                    has_relative_attention_bias=(self.relative_position_embedding and i == 0),\n",
        "                    num_buckets=self.num_buckets,\n",
        "                    max_distance=self.max_distance,\n",
        "                    gru_rel_pos=args.gru_rel_pos,\n",
        "                )\n",
        "                for i in range(args.encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layer_norm_first = args.layer_norm_first\n",
        "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
        "        self.layerdrop = args.encoder_layerdrop\n",
        "\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "    def forward(self, x, padding_mask=None, streaming_mask=None, layer=None):\n",
        "    # [batch_size, time, channel?]\n",
        "        x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)\n",
        "\n",
        "        if self.layer_norm_first and layer is None:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        return x, layer_results # [batch_size, time, embedding]\n",
        "\n",
        "    def extract_features(self, x, padding_mask=None, streaming_mask=None, tgt_layer=None):\n",
        "        # [batch_size, time, channel?]\n",
        "        if padding_mask is not None:\n",
        "            x[padding_mask] = 0\n",
        "\n",
        "        x_conv = self.pos_conv(x.transpose(1, 2)) #[batch_size, channel?,time]\n",
        "        x_conv = x_conv.transpose(1, 2)# [batch_size, channel?,time] -> [batch_size, time, channel?]\n",
        "        x = x + x_conv #특성값 더함\n",
        "\n",
        "        if not self.layer_norm_first:\n",
        "            x = self.layer_norm(x) #정규화\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1) #[batch_size, time, channel] -> [time, batch_size, channel]\n",
        "\n",
        "        layer_results = []\n",
        "        z = None\n",
        "        if tgt_layer is not None:\n",
        "            layer_results.append((x, z))\n",
        "        r = None\n",
        "        pos_bias = None\n",
        "        for i, layer in enumerate(self.layers): #각 레이어별로 계산, 무작위로 스킵\n",
        "            dropout_probability = np.random.random()\n",
        "            if not self.training or (dropout_probability > self.layerdrop):\n",
        "                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,\n",
        "                                       self_attn_mask=streaming_mask, pos_bias=pos_bias)\n",
        "            if tgt_layer is not None:\n",
        "                layer_results.append((x, z))\n",
        "            if i == tgt_layer:\n",
        "                r = x\n",
        "                break\n",
        "\n",
        "        if r is not None:\n",
        "            x = r\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(0, 1) #[time, batch_size, channel] -> [batch_size, time, channel]\n",
        "\n",
        "        return x, layer_results #[batch_size, time, channel]\n",
        "\n",
        "\n",
        "class TransformerSentenceEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = \"relu\",\n",
        "            layer_norm_first: bool = False,\n",
        "            has_relative_attention_bias: bool = False,\n",
        "            num_buckets: int = 0,\n",
        "            max_distance: int = 0,\n",
        "            rescale_init: bool = False,\n",
        "            gru_rel_pos: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        # Initialize parameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        # Initialize blocks\n",
        "        self.activation_name = activation_fn\n",
        "        self.activation_fn = get_activation_fn(activation_fn)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=True,\n",
        "            has_relative_attention_bias=has_relative_attention_bias,\n",
        "            num_buckets=num_buckets,\n",
        "            max_distance=max_distance,\n",
        "            rescale_init=rescale_init,\n",
        "            gru_rel_pos=gru_rel_pos,\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm_first = layer_norm_first\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "        if self.activation_name == \"glu\":\n",
        "            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor, # [seq_len, batch_size, embedding_dim]\n",
        "            self_attn_mask: torch.Tensor = None,\n",
        "            self_attn_padding_mask: torch.Tensor = None,\n",
        "            need_weights: bool = False,\n",
        "            pos_bias=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        LayerNorm is applied either before or after the self-attention/ffn\n",
        "        modules similar to the original Transformer imlementation.\n",
        "        \"\"\"\n",
        "        residual = x #잔차 연결\n",
        "\n",
        "        if self.layer_norm_first:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=False,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            residual = x\n",
        "            x = self.final_layer_norm(x)\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "        else:\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "            residual = x\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        return x, attn, pos_bias #[seq_len, batch_size, embedding_dim]\n"
      ],
      "metadata": {
        "id": "zi8rPiqPl57l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AVOCODO+WAVLM"
      ],
      "metadata": {
        "id": "_yOaTDVVpPkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 + wavlm\n",
        "class Avocodo_WavLM(LightningModule):\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "\n",
        "        wavlm_cfg_dict = OmegaConf.load(h.wavlm_config)\n",
        "        wavlm_cfg = WavLMConfig(wavlm_cfg_dict)\n",
        "\n",
        "        # Pretrained 모델 로드\n",
        "        #wavlm_pretrained_weights = torch.load(h.wavlm_config_pretrained)\n",
        "        self.wavlm = WavLM(wavlm_cfg)\n",
        "        #self.wavlm.load_state_dict(wavlm_pretrained_weights)\n",
        "        for param in self.wavlm.parameters(): #가중치 고정\n",
        "          param.requires_grad = False\n",
        "        self.wavlm.eval()\n",
        "\n",
        "        # Model components, pqmf lv2 고해상도, lv1 저해상도 필터\n",
        "        self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"]) # full-resolution waveform으ㄹ downsample하는 low-pass filter로 사용\n",
        "        self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "        self.generator = Generator(self.hparams.generator) #생성자\n",
        "        self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])      # Collaborative Multi-Band Discriminator\n",
        "        #다중 대역에서 생성된 신호와 실제 신호 비교\n",
        "        self.sbd = SBD(self.hparams.sbd)\n",
        "        # PQMF analysis를 통해 얻은 sub-band signal을 Discriminate\n",
        "\n",
        "        # Validation outputs storage, 자동 최적화 비활성화\n",
        "        self.validation_outputs = []\n",
        "\n",
        "        # Manual optimization\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def configure_optimizers(self): #생성자, 분별자에 대해 각각에 대한 adamw 최적화 생성\n",
        "        h = self.hparams.optimizer\n",
        "        opt_g = torch.optim.AdamW(\n",
        "            self.generator.parameters(),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        opt_d = torch.optim.AdamW(\n",
        "            itertools.chain(self.combd.parameters(), self.sbd.parameters()),            # CoMBD와 SBD 파라미터 업데이트\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        return [opt_g, opt_d]\n",
        "\n",
        "    def forward(self, z): #[batch_size, time]\n",
        "        return self.generator(z)[-1] #[batch_size, 1, time]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        ys = [\n",
        "            self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "            self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "            y\n",
        "        ]\n",
        "        y_g_hats = self.generator(x)\n",
        "\n",
        "        # Get optimizers\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        # Train Generator\n",
        "        opt_g.zero_grad()\n",
        "\n",
        "        # Feature Matching Loss\n",
        "        y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "        loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)\n",
        "        loss_gen_u, _ = generator_loss(y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "        loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "        loss_gen_s, _ = generator_loss(y_ds_hat_g)\n",
        "\n",
        "        # L1 Mel-Spectrogram Loss\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hats[-1].squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * self.hparams.loss_scale_mel\n",
        "\n",
        "        # WavLM Feature Loss (6번째 레이어)\n",
        "        wavlm_features, _ = self.wavlm.extract_features(\n",
        "            source=x.squeeze(1),\n",
        "            output_layer=6\n",
        "        )\n",
        "        target_features, _ = self.wavlm.extract_features(\n",
        "            source=y.squeeze(1),\n",
        "            output_layer=6\n",
        "        )\n",
        "        feature_loss = feature_loss_fn(wavlm_features, target_features)\n",
        "\n",
        "        self.log(\"train/feature_loss\", feature_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Total Generator Loss\n",
        "        g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel + feature_loss\n",
        "\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "        self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_d.zero_grad()\n",
        "        detached_y_g_hats = [x.detach() for x in y_g_hats]\n",
        "\n",
        "        y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "        loss_disc_u, _, _ = discriminator_loss(y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "        loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "        d_loss = loss_disc_s + loss_disc_u\n",
        "\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "        self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "\n",
        "        #wavlm_features, _ = self.wavlm.extract_features(x)\n",
        "\n",
        "        y_g_hat = self(x)\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hat.squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        val_loss = F.l1_loss(y_mel, y_g_hat_mel) #l1 손실 계산\n",
        "        self.validation_outputs.append(val_loss)\n",
        "\n",
        "        # Log validation loss\n",
        "        self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def on_validation_epoch_end(self): #각 에폭당 평균 검증 손실 계산\n",
        "        if self.validation_outputs:\n",
        "            avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "            self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "        self.validation_outputs.clear()\n",
        "\n"
      ],
      "metadata": {
        "id": "_Xz0THIelHy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283WFfS3vdmP"
      },
      "source": [
        "# WGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciNNoFIBvdmP"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc3VDfd3vdmP"
      },
      "source": [
        "$$L_D = \\mathbb{E}[D(G(z))] - \\mathbb{E}[D(x)]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9YaGg26vdmP"
      },
      "outputs": [],
      "source": [
        "# def wgan_discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "#     loss = 0\n",
        "#     r_losses = []\n",
        "#     g_losses = []\n",
        "#     for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "#         r_loss = -torch.mean(dr)\n",
        "#         g_loss = torch.mean(dg)\n",
        "#         loss += (r_loss + g_loss)\n",
        "#         r_losses.append(r_loss.item())\n",
        "#         g_losses.append(g_loss.item())\n",
        "\n",
        "#     return loss, r_losses, g_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS2hvdUnvdmP"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoDKCqnVvdmP"
      },
      "source": [
        "$$L_G = -\\mathbb{E}[D(G(z))] $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3HYxJAUvdmP"
      },
      "outputs": [],
      "source": [
        "# def wgan_generator_loss(disc_outputs):\n",
        "#     loss = 0\n",
        "#     gen_losses = []\n",
        "#     for dg in disc_outputs:\n",
        "#         l = -torch.mean(dg)\n",
        "#         gen_losses.append(l)\n",
        "#         loss += l\n",
        "\n",
        "#     return loss, gen_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzWzfjhxvdmP"
      },
      "source": [
        "### Weight Clipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKqeWI8DvdmP"
      },
      "outputs": [],
      "source": [
        "# def clip_weights(discriminator, clip_value):\n",
        "#     \"\"\"\n",
        "#     Clips the weights of the discriminator to satisfy 1-Lipschitz constraint.\n",
        "#     \"\"\"\n",
        "#     for p in discriminator.parameters():\n",
        "#         p.data.clamp_(-clip_value, clip_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a37_w_rFvdmP"
      },
      "source": [
        "### Avocodo_WGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F7BxizAvdmQ"
      },
      "outputs": [],
      "source": [
        "# class Avocodo_wgan(LightningModule):\n",
        "#     def __init__(self, h):\n",
        "#         super().__init__()\n",
        "#         self.save_hyperparameters(h)\n",
        "\n",
        "#         # Model components\n",
        "#         self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])      # full-resolution waveform으ㄹ downsample하는 low-pass filter로 사용\n",
        "#         self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "#         self.generator = Generator(self.hparams.generator)\n",
        "#         self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])      # Collaborative Multi-Band Discriminator\n",
        "#         self.sbd = SBD(self.hparams.sbd)    # PQMF analysis를 통해 얻은 sub-band signal을 Discriminate\n",
        "\n",
        "#         # Validation outputs storage\n",
        "#         self.validation_outputs = []\n",
        "\n",
        "#         # Manual optimization\n",
        "#         self.automatic_optimization = False\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         h = self.hparams.optimizer\n",
        "#         opt_g = torch.optim.AdamW(\n",
        "#             self.generator.parameters(),\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         opt_d = torch.optim.AdamW(\n",
        "#             itertools.chain(self.combd.parameters(), self.sbd.parameters()),            # CoMBD와 SBD 파라미터 업데이트\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         return [opt_g, opt_d]\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         return self.generator(z)[-1]\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y = y.unsqueeze(1)\n",
        "#         ys = [                                                                                                                      # pqmf로 다운샘플링 된 waveform 리스트\n",
        "#             self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "#             self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "#             y\n",
        "#         ]\n",
        "#         y_g_hats = self.generator(x)                ## Generator가 생성한 waveform\n",
        "\n",
        "#         # Get optimizers\n",
        "#         opt_g, opt_d = self.optimizers()\n",
        "\n",
        "#         # Train Generator\n",
        "#         opt_g.zero_grad()\n",
        "#         y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "#         loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)         # 실제(real) 중간 특징 맵과 생성(generate) 중간 특징 맵간의 loss 계산\n",
        "#         loss_gen_u, _ = wgan_generator_loss(y_du_hat_g)            # wgan loss 추가\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "#         loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "#         loss_gen_s, _ = wgan_generator_loss(y_ds_hat_g)            # wgan loss 추가\n",
        "\n",
        "\n",
        "#         # L1 Mel-Spectrogram Loss\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hats[-1].squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "#         self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "#         loss_mel = loss_mel * self.hparams.loss_scale_mel # lambda 값\n",
        "\n",
        "#         g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel\n",
        "\n",
        "\n",
        "#         self.manual_backward(g_loss)\n",
        "#         opt_g.step()\n",
        "#         self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         # Train Discriminator\n",
        "#         opt_d.zero_grad()\n",
        "#         detached_y_g_hats = [x.detach() for x in y_g_hats]      # generator의 출력 데이터를 분리 -> 그래디언트가 전파되지 않도록(generator의 학습에 영향을 주지 않도록)\n",
        "\n",
        "#         y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "#         loss_disc_u, _, _= wgan_discriminator_loss(y_du_hat_r, y_du_hat_g)        # wgan loss 추가\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "#         loss_disc_s, _, _= wgan_discriminator_loss(y_ds_hat_r, y_ds_hat_g)        # wgan loss 추가\n",
        "\n",
        "#         d_loss = loss_disc_s + loss_disc_u\n",
        "#         self.manual_backward(d_loss)\n",
        "#         opt_d.step()\n",
        "\n",
        "#         clip_weights(self.combd, 0.01)\n",
        "#         clip_weights(self.sbd, 0.01)\n",
        "\n",
        "#         self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y_g_hat = self(x)\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hat.squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         val_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "#         self.validation_outputs.append(val_loss)\n",
        "\n",
        "#         # Log validation loss\n",
        "#         self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return val_loss\n",
        "\n",
        "#     def on_validation_epoch_end(self):\n",
        "#         if self.validation_outputs:\n",
        "#             avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "#             self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "#         self.validation_outputs.clear()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q611q-3rvdmQ"
      },
      "source": [
        "## WGAN_GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHkDFMc6vdmQ"
      },
      "outputs": [],
      "source": [
        "# lambda_gp = 10\n",
        "\n",
        "# def compute_gradient_penalty(discriminator, real_samples, fake_samples, device=\"cuda\"):\n",
        "#     \"\"\"\n",
        "#     Compute Gradient Penalty for WGAN-GP.\n",
        "#     \"\"\"\n",
        "#     batch_size = real_samples.size(0)\n",
        "#     epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
        "#     interpolated = epsilon * real_samples + (1 - epsilon) * fake_samples\n",
        "#     interpolated.requires_grad_(True)\n",
        "\n",
        "#     interpolated_scores = discriminator(interpolated)\n",
        "#     gradients = torch.autograd.grad(\n",
        "#         outputs=interpolated_scores,\n",
        "#         inputs=interpolated,\n",
        "#         grad_outputs=torch.ones_like(interpolated_scores),\n",
        "#         create_graph=True,\n",
        "#         retain_graph=True\n",
        "#     )[0]\n",
        "\n",
        "#     gradients = gradients.view(batch_size, -1)\n",
        "#     gradient_norm = gradients.norm(2, dim=1)\n",
        "#     gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "#     return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqTrfHEtvdmQ"
      },
      "outputs": [],
      "source": [
        "# class Avocodo_wgan_gp(LightningModule):\n",
        "#     def __init__(self, h):\n",
        "#         super().__init__()\n",
        "#         self.save_hyperparameters(h)\n",
        "\n",
        "#         # Model components\n",
        "#         self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])      # full-resolution waveform으ㄹ downsample하는 low-pass filter로 사용\n",
        "#         self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "#         self.generator = Generator(self.hparams.generator)\n",
        "#         self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])      # Collaborative Multi-Band Discriminator\n",
        "#         self.sbd = SBD(self.hparams.sbd)    # PQMF analysis를 통해 얻은 sub-band signal을 Discriminate\n",
        "\n",
        "#         # Validation outputs storage\n",
        "#         self.validation_outputs = []\n",
        "\n",
        "#         # Manual optimization\n",
        "#         self.automatic_optimization = False\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         h = self.hparams.optimizer\n",
        "#         opt_g = torch.optim.AdamW(\n",
        "#             self.generator.parameters(),\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         opt_d = torch.optim.AdamW(\n",
        "#             itertools.chain(self.combd.parameters(), self.sbd.parameters()),            # CoMBD와 SBD 파라미터 업데이트\n",
        "#             lr=h.learning_rate,\n",
        "#             betas=(h.adam_b1, h.adam_b2)\n",
        "#         )\n",
        "#         return [opt_g, opt_d]\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         return self.generator(z)[-1]\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y = y.unsqueeze(1)\n",
        "#         ys = [                                                                                                                      # pqmf로 다운샘플링 된 waveform 리스트\n",
        "#             self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "#             self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "#             y\n",
        "#         ]\n",
        "#         y_g_hats = self.generator(x)                ## Generator가 생성한 waveform\n",
        "\n",
        "#         # Get optimizers\n",
        "#         opt_g, opt_d = self.optimizers()\n",
        "\n",
        "#         # Train Generator\n",
        "#         opt_g.zero_grad()\n",
        "#         y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "#         loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)         # 실제(real) 중간 특징 맵과 생성(generate) 중간 특징 맵간의 loss 계산\n",
        "#         loss_gen_u, _ = wgan_generator_loss(y_du_hat_g)            # wgan loss 추가\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "#         loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "#         loss_gen_s, _ = wgan_generator_loss(y_ds_hat_g)            # wgan loss 추가\n",
        "\n",
        "#         # L1 Mel-Spectrogram Loss\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hats[-1].squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "#         self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "#         loss_mel = loss_mel * self.hparams.loss_scale_mel # lambda 값\n",
        "\n",
        "#         g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel\n",
        "\n",
        "\n",
        "#         self.manual_backward(g_loss)\n",
        "#         opt_g.step()\n",
        "#         self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         # Train Discriminator\n",
        "#         opt_d.zero_grad()\n",
        "#         detached_y_g_hats = [x.detach() for x in y_g_hats]      # generator의 출력 데이터를 분리 -> 그래디언트가 전파되지 않도록(generator의 학습에 영향을 주지 않도록)\n",
        "\n",
        "#         y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "#         loss_disc_u,_,_= wgan_discriminator_loss(y_du_hat_r, y_du_hat_g)        # wgan loss 추가\n",
        "\n",
        "#         y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "#         loss_disc_s,_,_= wgan_discriminator_loss(y_ds_hat_r, y_ds_hat_g)        # wgan loss 추가\n",
        "\n",
        "#         #패널티 부분 추가\n",
        "#         gradient_penalty_combd = compute_gradient_penalty(self.combd,ys[-1], detached_y_g_hats[-1])\n",
        "#         gradient_penalty_sbd = compute_gradient_penalty(self.sbd,y, detached_y_g_hats[-1])\n",
        "\n",
        "#         d_loss = loss_disc_s + loss_disc_u + lambda_gp*gradient_penalty_combd  + lambda_gp*gradient_penalty_sbd\n",
        "#         self.manual_backward(d_loss)\n",
        "#         opt_d.step()\n",
        "\n",
        "#         self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         x, y, _, y_mel = batch\n",
        "#         y_g_hat = self(x)\n",
        "#         y_g_hat_mel = mel_spectrogram(\n",
        "#             y_g_hat.squeeze(1),\n",
        "#             self.hparams.audio.n_fft,\n",
        "#             self.hparams.audio.num_mels,\n",
        "#             self.hparams.audio.sampling_rate,\n",
        "#             self.hparams.audio.hop_size,\n",
        "#             self.hparams.audio.win_size,\n",
        "#             self.hparams.audio.fmin,\n",
        "#             self.hparams.audio.fmax_for_loss\n",
        "#         )\n",
        "#         val_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "#         self.validation_outputs.append(val_loss)\n",
        "\n",
        "#         # Log validation loss\n",
        "#         self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "#         return val_loss\n",
        "\n",
        "#     def on_validation_epoch_end(self):\n",
        "#         if self.validation_outputs:\n",
        "#             avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "#             self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "#         self.validation_outputs.clear()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VohGwH8g_Dov"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHLRL85QEBkW"
      },
      "outputs": [],
      "source": [
        "class TBLogger(TensorBoardLogger): #로그\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        metrics.pop('epoch', None)\n",
        "        return super().log_metrics(metrics, step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY4uFuC9EEZY"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser() #세팅\n",
        "\n",
        "parser.add_argument('--group_name', default=None)\n",
        "parser.add_argument('--input_wavs_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs')\n",
        "parser.add_argument('--input_mels_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram')\n",
        "parser.add_argument('--input_training_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt')\n",
        "parser.add_argument('--input_validation_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt')\n",
        "parser.add_argument('--config', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json')\n",
        "parser.add_argument('--training_epochs', default=100, type=int)\n",
        "parser.add_argument('--fine_tuning', default=False, type=bool)\n",
        "parser.add_argument('--wavlm_config_pretrained',default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt')\n",
        "\n",
        "\n",
        "\n",
        "if \"ipykernel_launcher\" in sys.argv[0]:\n",
        "    sys.argv = [\n",
        "        'script_name',\n",
        "        '--group_name', 'default_group',\n",
        "        '--input_wavs_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs',\n",
        "        '--input_mels_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram',\n",
        "        '--input_training_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt',\n",
        "        '--input_validation_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt',\n",
        "        '--config', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json',\n",
        "        '--training_epochs', '100',\n",
        "        '--fine_tuning', 'False',\n",
        "        '--wavlm_config_pretrained','/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rA9AK5Hy_CuE"
      },
      "outputs": [],
      "source": [
        "data_dir='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt'\n",
        "#data_dir='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt'\n",
        "real_dir='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs'\n",
        "# exists=[]\n",
        "# with open(data_dir, 'r') as file:\n",
        "#     datas = file.readlines()  # 각 줄을 리스트로 읽음\n",
        "\n",
        "# for data in datas:\n",
        "#     data = data.split('|')[0].strip()  # 파일 이름 추출\n",
        "#     data+=\".wav\"\n",
        "#     file_path = os.path.join(real_dir, data)  # 전체 경로 생성\n",
        "#     print(file_path)\n",
        "#     if not os.path.exists(file_path):  # 파일이 존재하지 않는 경우\n",
        "#         exists.append(data)\n",
        "\n",
        "# print(\"Missing files:\", exists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBKFZ_0w9FV9"
      },
      "outputs": [],
      "source": [
        "# args = parser.parse_args()\n",
        "# print(f\"Fine Tuning: {args.fine_tuning}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu96Tbu2Twyi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import librosa\n",
        "\n",
        "# 샘플 Mel-spectrogram 생성 함수\n",
        "def generate_mel_spectrogram(sample_rate=22050, n_fft=1024, hop_length=256, n_mels=80, duration=1.0):\n",
        "    # 가상 오디오 데이터 생성 (White Noise)\n",
        "    raw_audio = torch.randn(int(sample_rate * duration))  # 1초 길이의 오디오\n",
        "\n",
        "    # Mel-spectrogram 변환\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(\n",
        "        y=raw_audio.numpy(),\n",
        "        sr=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    mel_tensor = torch.tensor(mel_spectrogram).unsqueeze(0)  # 배치 차원 추가\n",
        "    return mel_tensor\n",
        "\n",
        "# 예시 데이터 준비\n",
        "mel_input = generate_mel_spectrogram(duration=1.0)  # 1초 길이의 Mel-spectrogram\n",
        "mel_input = mel_input.unsqueeze(0)  # 배치 차원 추가 (1, 80, Frames)\n",
        "mel_input = mel_input.squeeze(1)\n",
        "print(f\"Input Shape: {mel_input.shape}\")  # (1, 80, Frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJuJSVJR_DRA"
      },
      "outputs": [],
      "source": [
        "# Parse Arguments\n",
        "a, unknown = parser.parse_known_args()\n",
        "\n",
        "# OmegaConf 설정\n",
        "OmegaConf.register_new_resolver(\"from_args\", lambda x: getattr(a, x))\n",
        "OmegaConf.register_new_resolver(\"dir\", lambda base_dir, string: os.path.join(base_dir, string))\n",
        "conf = OmegaConf.load(a.config)\n",
        "OmegaConf.resolve(conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQYocu51VK1C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 데이터 및 모델 초기화\n",
        "dm = AvocodoData(conf.data)\n",
        "\n",
        "#필요한 모델만 불러와서 사용\n",
        "#model = Avocodo(conf.model)\n",
        "model = Avocodo_WavLM(conf.model)\n",
        "#model = Avocodo_wgan(conf.model)\n",
        "#model = Avocodo_wgan_gp(conf.model)\n",
        "\n",
        "# 모델 요약 출력\n",
        "summary(\n",
        "    model,\n",
        "    input_data=mel_input,  # 입력 데이터 전달\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\"],  # 표시할 열 선택\n",
        "    col_width=20,\n",
        "    depth=5  # 레이어 깊이 제한\n",
        ")\n",
        "\n",
        "limit_train_batches = 1.0\n",
        "limit_val_batches = 1.0\n",
        "log_every_n_steps = 50\n",
        "max_epochs = conf.model.train.training_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hmp4fZrEasS"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',               # Validation 손실 기준\n",
        "    dirpath='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint_wavlm',  # 체크포인트 저장 디렉토리\n",
        "    filename='best-checkpoint-{epoch:02d}-{val_loss:.2f}',  # 파일 이름 패턴\n",
        "    save_top_k=1,                     # 가장 좋은 k개의 모델만 저장\n",
        "    mode='min',                       # 손실 기준으로 최소값 저장\n",
        ")\n",
        "\n",
        "# 조기 종료 콜백: Validation 손실 개선이 없으면 중단\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',              # Validation 손실 기준\n",
        "    patience=10000000,                     # 몇 에포크 동안 개선 없으면 중단\n",
        "    verbose=True,                    # 중단 시 메시지 출력\n",
        "    mode='min'                       # 손실 기준으로 최소값 기준\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUWFRnb9EJ33"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=\"auto\",\n",
        "    max_epochs=max_epochs,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        early_stopping_callback,\n",
        "        RichProgressBar(\n",
        "            refresh_rate=1,\n",
        "            theme=RichProgressBarTheme(\n",
        "                description=\"#AF81EB\",\n",
        "                progress_bar=\"#8BE9FE\",\n",
        "                progress_bar_finished=\"#8BE9FE\",\n",
        "                progress_bar_pulse=\"#1363DF\",\n",
        "                batch_progress=\"#AF81EB\",\n",
        "                time=\"#1363DF\",\n",
        "                processing_speed=\"#1363DF\",\n",
        "                metrics=\"#9BF9FE\",\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    # logger=TensorBoardLogger(\"logs\", name=\"Avocodo\"),\n",
        "    logger=wandb_logger,\n",
        "    limit_train_batches=limit_train_batches,\n",
        "    limit_val_batches=limit_val_batches,\n",
        "    log_every_n_steps=log_every_n_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq4FBO9OArgF"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32FgGv8ihNR"
      },
      "source": [
        "# inference.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6baN8L8CmXVM"
      },
      "outputs": [],
      "source": [
        "\n",
        "h = None\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(\n",
        "        x,\n",
        "        1024,\n",
        "        80,\n",
        "        22050,\n",
        "        256,\n",
        "        1024,\n",
        "        0,\n",
        "        8000\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6rbIWAomjLc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def inference(a, conf):#추론 config 로그 저장\n",
        "    avocodo = Avocodo.load_from_checkpoint(\n",
        "        f\"{a.checkpoint_path}/version_{a.version}/checkpoints/{a.checkpoint_file_id}\",\n",
        "        map_location='cpu'\n",
        "    )\n",
        "    avocodo_data = AvocodoData(conf.audio)\n",
        "    avocodo_data.prepare_data()\n",
        "    validation_dataloader = avocodo_data.val_dataloader()\n",
        "\n",
        "    output_path = f'{a.output_dir}/version_{a.version}/'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    avocodo.generator.to(a.device)\n",
        "    avocodo.generator.remove_weight_norm()\n",
        "\n",
        "    m = torch.jit.script(avocodo.generator)\n",
        "    torch.jit.save(\n",
        "        m,\n",
        "        os.path.join(output_path, \"scripted.pt\")\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(validation_dataloader):\n",
        "            mels, _, file_ids, _ = batch\n",
        "\n",
        "            y_g_hat = avocodo(mels.to(a.device))\n",
        "\n",
        "            for _y_g_hat, file_id in zip(y_g_hat, file_ids):\n",
        "                audio = _y_g_hat.squeeze(0)\n",
        "                audio = audio * MAX_WAV_VALUE\n",
        "                audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "                output_file = os.path.join(\n",
        "                    output_path,\n",
        "                    file_id.split('/')[-1]\n",
        "                )\n",
        "                print(file_id)\n",
        "                write(output_file, conf.audio.sampling_rate, audio)\n",
        "    print('Done inference')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69y4YDJZiilp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--checkpoint_path', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint_wavlm')\n",
        "    parser.add_argument('--version', type=int, required=True)\n",
        "    parser.add_argument('--checkpoint_file_id', type=str, default='', required=True)\n",
        "    parser.add_argument('--output_dir', type=str, default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result_wavlm')\n",
        "    parser.add_argument('--script', type=bool, default=True)\n",
        "    parser.add_argument('--device', type=str, default='cuda')\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    conf = OmegaConf.load(os.path.join(a.checkpoint_path, f\"version_{a.version}\", \"hparams.yaml\"))\n",
        "    inference(a, conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn0baVSmmlsz"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "283WFfS3vdmP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}