{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ekp16rP1qOj0y7N3PrfTMGKJFLoOwutO",
      "authorship_tag": "ABX9TyOidxtVF+sEa2Hrb44Kr8jz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/auto_vc_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2Vzcr775Ib",
        "outputId": "6b911a19-8689-414f-e948-1c0c9fb446c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wavenet_vocoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAKmnK5n6Gez",
        "outputId": "9d9fd19b-8cfd-4256-b2bf-31f3bda68b41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wavenet_vocoder\n",
            "  Downloading wavenet_vocoder-0.1.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.13.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->wavenet_vocoder) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->wavenet_vocoder) (1.3.0)\n",
            "Building wheels for collected packages: wavenet_vocoder\n",
            "  Building wheel for wavenet_vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet_vocoder: filename=wavenet_vocoder-0.1.1-py3-none-any.whl size=12642 sha256=6255ce3ee820c0e1a36525e8ee47f38025d92ff60811bd623084c64a8640e3e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/a4/7b/f1d21f96be36a13e9c3948e8c28792bf8962da19781abd9dc8\n",
            "Successfully built wavenet_vocoder\n",
            "Installing collected packages: wavenet_vocoder\n",
            "Successfully installed wavenet_vocoder-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "import time\n",
        "import datetime"
      ],
      "metadata": {
        "id": "s8YKdCC3ciLI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V34cp_HHalfN"
      },
      "outputs": [],
      "source": [
        "#make_spect.py\n",
        "def butter_highpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def pySTFT(x, fft_length=1024, hop_length=256):\n",
        "\n",
        "    x = np.pad(x, int(fft_length//2), mode='reflect')\n",
        "\n",
        "    noverlap = fft_length - hop_length\n",
        "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)\n",
        "    strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])\n",
        "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "                                             strides=strides)\n",
        "\n",
        "    fft_window = get_window('hann', fft_length, fftbins=True)\n",
        "    result = np.fft.rfft(fft_window * result, n=fft_length).T\n",
        "\n",
        "    return np.abs(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_basis = mel(sr=16000,n_fft=1024,  n_mels=80, fmin=90, fmax=7600).T #멜 스펙트로그램 생성\n",
        "min_level = np.exp(-100 / 20 * np.log(10))\n",
        "b, a = butter_highpass(30, 16000, order=5)\n",
        "\n",
        "\n",
        "# audio file directory\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'\n",
        "\n",
        "# audio file directory\n",
        "# rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "# targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/results'\n",
        "\n",
        "#  audio file directory\n",
        "# rootDir = './wavs'\n",
        "# # spectrogram directory\n",
        "# targetDir = './spmel'\n",
        "\n",
        "\n",
        "\n",
        "dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "for subdir in sorted(subdirList):\n",
        "    print(subdir)\n",
        "    if not os.path.exists(os.path.join(targetDir, subdir)):\n",
        "        os.makedirs(os.path.join(targetDir, subdir))\n",
        "    _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))\n",
        "    prng = RandomState(int(subdir[1:]))\n",
        "    for fileName in sorted(fileList):\n",
        "        # Read audio file\n",
        "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))\n",
        "        # Remove drifting noise\n",
        "        y = signal.filtfilt(b, a, x)\n",
        "        # Ddd a little random noise for model roubstness\n",
        "        wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
        "        # Compute spect\n",
        "        D = pySTFT(wav).T\n",
        "        # Convert to mel and normalize\n",
        "        D_mel = np.dot(D, mel_basis)\n",
        "        D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16\n",
        "        S = np.clip((D_db + 100) / 100, 0, 1)\n",
        "        # save spect\n",
        "        np.save(os.path.join(targetDir, subdir, fileName[:-4]),\n",
        "                S.astype(np.float32), allow_pickle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc3uiuq89uh4",
        "outputId": "48d4d8fb-8209-4ebb-823a-fe21bc27de05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs\n",
            "p225\n",
            "p226\n",
            "p227\n",
            "p228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_bl.py\n",
        "\n",
        "class D_VECTOR(nn.Module):\n",
        "    \"\"\"d vector speaker embedding.\"\"\"\n",
        "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
        "        super(D_VECTOR, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        embeds = self.embedding(lstm_out[:,-1,:])\n",
        "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        embeds_normalized = embeds.div(norm)\n",
        "        return embeds_normalized\n"
      ],
      "metadata": {
        "id": "i515s4B4a9Jl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make_metadata.py\n",
        "\"\"\"\n",
        "Generate speaker embeddings and metadata for training\n",
        "\"\"\"\n",
        "\n",
        "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda()\n",
        "c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n",
        "# c_checkpoint = torch.load('3000000-BL.ckpt')\n",
        "new_state_dict = OrderedDict()\n",
        "for key, val in c_checkpoint['model_b'].items():\n",
        "    new_key = key[7:]\n",
        "    new_state_dict[new_key] = val\n",
        "C.load_state_dict(new_state_dict)\n",
        "num_uttrs = 10\n",
        "len_crop = 128\n",
        "\n",
        "\n",
        "# Directory containing mel-spectrograms, 별도 데이터셋?\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'#?\n",
        "# rootDir = './spmel'\n",
        "dirName, subdirList, _ = next(os.walk(rootDir)) #데이터셋 파일 필요함\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "speakers = []\n",
        "for speaker in sorted(subdirList):\n",
        "    print('Processing speaker: %s' % speaker)\n",
        "    utterances = []\n",
        "    utterances.append(speaker)\n",
        "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
        "\n",
        "    # make speaker embedding\n",
        "    assert len(fileList) >= num_uttrs\n",
        "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
        "    embs = []\n",
        "    for i in range(num_uttrs):\n",
        "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
        "        candidates = np.delete(np.arange(len(fileList)), idx_uttrs)\n",
        "        # choose another utterance if the current one is too short\n",
        "        while tmp.shape[0] < len_crop:\n",
        "            idx_alt = np.random.choice(candidates)\n",
        "            tmp = np.load(os.path.join(dirName, speaker, fileList[idx_alt]))\n",
        "            candidates = np.delete(candidates, np.argwhere(candidates==idx_alt))\n",
        "        left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
        "        melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :]).cuda()\n",
        "        emb = C(melsp)\n",
        "        embs.append(emb.detach().squeeze().cpu().numpy())\n",
        "    utterances.append(np.mean(embs, axis=0))\n",
        "\n",
        "    # create file list\n",
        "    for fileName in sorted(fileList):\n",
        "        utterances.append(os.path.join(speaker,fileName))\n",
        "    speakers.append(utterances)\n",
        "\n",
        "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
        "    pickle.dump(speakers, handle)"
      ],
      "metadata": {
        "id": "KziVBX5dbD9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c62d0f9-5033-471b-a70b-8adcb8d1b698"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-704b09355319>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel\n",
            "Processing speaker: p225\n",
            "Processing speaker: p226\n",
            "Processing speaker: p227\n",
            "Processing speaker: p228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loader.py\n",
        "\n",
        "\n",
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, len_crop):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.len_crop = len_crop\n",
        "        self.step = 10\n",
        "\n",
        "        metaname = os.path.join(self.root_dir, \"train.pkl\")\n",
        "        meta = pickle.load(open(metaname, \"rb\"))\n",
        "\n",
        "        \"\"\"Load data using multiprocessing\"\"\"\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the dataset...')\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]\n",
        "            for j, tmp in enumerate(sbmt):\n",
        "                if j < 2:  # fill in speaker id and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else: # load the mel-spectrograms\n",
        "\n",
        "\n",
        "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp),allow_pickle=True)\n",
        "                    #uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
        "\n",
        "\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # pick a random speaker\n",
        "        dataset = self.train_dataset\n",
        "        list_uttrs = dataset[index]\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        # pick random uttr with random crop\n",
        "        a = np.random.randint(2, len(list_uttrs))\n",
        "        tmp = list_uttrs[a]\n",
        "        if tmp.shape[0] < self.len_crop:\n",
        "            len_pad = self.len_crop - tmp.shape[0]\n",
        "            uttr = np.pad(tmp, ((0,len_pad),(0,0)), 'constant')\n",
        "        elif tmp.shape[0] > self.len_crop:\n",
        "            left = np.random.randint(tmp.shape[0]-self.len_crop)\n",
        "            uttr = tmp[left:left+self.len_crop, :]\n",
        "        else:\n",
        "            uttr = tmp\n",
        "\n",
        "        return uttr, emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size=16, len_crop=128, num_workers=0):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(root_dir, len_crop)\n",
        "\n",
        "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  worker_init_fn=worker_init_fn)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "QSI7SwAXbfj_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델"
      ],
      "metadata": {
        "id": "G6zEd5YR_knf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_vc.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module): #합성곱 놈\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class Encoder(nn.Module): #인코더 모델\n",
        "    \"\"\"Encoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, freq):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_neck = dim_neck\n",
        "        self.freq = freq\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(80+dim_emb if i==0 else 512,\n",
        "                         512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(512))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, c_org):\n",
        "        x = x.squeeze(1).transpose(2,1)\n",
        "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
        "        x = torch.cat((x, c_org), dim=1)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "        out_forward = outputs[:, :, :self.dim_neck]\n",
        "        out_backward = outputs[:, :, self.dim_neck:]\n",
        "\n",
        "        codes = []\n",
        "        for i in range(0, outputs.size(1), self.freq):\n",
        "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
        "\n",
        "        return codes\n",
        "\n",
        "\n",
        "class Decoder(nn.Module): #디코더 모델\n",
        "    \"\"\"Decoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(dim_pre,\n",
        "                         dim_pre,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(dim_pre))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
        "\n",
        "        self.linear_projection = LinearNorm(1024, 80)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #self.lstm1.flatten_parameters()\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        outputs, _ = self.lstm2(x)\n",
        "\n",
        "        decoder_output = self.linear_projection(outputs)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "class Postnet(nn.Module): #포스트넷?\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(80, 512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(512))\n",
        "        )\n",
        "\n",
        "        for i in range(1, 5 - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(512,\n",
        "                             512,\n",
        "                             kernel_size=5, stride=1,\n",
        "                             padding=2,\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(512))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(512, 80,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(80))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = torch.tanh(self.convolutions[i](x))\n",
        "\n",
        "        x = self.convolutions[-1](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator(nn.Module): #생성기 = 인코더+디코더+포스트넷\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
        "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, x, c_org, c_trg):\n",
        "\n",
        "        codes = self.encoder(x, c_org)\n",
        "        if c_trg is None:\n",
        "            return torch.cat(codes, dim=-1)\n",
        "\n",
        "        tmp = []\n",
        "        for code in codes:\n",
        "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
        "        code_exp = torch.cat(tmp, dim=1)\n",
        "\n",
        "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
        "\n",
        "        mel_outputs = self.decoder(encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
        "\n",
        "        mel_outputs = mel_outputs.unsqueeze(1)\n",
        "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "hqCzLhYybRMT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "LLAX8yLy_sDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n"
      ],
      "metadata": {
        "id": "lnlPJX6JK57d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/코딩공부/deep_daiv/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ6SWuZZfU08",
        "outputId": "ee43c42e-e7b0-427a-d626-9e88d4c497df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/코딩공부/deep_daiv/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "itRsZDeOg1f9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.argv=['']\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Model configuration.\n",
        "parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
        "parser.add_argument('--dim_neck', type=int, default=16)\n",
        "parser.add_argument('--dim_emb', type=int, default=256)\n",
        "parser.add_argument('--dim_pre', type=int, default=512)\n",
        "parser.add_argument('--freq', type=int, default=16)\n",
        "\n",
        "# Training configuration.\n",
        "parser.add_argument('--data_dir', type=str, default='./spmel')\n",
        "parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
        "parser.add_argument('--num_iters', type=int, default=1000000, help='number of total iterations')\n",
        "parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
        "\n",
        "# Miscellaneous.\n",
        "parser.add_argument('--log_step', type=int, default=10)\n",
        "\n",
        "config = parser.parse_args()"
      ],
      "metadata": {
        "id": "kmwCxNSLcN1e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from easydict import EasyDict as edict\n",
        "\n",
        "# config = edict()\n",
        "\n",
        "# # Model configuration.\n",
        "# config.lambda_cd = 1\n",
        "# config.dim_neck = 16\n",
        "# config.dim_emb = 256\n",
        "# config.dim_pre = 512\n",
        "# config.freq = 16\n",
        "\n",
        "# # Training configuration.\n",
        "# config.data_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# config.batch_size = 2\n",
        "# config.num_iters = 1000000\n",
        "# config.len_crop = 128\n",
        "\n",
        "# # Miscellaneous.\n",
        "# config.log_step = 10"
      ],
      "metadata": {
        "id": "YHF3fFgzQVL9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solver_encoder.py\n",
        "\n",
        "class Solver(object): #음성신호 학습\n",
        "\n",
        "    def __init__(self, vcc_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.vcc_loader = vcc_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.lambda_cd = config.lambda_cd\n",
        "        self.dim_neck = config.dim_neck\n",
        "        self.dim_emb = config.dim_emb\n",
        "        self.dim_pre = config.dim_pre\n",
        "        self.freq = config.freq\n",
        "\n",
        "        # Training configurations.\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
        "        self.log_step = config.log_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), 0.0001)\n",
        "\n",
        "        self.G.to(self.device)\n",
        "\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    #=====================================================================================================================================#\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # Set data loader.\n",
        "        data_loader = self.vcc_loader\n",
        "\n",
        "        # Print logs in specified order\n",
        "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Fetch data.\n",
        "            try:\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, emb_org = next(data_iter)\n",
        "\n",
        "\n",
        "\n",
        "            x_real = x_real.to(self.device)\n",
        "            emb_org = emb_org.to(self.device)\n",
        "\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               2. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            self.G = self.G.train()\n",
        "\n",
        "            # Identity mapping loss\n",
        "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
        "            g_loss_id = F.mse_loss(x_real, x_identic)\n",
        "            g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n",
        "\n",
        "            # Code semantic loss.\n",
        "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
        "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
        "\n",
        "\n",
        "            # Backward and optimize.\n",
        "            g_loss = g_loss_id + g_loss_id_psnt + self.lambda_cd * g_loss_cd\n",
        "            self.reset_grad()\n",
        "            g_loss.backward()\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['G/loss_id'] = g_loss_id.item()\n",
        "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
        "            loss['G/loss_cd'] = g_loss_cd.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag in keys:\n",
        "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
        "                print(log)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXgGFoXnb0De"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "    # Data loader.\n",
        "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
        "\n",
        "solver = Solver(vcc_loader, config)\n",
        "\n",
        "solver.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "C-tF8qGvMpXK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7d2bc53-6852-485c-cca5-81f093ce58eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished loading the dataset...\n",
            "Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-0a75626680c8>:86: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  g_loss_id = F.mse_loss(x_real, x_identic)\n",
            "<ipython-input-19-0a75626680c8>:87: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed [0:00:02], Iteration [10/1000000], G/loss_id: 0.1532, G/loss_id_psnt: 1.1175, G/loss_cd: 0.1230\n",
            "Elapsed [0:00:03], Iteration [20/1000000], G/loss_id: 0.0594, G/loss_id_psnt: 0.9366, G/loss_cd: 0.1170\n",
            "Elapsed [0:00:04], Iteration [30/1000000], G/loss_id: 0.1137, G/loss_id_psnt: 0.6641, G/loss_cd: 0.1126\n",
            "Elapsed [0:00:04], Iteration [40/1000000], G/loss_id: 0.1895, G/loss_id_psnt: 0.4757, G/loss_cd: 0.1096\n",
            "Elapsed [0:00:05], Iteration [50/1000000], G/loss_id: 0.3117, G/loss_id_psnt: 0.2996, G/loss_cd: 0.0920\n",
            "Elapsed [0:00:06], Iteration [60/1000000], G/loss_id: 0.2179, G/loss_id_psnt: 0.3585, G/loss_cd: 0.0848\n",
            "Elapsed [0:00:07], Iteration [70/1000000], G/loss_id: 0.2552, G/loss_id_psnt: 0.3200, G/loss_cd: 0.0782\n",
            "Elapsed [0:00:07], Iteration [80/1000000], G/loss_id: 0.2784, G/loss_id_psnt: 0.2824, G/loss_cd: 0.0626\n",
            "Elapsed [0:00:08], Iteration [90/1000000], G/loss_id: 0.2482, G/loss_id_psnt: 0.3102, G/loss_cd: 0.0723\n",
            "Elapsed [0:00:09], Iteration [100/1000000], G/loss_id: 0.2492, G/loss_id_psnt: 0.3042, G/loss_cd: 0.0581\n",
            "Elapsed [0:00:10], Iteration [110/1000000], G/loss_id: 0.2723, G/loss_id_psnt: 0.2643, G/loss_cd: 0.0596\n",
            "Elapsed [0:00:10], Iteration [120/1000000], G/loss_id: 0.2588, G/loss_id_psnt: 0.2765, G/loss_cd: 0.0477\n",
            "Elapsed [0:00:11], Iteration [130/1000000], G/loss_id: 0.2523, G/loss_id_psnt: 0.2886, G/loss_cd: 0.0457\n",
            "Elapsed [0:00:12], Iteration [140/1000000], G/loss_id: 0.2203, G/loss_id_psnt: 0.3287, G/loss_cd: 0.0473\n",
            "Elapsed [0:00:13], Iteration [150/1000000], G/loss_id: 0.2647, G/loss_id_psnt: 0.2743, G/loss_cd: 0.0397\n",
            "Elapsed [0:00:13], Iteration [160/1000000], G/loss_id: 0.2790, G/loss_id_psnt: 0.2700, G/loss_cd: 0.0364\n",
            "Elapsed [0:00:14], Iteration [170/1000000], G/loss_id: 0.2611, G/loss_id_psnt: 0.2828, G/loss_cd: 0.0434\n",
            "Elapsed [0:00:15], Iteration [180/1000000], G/loss_id: 0.2846, G/loss_id_psnt: 0.2536, G/loss_cd: 0.0420\n",
            "Elapsed [0:00:15], Iteration [190/1000000], G/loss_id: 0.2532, G/loss_id_psnt: 0.2790, G/loss_cd: 0.0421\n",
            "Elapsed [0:00:16], Iteration [200/1000000], G/loss_id: 0.2463, G/loss_id_psnt: 0.2816, G/loss_cd: 0.0387\n",
            "Elapsed [0:00:17], Iteration [210/1000000], G/loss_id: 0.2405, G/loss_id_psnt: 0.2873, G/loss_cd: 0.0329\n",
            "Elapsed [0:00:18], Iteration [220/1000000], G/loss_id: 0.2572, G/loss_id_psnt: 0.2663, G/loss_cd: 0.0311\n",
            "Elapsed [0:00:18], Iteration [230/1000000], G/loss_id: 0.2638, G/loss_id_psnt: 0.2594, G/loss_cd: 0.0261\n",
            "Elapsed [0:00:19], Iteration [240/1000000], G/loss_id: 0.2529, G/loss_id_psnt: 0.2757, G/loss_cd: 0.0251\n",
            "Elapsed [0:00:20], Iteration [250/1000000], G/loss_id: 0.2597, G/loss_id_psnt: 0.2663, G/loss_cd: 0.0269\n",
            "Elapsed [0:00:21], Iteration [260/1000000], G/loss_id: 0.2722, G/loss_id_psnt: 0.2426, G/loss_cd: 0.0236\n",
            "Elapsed [0:00:21], Iteration [270/1000000], G/loss_id: 0.2631, G/loss_id_psnt: 0.2610, G/loss_cd: 0.0225\n",
            "Elapsed [0:00:22], Iteration [280/1000000], G/loss_id: 0.2674, G/loss_id_psnt: 0.2698, G/loss_cd: 0.0250\n",
            "Elapsed [0:00:23], Iteration [290/1000000], G/loss_id: 0.2780, G/loss_id_psnt: 0.2457, G/loss_cd: 0.0221\n",
            "Elapsed [0:00:24], Iteration [300/1000000], G/loss_id: 0.2800, G/loss_id_psnt: 0.2496, G/loss_cd: 0.0222\n",
            "Elapsed [0:00:24], Iteration [310/1000000], G/loss_id: 0.2651, G/loss_id_psnt: 0.2577, G/loss_cd: 0.0252\n",
            "Elapsed [0:00:25], Iteration [320/1000000], G/loss_id: 0.2438, G/loss_id_psnt: 0.2767, G/loss_cd: 0.0235\n",
            "Elapsed [0:00:26], Iteration [330/1000000], G/loss_id: 0.2526, G/loss_id_psnt: 0.2655, G/loss_cd: 0.0216\n",
            "Elapsed [0:00:27], Iteration [340/1000000], G/loss_id: 0.2494, G/loss_id_psnt: 0.2769, G/loss_cd: 0.0172\n",
            "Elapsed [0:00:27], Iteration [350/1000000], G/loss_id: 0.2594, G/loss_id_psnt: 0.2600, G/loss_cd: 0.0207\n",
            "Elapsed [0:00:28], Iteration [360/1000000], G/loss_id: 0.2325, G/loss_id_psnt: 0.2839, G/loss_cd: 0.0197\n",
            "Elapsed [0:00:29], Iteration [370/1000000], G/loss_id: 0.2647, G/loss_id_psnt: 0.2466, G/loss_cd: 0.0212\n",
            "Elapsed [0:00:29], Iteration [380/1000000], G/loss_id: 0.2473, G/loss_id_psnt: 0.2719, G/loss_cd: 0.0158\n",
            "Elapsed [0:00:30], Iteration [390/1000000], G/loss_id: 0.2658, G/loss_id_psnt: 0.2495, G/loss_cd: 0.0180\n",
            "Elapsed [0:00:31], Iteration [400/1000000], G/loss_id: 0.2526, G/loss_id_psnt: 0.2548, G/loss_cd: 0.0138\n",
            "Elapsed [0:00:32], Iteration [410/1000000], G/loss_id: 0.2568, G/loss_id_psnt: 0.2506, G/loss_cd: 0.0128\n",
            "Elapsed [0:00:32], Iteration [420/1000000], G/loss_id: 0.2529, G/loss_id_psnt: 0.2594, G/loss_cd: 0.0150\n",
            "Elapsed [0:00:33], Iteration [430/1000000], G/loss_id: 0.2664, G/loss_id_psnt: 0.2415, G/loss_cd: 0.0152\n",
            "Elapsed [0:00:34], Iteration [440/1000000], G/loss_id: 0.2394, G/loss_id_psnt: 0.2747, G/loss_cd: 0.0153\n",
            "Elapsed [0:00:35], Iteration [450/1000000], G/loss_id: 0.2472, G/loss_id_psnt: 0.2697, G/loss_cd: 0.0104\n",
            "Elapsed [0:00:35], Iteration [460/1000000], G/loss_id: 0.2525, G/loss_id_psnt: 0.2528, G/loss_cd: 0.0125\n",
            "Elapsed [0:00:36], Iteration [470/1000000], G/loss_id: 0.2430, G/loss_id_psnt: 0.2497, G/loss_cd: 0.0143\n",
            "Elapsed [0:00:37], Iteration [480/1000000], G/loss_id: 0.2565, G/loss_id_psnt: 0.2454, G/loss_cd: 0.0116\n",
            "Elapsed [0:00:38], Iteration [490/1000000], G/loss_id: 0.2553, G/loss_id_psnt: 0.2460, G/loss_cd: 0.0080\n",
            "Elapsed [0:00:38], Iteration [500/1000000], G/loss_id: 0.2472, G/loss_id_psnt: 0.2569, G/loss_cd: 0.0124\n",
            "Elapsed [0:00:39], Iteration [510/1000000], G/loss_id: 0.2609, G/loss_id_psnt: 0.2396, G/loss_cd: 0.0085\n",
            "Elapsed [0:00:40], Iteration [520/1000000], G/loss_id: 0.2437, G/loss_id_psnt: 0.2586, G/loss_cd: 0.0110\n",
            "Elapsed [0:00:41], Iteration [530/1000000], G/loss_id: 0.2449, G/loss_id_psnt: 0.2479, G/loss_cd: 0.0124\n",
            "Elapsed [0:00:41], Iteration [540/1000000], G/loss_id: 0.2560, G/loss_id_psnt: 0.2428, G/loss_cd: 0.0086\n",
            "Elapsed [0:00:42], Iteration [550/1000000], G/loss_id: 0.2471, G/loss_id_psnt: 0.2483, G/loss_cd: 0.0080\n",
            "Elapsed [0:00:43], Iteration [560/1000000], G/loss_id: 0.2383, G/loss_id_psnt: 0.2562, G/loss_cd: 0.0077\n",
            "Elapsed [0:00:43], Iteration [570/1000000], G/loss_id: 0.2468, G/loss_id_psnt: 0.2550, G/loss_cd: 0.0109\n",
            "Elapsed [0:00:44], Iteration [580/1000000], G/loss_id: 0.2459, G/loss_id_psnt: 0.2473, G/loss_cd: 0.0093\n",
            "Elapsed [0:00:45], Iteration [590/1000000], G/loss_id: 0.2541, G/loss_id_psnt: 0.2374, G/loss_cd: 0.0108\n",
            "Elapsed [0:00:46], Iteration [600/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2624, G/loss_cd: 0.0123\n",
            "Elapsed [0:00:46], Iteration [610/1000000], G/loss_id: 0.2562, G/loss_id_psnt: 0.2398, G/loss_cd: 0.0074\n",
            "Elapsed [0:00:47], Iteration [620/1000000], G/loss_id: 0.2481, G/loss_id_psnt: 0.2542, G/loss_cd: 0.0065\n",
            "Elapsed [0:00:48], Iteration [630/1000000], G/loss_id: 0.2398, G/loss_id_psnt: 0.2477, G/loss_cd: 0.0069\n",
            "Elapsed [0:00:49], Iteration [640/1000000], G/loss_id: 0.2508, G/loss_id_psnt: 0.2420, G/loss_cd: 0.0060\n",
            "Elapsed [0:00:50], Iteration [650/1000000], G/loss_id: 0.2273, G/loss_id_psnt: 0.2653, G/loss_cd: 0.0111\n",
            "Elapsed [0:00:52], Iteration [660/1000000], G/loss_id: 0.2374, G/loss_id_psnt: 0.2550, G/loss_cd: 0.0085\n",
            "Elapsed [0:00:53], Iteration [670/1000000], G/loss_id: 0.2422, G/loss_id_psnt: 0.2424, G/loss_cd: 0.0090\n",
            "Elapsed [0:00:54], Iteration [680/1000000], G/loss_id: 0.2274, G/loss_id_psnt: 0.2537, G/loss_cd: 0.0079\n",
            "Elapsed [0:00:54], Iteration [690/1000000], G/loss_id: 0.2479, G/loss_id_psnt: 0.2333, G/loss_cd: 0.0066\n",
            "Elapsed [0:00:55], Iteration [700/1000000], G/loss_id: 0.2387, G/loss_id_psnt: 0.2464, G/loss_cd: 0.0062\n",
            "Elapsed [0:00:56], Iteration [710/1000000], G/loss_id: 0.2302, G/loss_id_psnt: 0.2557, G/loss_cd: 0.0055\n",
            "Elapsed [0:00:57], Iteration [720/1000000], G/loss_id: 0.2448, G/loss_id_psnt: 0.2371, G/loss_cd: 0.0056\n",
            "Elapsed [0:00:57], Iteration [730/1000000], G/loss_id: 0.2340, G/loss_id_psnt: 0.2421, G/loss_cd: 0.0061\n",
            "Elapsed [0:00:58], Iteration [740/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2475, G/loss_cd: 0.0050\n",
            "Elapsed [0:00:59], Iteration [750/1000000], G/loss_id: 0.2380, G/loss_id_psnt: 0.2407, G/loss_cd: 0.0059\n",
            "Elapsed [0:01:00], Iteration [760/1000000], G/loss_id: 0.2481, G/loss_id_psnt: 0.2261, G/loss_cd: 0.0074\n",
            "Elapsed [0:01:00], Iteration [770/1000000], G/loss_id: 0.2324, G/loss_id_psnt: 0.2451, G/loss_cd: 0.0056\n",
            "Elapsed [0:01:01], Iteration [780/1000000], G/loss_id: 0.2411, G/loss_id_psnt: 0.2342, G/loss_cd: 0.0061\n",
            "Elapsed [0:01:02], Iteration [790/1000000], G/loss_id: 0.2349, G/loss_id_psnt: 0.2451, G/loss_cd: 0.0043\n",
            "Elapsed [0:01:03], Iteration [800/1000000], G/loss_id: 0.2389, G/loss_id_psnt: 0.2346, G/loss_cd: 0.0037\n",
            "Elapsed [0:01:03], Iteration [810/1000000], G/loss_id: 0.2366, G/loss_id_psnt: 0.2310, G/loss_cd: 0.0061\n",
            "Elapsed [0:01:04], Iteration [820/1000000], G/loss_id: 0.2197, G/loss_id_psnt: 0.2430, G/loss_cd: 0.0049\n",
            "Elapsed [0:01:05], Iteration [830/1000000], G/loss_id: 0.2420, G/loss_id_psnt: 0.2348, G/loss_cd: 0.0036\n",
            "Elapsed [0:01:06], Iteration [840/1000000], G/loss_id: 0.2390, G/loss_id_psnt: 0.2397, G/loss_cd: 0.0049\n",
            "Elapsed [0:01:07], Iteration [850/1000000], G/loss_id: 0.2419, G/loss_id_psnt: 0.2327, G/loss_cd: 0.0041\n",
            "Elapsed [0:01:08], Iteration [860/1000000], G/loss_id: 0.2284, G/loss_id_psnt: 0.2467, G/loss_cd: 0.0030\n",
            "Elapsed [0:01:08], Iteration [870/1000000], G/loss_id: 0.2312, G/loss_id_psnt: 0.2273, G/loss_cd: 0.0060\n",
            "Elapsed [0:01:09], Iteration [880/1000000], G/loss_id: 0.2258, G/loss_id_psnt: 0.2397, G/loss_cd: 0.0046\n",
            "Elapsed [0:01:10], Iteration [890/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2282, G/loss_cd: 0.0041\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-04e12e321f52>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0a75626680c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg_loss_id_psnt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_cd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_loss_cd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 보코더\n"
      ],
      "metadata": {
        "id": "PerBW2C-hF6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import soundfile as sf\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from wavenet_vocoder import builder"
      ],
      "metadata": {
        "id": "7PWLPDEzjfO_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hparams.py\n",
        "# NOTE: If you want full control for model architecture. please take a look\n",
        "# at the code and change whatever you want. Some hyper parameters are hardcoded.\n",
        "\n",
        "\n",
        "class Map(dict):\n",
        "\t\"\"\"\n",
        "    Example:\n",
        "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
        "\n",
        "    Credits to epool:\n",
        "    https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, *args, **kwargs):\n",
        "\t\tsuper(Map, self).__init__(*args, **kwargs)\n",
        "\t\tfor arg in args:\n",
        "\t\t\tif isinstance(arg, dict):\n",
        "\t\t\t\tfor k, v in arg.items():\n",
        "\t\t\t\t\tself[k] = v\n",
        "\n",
        "\t\tif kwargs:\n",
        "\t\t\tfor k, v in kwargs.iteritems():\n",
        "\t\t\t\tself[k] = v\n",
        "\n",
        "\tdef __getattr__(self, attr):\n",
        "\t\treturn self.get(attr)\n",
        "\n",
        "\tdef __setattr__(self, key, value):\n",
        "\t\tself.__setitem__(key, value)\n",
        "\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tsuper(Map, self).__setitem__(key, value)\n",
        "\t\tself.__dict__.update({key: value})\n",
        "\n",
        "\tdef __delattr__(self, item):\n",
        "\t\tself.__delitem__(item)\n",
        "\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tsuper(Map, self).__delitem__(key)\n",
        "\t\tdel self.__dict__[key]\n",
        "\n",
        "\n",
        "# Default hyperparameters:\n",
        "hparams = Map({ #하이퍼 파라미터 묶음\n",
        "\t'name': \"wavenet_vocoder\",\n",
        "\n",
        "\t# Convenient model builder\n",
        "\t'builder': \"wavenet\",\n",
        "\n",
        "\t# Input type:\n",
        "\t# 1. raw [-1, 1]\n",
        "\t# 2. mulaw [-1, 1]\n",
        "\t# 3. mulaw-quantize [0, mu]\n",
        "\t# If input_type is raw or mulaw, network assumes scalar input and\n",
        "\t# discretized mixture of logistic distributions output, otherwise one-hot\n",
        "\t# input and softmax output are assumed.\n",
        "\t# **NOTE**: if you change the one of the two parameters below, you need to\n",
        "\t# re-run preprocessing before training.\n",
        "\t'input_type': \"raw\",\n",
        "\t'quantize_channels': 65536,  # 65536 or 256\n",
        "\n",
        "\t# Audio:\n",
        "\t'sample_rate': 16000,\n",
        "\t# this is only valid for mulaw is True\n",
        "\t'silence_threshold': 2,\n",
        "\t'num_mels': 80,\n",
        "\t'fmin': 125,\n",
        "\t'fmax': 7600,\n",
        "\t'fft_size': 1024,\n",
        "\t# shift can be specified by either hop_size or frame_shift_ms\n",
        "\t'hop_size': 256,\n",
        "\t'frame_shift_ms': None,\n",
        "\t'min_level_db': -100,\n",
        "\t'ref_level_db': 20,\n",
        "\t# whether to rescale waveform or not.\n",
        "\t# Let x is an input waveform, rescaled waveform y is given by:\n",
        "\t# y = x / np.abs(x).max() * rescaling_max\n",
        "\t'rescaling': True,\n",
        "\t'rescaling_max': 0.999,\n",
        "\t# mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n",
        "\t# happen depends on min_level_db and ref_level_db, causing clipping noise.\n",
        "\t# If False, assertion is added to ensure no clipping happens.o0\n",
        "\t'allow_clipping_in_normalization': True,\n",
        "\n",
        "\t# Mixture of logistic distributions:\n",
        "\t'log_scale_min': float(-32.23619130191664),\n",
        "\n",
        "\t# Model:\n",
        "\t# This should equal to `quantize_channels` if mu-law quantize enabled\n",
        "\t# otherwise num_mixture * 3 (pi, mean, log_scale)\n",
        "\t'out_channels': 10 * 3,\n",
        "\t'layers': 24,\n",
        "\t'stacks': 4,\n",
        "\t'residual_channels': 512,\n",
        "\t'gate_channels': 512,  # split into 2 gropus internally for gated activation\n",
        "\t'skip_out_channels': 256,\n",
        "\t'dropout': 1 - 0.95,\n",
        "\t'kernel_size': 3,\n",
        "\t# If True, apply weight normalization as same as DeepVoice3\n",
        "\t'weight_normalization': True,\n",
        "\t# Use legacy code or not. Default is True since we already provided a model\n",
        "\t# based on the legacy code that can generate high-quality audio.\n",
        "\t# Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n",
        "\t'legacy': True,\n",
        "\n",
        "\t# Local conditioning (set negative value to disable))\n",
        "\t'cin_channels': 80,\n",
        "\t# If True, use transposed convolutions to upsample conditional features,\n",
        "\t# otherwise repeat features to adjust time resolution\n",
        "\t'upsample_conditional_features': True,\n",
        "\t# should np.prod(upsample_scales) == hop_size\n",
        "\t'upsample_scales': [4, 4, 4, 4],\n",
        "\t# Freq axis kernel size for upsampling network\n",
        "\t'freq_axis_kernel_size': 3,\n",
        "\n",
        "\t# Global conditioning (set negative value to disable)\n",
        "\t# currently limited for speaker embedding\n",
        "\t# this should only be enabled for multi-speaker dataset\n",
        "\t'gin_channels': -1,  # i.e., speaker embedding dim\n",
        "\t'n_speakers': -1,\n",
        "\n",
        "\t# Data loader\n",
        "\t'pin_memory': True,\n",
        "\t'num_workers': 2,\n",
        "\n",
        "\t# train/test\n",
        "\t# test size can be specified as portion or num samples\n",
        "\t'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n",
        "\t'test_num_samples': None,\n",
        "\t'random_state': 1234,\n",
        "\n",
        "\t# Loss\n",
        "\n",
        "\t# Training:\n",
        "\t'batch_size': 2,\n",
        "\t'adam_beta1': 0.9,\n",
        "\t'adam_beta2': 0.999,\n",
        "\t'adam_eps': 1e-8,\n",
        "\t'amsgrad': False,\n",
        "\t'initial_learning_rate': 1e-3,\n",
        "\t# see lrschedule.py for available lr_schedule\n",
        "\t'lr_schedule': \"noam_learning_rate_decay\",\n",
        "\t'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n",
        "\t'nepochs': 2000,\n",
        "\t'weight_decay': 0.0,\n",
        "\t'clip_thresh': -1,\n",
        "\t# max time steps can either be specified as sec or steps\n",
        "\t# if both are None, then full audio samples are used in a batch\n",
        "\t'max_time_sec': None,\n",
        "\t'max_time_steps': 8000,\n",
        "\t# Hold moving averaged parameters and use them for evaluation\n",
        "\t'exponential_moving_average': True,\n",
        "\t# averaged = decay * averaged + (1 - decay) * x\n",
        "\t'ema_decay': 0.9999,\n",
        "\n",
        "\t# Save\n",
        "\t# per-step intervals\n",
        "\t'checkpoint_interval': 10000,\n",
        "\t'train_eval_interval': 10000,\n",
        "\t# per-epoch interval\n",
        "\t'test_eval_epoch_interval': 5,\n",
        "\t'save_optimizer_state': True,\n",
        "\n",
        "\t# Eval:\n",
        "})\n",
        "\n",
        "\n",
        "# def hparams_debug_string():\n",
        "# \tvalues = hparams.values()\n",
        "# \thp = ['  %s: %s' % (name, values[name]) for name in sorted(values)]\n",
        "# \treturn 'Hyperparameters:\\n' + '\\n'.join(hp)"
      ],
      "metadata": {
        "id": "IKvwjnd0bTi4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "9oqr0SaMjmT2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(): #모델 생성\n",
        "\n",
        "    model = getattr(builder, hparams.builder)(\n",
        "        out_channels=hparams.out_channels,\n",
        "        layers=hparams.layers,\n",
        "        stacks=hparams.stacks,\n",
        "        residual_channels=hparams.residual_channels,\n",
        "        gate_channels=hparams.gate_channels,\n",
        "        skip_out_channels=hparams.skip_out_channels,\n",
        "        cin_channels=hparams.cin_channels,\n",
        "        gin_channels=hparams.gin_channels,\n",
        "        weight_normalization=hparams.weight_normalization,\n",
        "        n_speakers=hparams.n_speakers,\n",
        "        dropout=hparams.dropout,\n",
        "        kernel_size=hparams.kernel_size,\n",
        "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
        "        upsample_scales=hparams.upsample_scales,\n",
        "        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n",
        "        scalar_input=True,\n",
        "        legacy=hparams.legacy,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def wavegen(model, c=None, tqdm=tqdm): #파형 생성\n",
        "    \"\"\"Generate waveform samples by WaveNet.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.make_generation_fast_()\n",
        "\n",
        "    Tc = c.shape[0]\n",
        "    upsample_factor = hparams.hop_size\n",
        "    # Overwrite length according to feature size\n",
        "    length = Tc * upsample_factor\n",
        "\n",
        "    # B x C x T\n",
        "    c = torch.FloatTensor(c.T).unsqueeze(0)\n",
        "\n",
        "    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n",
        "\n",
        "    # Transform data to GPU\n",
        "    initial_input = initial_input.to(device)\n",
        "    c = None if c is None else c.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = model.incremental_forward(\n",
        "            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
        "            log_scale_min=hparams.log_scale_min)\n",
        "\n",
        "    y_hat = y_hat.view(-1).cpu().data.numpy()\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "dOF9y4XucFVa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#synthesis.py\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Synthesis waveform from trained WaveNet.\n",
        "\n",
        "Modified from https://github.com/r9y9/wavenet_vocoder\n",
        "\"\"\"\n",
        "\n",
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
      ],
      "metadata": {
        "id": "etqieCP-7Xtb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
        "device = torch.device(\"cuda\")\n",
        "model = build_model().to(device)\n",
        "checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "for spect in spect_vc:\n",
        "    name = spect[0]\n",
        "    c = spect[1]\n",
        "    print(name)\n",
        "    waveform = wavegen(model, c=c)\n",
        "    sf.write(name+'.wav', waveform, samplerate=16000)"
      ],
      "metadata": {
        "id": "vG1BpILHjppf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ce4a71-606e-4fe8-f3bf-668b6fd5ecfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "<ipython-input-26-a6ad4607df90>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [04:54<00:00, 78.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [04:50<00:00, 79.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 8641/23040 [01:48<02:40, 89.45it/s]"
          ]
        }
      ]
    }
  ]
}