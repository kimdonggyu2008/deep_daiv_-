{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/auto_vc_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2Vzcr775Ib",
        "outputId": "6b911a19-8689-414f-e948-1c0c9fb446c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAKmnK5n6Gez",
        "outputId": "9d9fd19b-8cfd-4256-b2bf-31f3bda68b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wavenet_vocoder\n",
            "  Downloading wavenet_vocoder-0.1.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.13.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->wavenet_vocoder) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->wavenet_vocoder) (1.3.0)\n",
            "Building wheels for collected packages: wavenet_vocoder\n",
            "  Building wheel for wavenet_vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet_vocoder: filename=wavenet_vocoder-0.1.1-py3-none-any.whl size=12642 sha256=6255ce3ee820c0e1a36525e8ee47f38025d92ff60811bd623084c64a8640e3e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/a4/7b/f1d21f96be36a13e9c3948e8c28792bf8962da19781abd9dc8\n",
            "Successfully built wavenet_vocoder\n",
            "Installing collected packages: wavenet_vocoder\n",
            "Successfully installed wavenet_vocoder-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wavenet_vocoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8YKdCC3ciLI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V34cp_HHalfN"
      },
      "outputs": [],
      "source": [
        "#make_spect.py\n",
        "def butter_highpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq #차단할 주파수 대역 지정\n",
        "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    return b, a #버터워스 고역 필터 계수\n",
        "\n",
        "\n",
        "def pySTFT(x, fft_length=1024, hop_length=256): #stft실행\n",
        "\n",
        "    x = np.pad(x, int(fft_length//2), mode='reflect') #패딩, 불연속성 완화\n",
        "\n",
        "    noverlap = fft_length - hop_length #중첩된 샘플 수 계산\n",
        "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)#??\n",
        "    strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])#스트라이드 갯수 계산\n",
        "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "                                             strides=strides)#?? 윈도우변환??\n",
        "\n",
        "    fft_window = get_window('hann', fft_length, fftbins=True) #해닝 창 함수 적용\n",
        "    result = np.fft.rfft(fft_window * result, n=fft_length).T #ftf후 rftf적용\n",
        "\n",
        "    return np.abs(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc3uiuq89uh4",
        "outputId": "48d4d8fb-8209-4ebb-823a-fe21bc27de05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs\n",
            "p225\n",
            "p226\n",
            "p227\n",
            "p228\n"
          ]
        }
      ],
      "source": [
        "mel_basis = mel(sr=16000,n_fft=1024,  n_mels=80, fmin=90, fmax=7600).T #멜 스펙트로그램 생성\n",
        "#신호의 저주파 성분 제거\n",
        "min_level = np.exp(-100 / 20 * np.log(10))\n",
        "#작은 신호를 0말고 최솟값으로 변환\n",
        "b, a = butter_highpass(30, 16000, order=5)\n",
        "#고역필터 계수 지정\n",
        "\n",
        "# audio file directory\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'\n",
        "\n",
        "# audio file directory\n",
        "# rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "# targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/results'\n",
        "\n",
        "#  audio file directory\n",
        "# rootDir = './wavs'\n",
        "# # spectrogram directory\n",
        "# targetDir = './spmel'\n",
        "\n",
        "\n",
        "\n",
        "dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "for subdir in sorted(subdirList): #데이터 파일 확인\n",
        "    print(subdir)\n",
        "    if not os.path.exists(os.path.join(targetDir, subdir)):#타겟 디렉토리 없으면 만듦\n",
        "        os.makedirs(os.path.join(targetDir, subdir))\n",
        "    _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))#순차적 방문\n",
        "    prng = RandomState(int(subdir[1:]))\n",
        "    for fileName in sorted(fileList):\n",
        "        # Read audio file\n",
        "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))#오디오파일 읽어오기\n",
        "        # Remove drifting noise\n",
        "        y = signal.filtfilt(b, a, x)#만든 고역필터 적용\n",
        "        # Ddd a little random noise for model roubstness\n",
        "        wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
        "        # Compute spect\n",
        "        D = pySTFT(wav).T #stft실행 후 스펙트로그램 생성\n",
        "        # Convert to mel and normalize\n",
        "        D_mel = np.dot(D, mel_basis)#스펙트로그램과 필터 뱅크 곱함 ??\n",
        "        D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16 #최소값 지정\n",
        "        S = np.clip((D_db + 100) / 100, 0, 1)#결과값을 0~1로 제한해서 클리핑 제한\n",
        "        # save spect\n",
        "        np.save(os.path.join(targetDir, subdir, fileName[:-4]),\n",
        "                S.astype(np.float32), allow_pickle=False)#멜스펙트로그램 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i515s4B4a9Jl"
      },
      "outputs": [],
      "source": [
        "#model_bl.py\n",
        "\n",
        "class D_VECTOR(nn.Module):\n",
        "    \"\"\"d vector speaker embedding.\"\"\"\n",
        "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
        "        super(D_VECTOR, self).__init__()#모듈의 d_vector 상속\n",
        "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()#batch_size, sequence_len, n_feature\n",
        "        lstm_out, _ = self.lstm(x)#batch_size,sequence_len,hidden_size\n",
        "        embeds = self.embedding(lstm_out[:,-1,:])#sequence_len의 마지막을 임베딩으로 변환\n",
        "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        embeds_normalized = embeds.div(norm)#batch_size, dim_emb\n",
        "        return embeds_normalized#batch_size, dim_Emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KziVBX5dbD9M",
        "outputId": "1c62d0f9-5033-471b-a70b-8adcb8d1b698"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-704b09355319>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel\n",
            "Processing speaker: p225\n",
            "Processing speaker: p226\n",
            "Processing speaker: p227\n",
            "Processing speaker: p228\n"
          ]
        }
      ],
      "source": [
        "#make_metadata.py\n",
        "\"\"\"\n",
        "Generate speaker embeddings and metadata for training\n",
        "\"\"\"\n",
        "\n",
        "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda() #화자별 임베딩\n",
        "c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n",
        "# c_checkpoint = torch.load('3000000-BL.ckpt')\n",
        "new_state_dict = OrderedDict()\n",
        "for key, val in c_checkpoint['model_b'].items():\n",
        "    new_key = key[7:]\n",
        "    new_state_dict[new_key] = val\n",
        "C.load_state_dict(new_state_dict)\n",
        "num_uttrs = 10\n",
        "len_crop = 128\n",
        "\n",
        "\n",
        "# Directory containing mel-spectrograms, 별도 데이터셋?\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'#?\n",
        "# rootDir = './spmel'\n",
        "dirName, subdirList, _ = next(os.walk(rootDir)) #데이터셋 파일 필요함\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "speakers = [] #화자 전체의 임베딩 목록\n",
        "for speaker in sorted(subdirList):\n",
        "    print('Processing speaker: %s' % speaker)\n",
        "    utterances = [] #단일 화자별 임베딩을 저장\n",
        "    utterances.append(speaker)\n",
        "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))#하나씩 추출개시\n",
        "\n",
        "    # make speaker embedding\n",
        "    assert len(fileList) >= num_uttrs\n",
        "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
        "    embs = []\n",
        "    for i in range(num_uttrs):\n",
        "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
        "        #디렉토리, 화자, 화자별 파일 순으로 로딩\n",
        "        candidates = np.delete(np.arange(len(fileList)), idx_uttrs)\n",
        "        # choose another utterance if the current one is too short\n",
        "        while tmp.shape[0] < len_crop:#지정 길이보다 짧으면 다른걸로 고름\n",
        "            idx_alt = np.random.choice(candidates)\n",
        "            tmp = np.load(os.path.join(dirName, speaker, fileList[idx_alt]))\n",
        "            candidates = np.delete(candidates, np.argwhere(candidates==idx_alt))\n",
        "        left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
        "        melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :]).cuda()\n",
        "        #주어진 간격(len_crop)만큼만 잘라서 학습에 활용\n",
        "        emb = C(melsp)#해당 부분을 임베딩\n",
        "        embs.append(emb.detach().squeeze().cpu().numpy())#해당 임베딩들 수집\n",
        "    utterances.append(np.mean(embs, axis=0))#평균값으로 묶어서 화자 정보로 추가\n",
        "\n",
        "    # create file list\n",
        "    for fileName in sorted(fileList):#각 화자에 대한 정보 파일로 저장\n",
        "        utterances.append(os.path.join(speaker,fileName))\n",
        "    speakers.append(utterances)\n",
        "\n",
        "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
        "    pickle.dump(speakers, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSI7SwAXbfj_"
      },
      "outputs": [],
      "source": [
        "#data_loader.py\n",
        "\n",
        "\n",
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, len_crop):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.len_crop = len_crop\n",
        "        self.step = 10\n",
        "\n",
        "        metaname = os.path.join(self.root_dir, \"train.pkl\") #메타데이터 읽어오기\n",
        "        meta = pickle.load(open(metaname, \"rb\"))\n",
        "\n",
        "        \"\"\"Load data using multiprocessing\"\"\"\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))#프로세스를 적용할 파일들\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:#프로세스 적용 후 저장\n",
        "            p.join()\n",
        "\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the dataset...')\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):#메타데이터 일부, 데이터셋 리스트, 인덱스 오프셋\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]#각 화자별로 넣을 수 있는 none칸 생성\n",
        "            for j, tmp in enumerate(sbmt):#화자별로 ID 임베딩 저장\n",
        "                if j < 2:  # fill in speaker id and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else: # load the mel-spectrograms\n",
        "\n",
        "                    #해당 화자의 멜스펙트로그램 읽어옴\n",
        "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp),allow_pickle=True)\n",
        "                    #uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
        "\n",
        "\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):#데이터셋의 인덱스\n",
        "        # pick a random speaker\n",
        "        dataset = self.train_dataset\n",
        "        list_uttrs = dataset[index] #데이터셋 상에서 해당 인덱스를 가지는 화자 검색\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        # pick random uttr with random crop\n",
        "        a = np.random.randint(2, len(list_uttrs)) #무작위 화자 선택\n",
        "        tmp = list_uttrs[a]\n",
        "        if tmp.shape[0] < self.len_crop: #추출 길이보다 샘플이 짧음\n",
        "            len_pad = self.len_crop - tmp.shape[0]#필요 길이까지 0으로 패딩 추가\n",
        "            uttr = np.pad(tmp, ((0,len_pad),(0,0)), 'constant')\n",
        "        elif tmp.shape[0] > self.len_crop:\n",
        "            left = np.random.randint(tmp.shape[0]-self.len_crop)#랜덤위치 잡고 LEN_CROP길이 가져옴\n",
        "            uttr = tmp[left:left+self.len_crop, :]\n",
        "        else:\n",
        "            uttr = tmp#같으면 전체 반환\n",
        "\n",
        "        return uttr, emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens #샘플의 토큰 갯수 반환\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size=16, len_crop=128, num_workers=0):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(root_dir, len_crop)\n",
        "\n",
        "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  worker_init_fn=worker_init_fn)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6zEd5YR_knf"
      },
      "source": [
        "# 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqCzLhYybRMT"
      },
      "outputs": [],
      "source": [
        "#model_vc.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):#가중치 설정용 선형 놈\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(#가중치 분산이 입력데이터 갯수에 반비례하게 초기화됨\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module): #합성곱 놈\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)#커널 사이즈에 따라 패딩 조정, 크기 유지\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels, #1D 컨볼루션 층, 시간따라 바뀌는것만 보면 됨\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(#초기화\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class Encoder(nn.Module): #인코더 모델\n",
        "    \"\"\"Encoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, freq):#보틀넥 구조 디멘션 갯수\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_neck = dim_neck\n",
        "        self.freq = freq\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(#컨볼루션 3개 넣음\n",
        "                ConvNorm(80+dim_emb if i==0 else 512,#입력이 주파수 대역 갯수 + 화자임베딩\n",
        "                        #또는 원 크기?\n",
        "                         512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(512))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
        "        #쌍방 LSTM\n",
        "\n",
        "    def forward(self, x, c_org):#배치 사이즈, 1, 시간 프레임 T, 주파수대역 갯수 80\n",
        "        #C_ORG = 화자 임베딩 1차원 스칼라\n",
        "        x = x.squeeze(1).transpose(2,1)#배치사이즈, 주파수 대역, 시간 프래임\n",
        "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))#화자,화자, 시간프레임\n",
        "        x = torch.cat((x, c_org), dim=1)#\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "        out_forward = outputs[:, :, :self.dim_neck]\n",
        "        out_backward = outputs[:, :, self.dim_neck:]\n",
        "\n",
        "        codes = []\n",
        "        for i in range(0, outputs.size(1), self.freq):\n",
        "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
        "\n",
        "        return codes\n",
        "\n",
        "\n",
        "class Decoder(nn.Module): #디코더 모델\n",
        "    \"\"\"Decoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(dim_pre,\n",
        "                         dim_pre,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(dim_pre))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
        "\n",
        "        self.linear_projection = LinearNorm(1024, 80)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #self.lstm1.flatten_parameters()\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        outputs, _ = self.lstm2(x)\n",
        "\n",
        "        decoder_output = self.linear_projection(outputs)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "class Postnet(nn.Module): #포스트넷?\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(80, 512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(512))\n",
        "        )\n",
        "\n",
        "        for i in range(1, 5 - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(512,\n",
        "                             512,\n",
        "                             kernel_size=5, stride=1,\n",
        "                             padding=2,\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(512))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(512, 80,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(80))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = torch.tanh(self.convolutions[i](x))\n",
        "\n",
        "        x = self.convolutions[-1](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator(nn.Module): #생성기 = 인코더+디코더+포스트넷\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
        "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, x, c_org, c_trg):\n",
        "\n",
        "        codes = self.encoder(x, c_org)\n",
        "        if c_trg is None:\n",
        "            return torch.cat(codes, dim=-1)\n",
        "\n",
        "        tmp = []\n",
        "        for code in codes:\n",
        "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
        "        code_exp = torch.cat(tmp, dim=1)\n",
        "\n",
        "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
        "\n",
        "        mel_outputs = self.decoder(encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
        "\n",
        "        mel_outputs = mel_outputs.unsqueeze(1)\n",
        "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAX8yLy_sDv"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnlPJX6JK57d"
      },
      "outputs": [],
      "source": [
        "#main.py\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ6SWuZZfU08",
        "outputId": "ee43c42e-e7b0-427a-d626-9e88d4c497df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/코딩공부/deep_daiv/dataset\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/코딩공부/deep_daiv/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itRsZDeOg1f9"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmwCxNSLcN1e"
      },
      "outputs": [],
      "source": [
        "sys.argv=['']\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Model configuration.\n",
        "parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
        "parser.add_argument('--dim_neck', type=int, default=16)\n",
        "parser.add_argument('--dim_emb', type=int, default=256)\n",
        "parser.add_argument('--dim_pre', type=int, default=512)\n",
        "parser.add_argument('--freq', type=int, default=16)\n",
        "\n",
        "# Training configuration.\n",
        "parser.add_argument('--data_dir', type=str, default='./spmel')\n",
        "parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
        "parser.add_argument('--num_iters', type=int, default=1000000, help='number of total iterations')\n",
        "parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
        "\n",
        "# Miscellaneous.\n",
        "parser.add_argument('--log_step', type=int, default=10)\n",
        "\n",
        "config = parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHF3fFgzQVL9"
      },
      "outputs": [],
      "source": [
        "# from easydict import EasyDict as edict\n",
        "\n",
        "# config = edict()\n",
        "\n",
        "# # Model configuration.\n",
        "# config.lambda_cd = 1\n",
        "# config.dim_neck = 16\n",
        "# config.dim_emb = 256\n",
        "# config.dim_pre = 512\n",
        "# config.freq = 16\n",
        "\n",
        "# # Training configuration.\n",
        "# config.data_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# config.batch_size = 2\n",
        "# config.num_iters = 1000000\n",
        "# config.len_crop = 128\n",
        "\n",
        "# # Miscellaneous.\n",
        "# config.log_step = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXgGFoXnb0De"
      },
      "outputs": [],
      "source": [
        "#solver_encoder.py\n",
        "\n",
        "class Solver(object): #음성신호 학습\n",
        "\n",
        "    def __init__(self, vcc_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.vcc_loader = vcc_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.lambda_cd = config.lambda_cd\n",
        "        self.dim_neck = config.dim_neck\n",
        "        self.dim_emb = config.dim_emb\n",
        "        self.dim_pre = config.dim_pre\n",
        "        self.freq = config.freq\n",
        "\n",
        "        # Training configurations.\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
        "        self.log_step = config.log_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), 0.0001)\n",
        "\n",
        "        self.G.to(self.device)\n",
        "\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    #=====================================================================================================================================#\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # Set data loader.\n",
        "        data_loader = self.vcc_loader\n",
        "\n",
        "        # Print logs in specified order\n",
        "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Fetch data.\n",
        "            try:\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, emb_org = next(data_iter)\n",
        "\n",
        "\n",
        "\n",
        "            x_real = x_real.to(self.device)\n",
        "            emb_org = emb_org.to(self.device)\n",
        "\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               2. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            self.G = self.G.train()\n",
        "\n",
        "            # Identity mapping loss\n",
        "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
        "            g_loss_id = F.mse_loss(x_real, x_identic)\n",
        "            g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n",
        "\n",
        "            # Code semantic loss.\n",
        "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
        "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
        "\n",
        "\n",
        "            # Backward and optimize.\n",
        "            g_loss = g_loss_id + g_loss_id_psnt + self.lambda_cd * g_loss_cd\n",
        "            self.reset_grad()\n",
        "            g_loss.backward()\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['G/loss_id'] = g_loss_id.item()\n",
        "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
        "            loss['G/loss_cd'] = g_loss_cd.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag in keys:\n",
        "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
        "                print(log)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-tF8qGvMpXK",
        "outputId": "c7d2bc53-6852-485c-cca5-81f093ce58eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished loading the dataset...\n",
            "Start training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-0a75626680c8>:86: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  g_loss_id = F.mse_loss(x_real, x_identic)\n",
            "<ipython-input-19-0a75626680c8>:87: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed [0:00:02], Iteration [10/1000000], G/loss_id: 0.1532, G/loss_id_psnt: 1.1175, G/loss_cd: 0.1230\n",
            "Elapsed [0:00:03], Iteration [20/1000000], G/loss_id: 0.0594, G/loss_id_psnt: 0.9366, G/loss_cd: 0.1170\n",
            "Elapsed [0:00:04], Iteration [30/1000000], G/loss_id: 0.1137, G/loss_id_psnt: 0.6641, G/loss_cd: 0.1126\n",
            "Elapsed [0:00:04], Iteration [40/1000000], G/loss_id: 0.1895, G/loss_id_psnt: 0.4757, G/loss_cd: 0.1096\n",
            "Elapsed [0:00:05], Iteration [50/1000000], G/loss_id: 0.3117, G/loss_id_psnt: 0.2996, G/loss_cd: 0.0920\n",
            "Elapsed [0:00:06], Iteration [60/1000000], G/loss_id: 0.2179, G/loss_id_psnt: 0.3585, G/loss_cd: 0.0848\n",
            "Elapsed [0:00:07], Iteration [70/1000000], G/loss_id: 0.2552, G/loss_id_psnt: 0.3200, G/loss_cd: 0.0782\n",
            "Elapsed [0:00:07], Iteration [80/1000000], G/loss_id: 0.2784, G/loss_id_psnt: 0.2824, G/loss_cd: 0.0626\n",
            "Elapsed [0:00:08], Iteration [90/1000000], G/loss_id: 0.2482, G/loss_id_psnt: 0.3102, G/loss_cd: 0.0723\n",
            "Elapsed [0:00:09], Iteration [100/1000000], G/loss_id: 0.2492, G/loss_id_psnt: 0.3042, G/loss_cd: 0.0581\n",
            "Elapsed [0:00:10], Iteration [110/1000000], G/loss_id: 0.2723, G/loss_id_psnt: 0.2643, G/loss_cd: 0.0596\n",
            "Elapsed [0:00:10], Iteration [120/1000000], G/loss_id: 0.2588, G/loss_id_psnt: 0.2765, G/loss_cd: 0.0477\n",
            "Elapsed [0:00:11], Iteration [130/1000000], G/loss_id: 0.2523, G/loss_id_psnt: 0.2886, G/loss_cd: 0.0457\n",
            "Elapsed [0:00:12], Iteration [140/1000000], G/loss_id: 0.2203, G/loss_id_psnt: 0.3287, G/loss_cd: 0.0473\n",
            "Elapsed [0:00:13], Iteration [150/1000000], G/loss_id: 0.2647, G/loss_id_psnt: 0.2743, G/loss_cd: 0.0397\n",
            "Elapsed [0:00:13], Iteration [160/1000000], G/loss_id: 0.2790, G/loss_id_psnt: 0.2700, G/loss_cd: 0.0364\n",
            "Elapsed [0:00:14], Iteration [170/1000000], G/loss_id: 0.2611, G/loss_id_psnt: 0.2828, G/loss_cd: 0.0434\n",
            "Elapsed [0:00:15], Iteration [180/1000000], G/loss_id: 0.2846, G/loss_id_psnt: 0.2536, G/loss_cd: 0.0420\n",
            "Elapsed [0:00:15], Iteration [190/1000000], G/loss_id: 0.2532, G/loss_id_psnt: 0.2790, G/loss_cd: 0.0421\n",
            "Elapsed [0:00:16], Iteration [200/1000000], G/loss_id: 0.2463, G/loss_id_psnt: 0.2816, G/loss_cd: 0.0387\n",
            "Elapsed [0:00:17], Iteration [210/1000000], G/loss_id: 0.2405, G/loss_id_psnt: 0.2873, G/loss_cd: 0.0329\n",
            "Elapsed [0:00:18], Iteration [220/1000000], G/loss_id: 0.2572, G/loss_id_psnt: 0.2663, G/loss_cd: 0.0311\n",
            "Elapsed [0:00:18], Iteration [230/1000000], G/loss_id: 0.2638, G/loss_id_psnt: 0.2594, G/loss_cd: 0.0261\n",
            "Elapsed [0:00:19], Iteration [240/1000000], G/loss_id: 0.2529, G/loss_id_psnt: 0.2757, G/loss_cd: 0.0251\n",
            "Elapsed [0:00:20], Iteration [250/1000000], G/loss_id: 0.2597, G/loss_id_psnt: 0.2663, G/loss_cd: 0.0269\n",
            "Elapsed [0:00:21], Iteration [260/1000000], G/loss_id: 0.2722, G/loss_id_psnt: 0.2426, G/loss_cd: 0.0236\n",
            "Elapsed [0:00:21], Iteration [270/1000000], G/loss_id: 0.2631, G/loss_id_psnt: 0.2610, G/loss_cd: 0.0225\n",
            "Elapsed [0:00:22], Iteration [280/1000000], G/loss_id: 0.2674, G/loss_id_psnt: 0.2698, G/loss_cd: 0.0250\n",
            "Elapsed [0:00:23], Iteration [290/1000000], G/loss_id: 0.2780, G/loss_id_psnt: 0.2457, G/loss_cd: 0.0221\n",
            "Elapsed [0:00:24], Iteration [300/1000000], G/loss_id: 0.2800, G/loss_id_psnt: 0.2496, G/loss_cd: 0.0222\n",
            "Elapsed [0:00:24], Iteration [310/1000000], G/loss_id: 0.2651, G/loss_id_psnt: 0.2577, G/loss_cd: 0.0252\n",
            "Elapsed [0:00:25], Iteration [320/1000000], G/loss_id: 0.2438, G/loss_id_psnt: 0.2767, G/loss_cd: 0.0235\n",
            "Elapsed [0:00:26], Iteration [330/1000000], G/loss_id: 0.2526, G/loss_id_psnt: 0.2655, G/loss_cd: 0.0216\n",
            "Elapsed [0:00:27], Iteration [340/1000000], G/loss_id: 0.2494, G/loss_id_psnt: 0.2769, G/loss_cd: 0.0172\n",
            "Elapsed [0:00:27], Iteration [350/1000000], G/loss_id: 0.2594, G/loss_id_psnt: 0.2600, G/loss_cd: 0.0207\n",
            "Elapsed [0:00:28], Iteration [360/1000000], G/loss_id: 0.2325, G/loss_id_psnt: 0.2839, G/loss_cd: 0.0197\n",
            "Elapsed [0:00:29], Iteration [370/1000000], G/loss_id: 0.2647, G/loss_id_psnt: 0.2466, G/loss_cd: 0.0212\n",
            "Elapsed [0:00:29], Iteration [380/1000000], G/loss_id: 0.2473, G/loss_id_psnt: 0.2719, G/loss_cd: 0.0158\n",
            "Elapsed [0:00:30], Iteration [390/1000000], G/loss_id: 0.2658, G/loss_id_psnt: 0.2495, G/loss_cd: 0.0180\n",
            "Elapsed [0:00:31], Iteration [400/1000000], G/loss_id: 0.2526, G/loss_id_psnt: 0.2548, G/loss_cd: 0.0138\n",
            "Elapsed [0:00:32], Iteration [410/1000000], G/loss_id: 0.2568, G/loss_id_psnt: 0.2506, G/loss_cd: 0.0128\n",
            "Elapsed [0:00:32], Iteration [420/1000000], G/loss_id: 0.2529, G/loss_id_psnt: 0.2594, G/loss_cd: 0.0150\n",
            "Elapsed [0:00:33], Iteration [430/1000000], G/loss_id: 0.2664, G/loss_id_psnt: 0.2415, G/loss_cd: 0.0152\n",
            "Elapsed [0:00:34], Iteration [440/1000000], G/loss_id: 0.2394, G/loss_id_psnt: 0.2747, G/loss_cd: 0.0153\n",
            "Elapsed [0:00:35], Iteration [450/1000000], G/loss_id: 0.2472, G/loss_id_psnt: 0.2697, G/loss_cd: 0.0104\n",
            "Elapsed [0:00:35], Iteration [460/1000000], G/loss_id: 0.2525, G/loss_id_psnt: 0.2528, G/loss_cd: 0.0125\n",
            "Elapsed [0:00:36], Iteration [470/1000000], G/loss_id: 0.2430, G/loss_id_psnt: 0.2497, G/loss_cd: 0.0143\n",
            "Elapsed [0:00:37], Iteration [480/1000000], G/loss_id: 0.2565, G/loss_id_psnt: 0.2454, G/loss_cd: 0.0116\n",
            "Elapsed [0:00:38], Iteration [490/1000000], G/loss_id: 0.2553, G/loss_id_psnt: 0.2460, G/loss_cd: 0.0080\n",
            "Elapsed [0:00:38], Iteration [500/1000000], G/loss_id: 0.2472, G/loss_id_psnt: 0.2569, G/loss_cd: 0.0124\n",
            "Elapsed [0:00:39], Iteration [510/1000000], G/loss_id: 0.2609, G/loss_id_psnt: 0.2396, G/loss_cd: 0.0085\n",
            "Elapsed [0:00:40], Iteration [520/1000000], G/loss_id: 0.2437, G/loss_id_psnt: 0.2586, G/loss_cd: 0.0110\n",
            "Elapsed [0:00:41], Iteration [530/1000000], G/loss_id: 0.2449, G/loss_id_psnt: 0.2479, G/loss_cd: 0.0124\n",
            "Elapsed [0:00:41], Iteration [540/1000000], G/loss_id: 0.2560, G/loss_id_psnt: 0.2428, G/loss_cd: 0.0086\n",
            "Elapsed [0:00:42], Iteration [550/1000000], G/loss_id: 0.2471, G/loss_id_psnt: 0.2483, G/loss_cd: 0.0080\n",
            "Elapsed [0:00:43], Iteration [560/1000000], G/loss_id: 0.2383, G/loss_id_psnt: 0.2562, G/loss_cd: 0.0077\n",
            "Elapsed [0:00:43], Iteration [570/1000000], G/loss_id: 0.2468, G/loss_id_psnt: 0.2550, G/loss_cd: 0.0109\n",
            "Elapsed [0:00:44], Iteration [580/1000000], G/loss_id: 0.2459, G/loss_id_psnt: 0.2473, G/loss_cd: 0.0093\n",
            "Elapsed [0:00:45], Iteration [590/1000000], G/loss_id: 0.2541, G/loss_id_psnt: 0.2374, G/loss_cd: 0.0108\n",
            "Elapsed [0:00:46], Iteration [600/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2624, G/loss_cd: 0.0123\n",
            "Elapsed [0:00:46], Iteration [610/1000000], G/loss_id: 0.2562, G/loss_id_psnt: 0.2398, G/loss_cd: 0.0074\n",
            "Elapsed [0:00:47], Iteration [620/1000000], G/loss_id: 0.2481, G/loss_id_psnt: 0.2542, G/loss_cd: 0.0065\n",
            "Elapsed [0:00:48], Iteration [630/1000000], G/loss_id: 0.2398, G/loss_id_psnt: 0.2477, G/loss_cd: 0.0069\n",
            "Elapsed [0:00:49], Iteration [640/1000000], G/loss_id: 0.2508, G/loss_id_psnt: 0.2420, G/loss_cd: 0.0060\n",
            "Elapsed [0:00:50], Iteration [650/1000000], G/loss_id: 0.2273, G/loss_id_psnt: 0.2653, G/loss_cd: 0.0111\n",
            "Elapsed [0:00:52], Iteration [660/1000000], G/loss_id: 0.2374, G/loss_id_psnt: 0.2550, G/loss_cd: 0.0085\n",
            "Elapsed [0:00:53], Iteration [670/1000000], G/loss_id: 0.2422, G/loss_id_psnt: 0.2424, G/loss_cd: 0.0090\n",
            "Elapsed [0:00:54], Iteration [680/1000000], G/loss_id: 0.2274, G/loss_id_psnt: 0.2537, G/loss_cd: 0.0079\n",
            "Elapsed [0:00:54], Iteration [690/1000000], G/loss_id: 0.2479, G/loss_id_psnt: 0.2333, G/loss_cd: 0.0066\n",
            "Elapsed [0:00:55], Iteration [700/1000000], G/loss_id: 0.2387, G/loss_id_psnt: 0.2464, G/loss_cd: 0.0062\n",
            "Elapsed [0:00:56], Iteration [710/1000000], G/loss_id: 0.2302, G/loss_id_psnt: 0.2557, G/loss_cd: 0.0055\n",
            "Elapsed [0:00:57], Iteration [720/1000000], G/loss_id: 0.2448, G/loss_id_psnt: 0.2371, G/loss_cd: 0.0056\n",
            "Elapsed [0:00:57], Iteration [730/1000000], G/loss_id: 0.2340, G/loss_id_psnt: 0.2421, G/loss_cd: 0.0061\n",
            "Elapsed [0:00:58], Iteration [740/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2475, G/loss_cd: 0.0050\n",
            "Elapsed [0:00:59], Iteration [750/1000000], G/loss_id: 0.2380, G/loss_id_psnt: 0.2407, G/loss_cd: 0.0059\n",
            "Elapsed [0:01:00], Iteration [760/1000000], G/loss_id: 0.2481, G/loss_id_psnt: 0.2261, G/loss_cd: 0.0074\n",
            "Elapsed [0:01:00], Iteration [770/1000000], G/loss_id: 0.2324, G/loss_id_psnt: 0.2451, G/loss_cd: 0.0056\n",
            "Elapsed [0:01:01], Iteration [780/1000000], G/loss_id: 0.2411, G/loss_id_psnt: 0.2342, G/loss_cd: 0.0061\n",
            "Elapsed [0:01:02], Iteration [790/1000000], G/loss_id: 0.2349, G/loss_id_psnt: 0.2451, G/loss_cd: 0.0043\n",
            "Elapsed [0:01:03], Iteration [800/1000000], G/loss_id: 0.2389, G/loss_id_psnt: 0.2346, G/loss_cd: 0.0037\n",
            "Elapsed [0:01:03], Iteration [810/1000000], G/loss_id: 0.2366, G/loss_id_psnt: 0.2310, G/loss_cd: 0.0061\n",
            "Elapsed [0:01:04], Iteration [820/1000000], G/loss_id: 0.2197, G/loss_id_psnt: 0.2430, G/loss_cd: 0.0049\n",
            "Elapsed [0:01:05], Iteration [830/1000000], G/loss_id: 0.2420, G/loss_id_psnt: 0.2348, G/loss_cd: 0.0036\n",
            "Elapsed [0:01:06], Iteration [840/1000000], G/loss_id: 0.2390, G/loss_id_psnt: 0.2397, G/loss_cd: 0.0049\n",
            "Elapsed [0:01:07], Iteration [850/1000000], G/loss_id: 0.2419, G/loss_id_psnt: 0.2327, G/loss_cd: 0.0041\n",
            "Elapsed [0:01:08], Iteration [860/1000000], G/loss_id: 0.2284, G/loss_id_psnt: 0.2467, G/loss_cd: 0.0030\n",
            "Elapsed [0:01:08], Iteration [870/1000000], G/loss_id: 0.2312, G/loss_id_psnt: 0.2273, G/loss_cd: 0.0060\n",
            "Elapsed [0:01:09], Iteration [880/1000000], G/loss_id: 0.2258, G/loss_id_psnt: 0.2397, G/loss_cd: 0.0046\n",
            "Elapsed [0:01:10], Iteration [890/1000000], G/loss_id: 0.2338, G/loss_id_psnt: 0.2282, G/loss_cd: 0.0041\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-04e12e321f52>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0a75626680c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg_loss_id_psnt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_cd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_loss_cd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "    # Data loader.\n",
        "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
        "\n",
        "solver = Solver(vcc_loader, config)\n",
        "\n",
        "solver.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PerBW2C-hF6J"
      },
      "source": [
        "# 보코더\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PWLPDEzjfO_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import soundfile as sf\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from wavenet_vocoder import builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKvwjnd0bTi4"
      },
      "outputs": [],
      "source": [
        "#hparams.py\n",
        "# NOTE: If you want full control for model architecture. please take a look\n",
        "# at the code and change whatever you want. Some hyper parameters are hardcoded.\n",
        "\n",
        "\n",
        "class Map(dict):\n",
        "\t\"\"\"\n",
        "    Example:\n",
        "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
        "\n",
        "    Credits to epool:\n",
        "    https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, *args, **kwargs):\n",
        "\t\tsuper(Map, self).__init__(*args, **kwargs)\n",
        "\t\tfor arg in args:\n",
        "\t\t\tif isinstance(arg, dict):\n",
        "\t\t\t\tfor k, v in arg.items():\n",
        "\t\t\t\t\tself[k] = v\n",
        "\n",
        "\t\tif kwargs:\n",
        "\t\t\tfor k, v in kwargs.iteritems():\n",
        "\t\t\t\tself[k] = v\n",
        "\n",
        "\tdef __getattr__(self, attr):\n",
        "\t\treturn self.get(attr)\n",
        "\n",
        "\tdef __setattr__(self, key, value):\n",
        "\t\tself.__setitem__(key, value)\n",
        "\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tsuper(Map, self).__setitem__(key, value)\n",
        "\t\tself.__dict__.update({key: value})\n",
        "\n",
        "\tdef __delattr__(self, item):\n",
        "\t\tself.__delitem__(item)\n",
        "\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tsuper(Map, self).__delitem__(key)\n",
        "\t\tdel self.__dict__[key]\n",
        "\n",
        "\n",
        "# Default hyperparameters:\n",
        "hparams = Map({ #하이퍼 파라미터 묶음\n",
        "\t'name': \"wavenet_vocoder\",\n",
        "\n",
        "\t# Convenient model builder\n",
        "\t'builder': \"wavenet\",\n",
        "\n",
        "\t# Input type:\n",
        "\t# 1. raw [-1, 1]\n",
        "\t# 2. mulaw [-1, 1]\n",
        "\t# 3. mulaw-quantize [0, mu]\n",
        "\t# If input_type is raw or mulaw, network assumes scalar input and\n",
        "\t# discretized mixture of logistic distributions output, otherwise one-hot\n",
        "\t# input and softmax output are assumed.\n",
        "\t# **NOTE**: if you change the one of the two parameters below, you need to\n",
        "\t# re-run preprocessing before training.\n",
        "\t'input_type': \"raw\",\n",
        "\t'quantize_channels': 65536,  # 65536 or 256\n",
        "\n",
        "\t# Audio:\n",
        "\t'sample_rate': 16000,\n",
        "\t# this is only valid for mulaw is True\n",
        "\t'silence_threshold': 2,\n",
        "\t'num_mels': 80,\n",
        "\t'fmin': 125,\n",
        "\t'fmax': 7600,\n",
        "\t'fft_size': 1024,\n",
        "\t# shift can be specified by either hop_size or frame_shift_ms\n",
        "\t'hop_size': 256,\n",
        "\t'frame_shift_ms': None,\n",
        "\t'min_level_db': -100,\n",
        "\t'ref_level_db': 20,\n",
        "\t# whether to rescale waveform or not.\n",
        "\t# Let x is an input waveform, rescaled waveform y is given by:\n",
        "\t# y = x / np.abs(x).max() * rescaling_max\n",
        "\t'rescaling': True,\n",
        "\t'rescaling_max': 0.999,\n",
        "\t# mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n",
        "\t# happen depends on min_level_db and ref_level_db, causing clipping noise.\n",
        "\t# If False, assertion is added to ensure no clipping happens.o0\n",
        "\t'allow_clipping_in_normalization': True,\n",
        "\n",
        "\t# Mixture of logistic distributions:\n",
        "\t'log_scale_min': float(-32.23619130191664),\n",
        "\n",
        "\t# Model:\n",
        "\t# This should equal to `quantize_channels` if mu-law quantize enabled\n",
        "\t# otherwise num_mixture * 3 (pi, mean, log_scale)\n",
        "\t'out_channels': 10 * 3,\n",
        "\t'layers': 24,\n",
        "\t'stacks': 4,\n",
        "\t'residual_channels': 512,\n",
        "\t'gate_channels': 512,  # split into 2 gropus internally for gated activation\n",
        "\t'skip_out_channels': 256,\n",
        "\t'dropout': 1 - 0.95,\n",
        "\t'kernel_size': 3,\n",
        "\t# If True, apply weight normalization as same as DeepVoice3\n",
        "\t'weight_normalization': True,\n",
        "\t# Use legacy code or not. Default is True since we already provided a model\n",
        "\t# based on the legacy code that can generate high-quality audio.\n",
        "\t# Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n",
        "\t'legacy': True,\n",
        "\n",
        "\t# Local conditioning (set negative value to disable))\n",
        "\t'cin_channels': 80,\n",
        "\t# If True, use transposed convolutions to upsample conditional features,\n",
        "\t# otherwise repeat features to adjust time resolution\n",
        "\t'upsample_conditional_features': True,\n",
        "\t# should np.prod(upsample_scales) == hop_size\n",
        "\t'upsample_scales': [4, 4, 4, 4],\n",
        "\t# Freq axis kernel size for upsampling network\n",
        "\t'freq_axis_kernel_size': 3,\n",
        "\n",
        "\t# Global conditioning (set negative value to disable)\n",
        "\t# currently limited for speaker embedding\n",
        "\t# this should only be enabled for multi-speaker dataset\n",
        "\t'gin_channels': -1,  # i.e., speaker embedding dim\n",
        "\t'n_speakers': -1,\n",
        "\n",
        "\t# Data loader\n",
        "\t'pin_memory': True,\n",
        "\t'num_workers': 2,\n",
        "\n",
        "\t# train/test\n",
        "\t# test size can be specified as portion or num samples\n",
        "\t'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n",
        "\t'test_num_samples': None,\n",
        "\t'random_state': 1234,\n",
        "\n",
        "\t# Loss\n",
        "\n",
        "\t# Training:\n",
        "\t'batch_size': 2,\n",
        "\t'adam_beta1': 0.9,\n",
        "\t'adam_beta2': 0.999,\n",
        "\t'adam_eps': 1e-8,\n",
        "\t'amsgrad': False,\n",
        "\t'initial_learning_rate': 1e-3,\n",
        "\t# see lrschedule.py for available lr_schedule\n",
        "\t'lr_schedule': \"noam_learning_rate_decay\",\n",
        "\t'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n",
        "\t'nepochs': 2000,\n",
        "\t'weight_decay': 0.0,\n",
        "\t'clip_thresh': -1,\n",
        "\t# max time steps can either be specified as sec or steps\n",
        "\t# if both are None, then full audio samples are used in a batch\n",
        "\t'max_time_sec': None,\n",
        "\t'max_time_steps': 8000,\n",
        "\t# Hold moving averaged parameters and use them for evaluation\n",
        "\t'exponential_moving_average': True,\n",
        "\t# averaged = decay * averaged + (1 - decay) * x\n",
        "\t'ema_decay': 0.9999,\n",
        "\n",
        "\t# Save\n",
        "\t# per-step intervals\n",
        "\t'checkpoint_interval': 10000,\n",
        "\t'train_eval_interval': 10000,\n",
        "\t# per-epoch interval\n",
        "\t'test_eval_epoch_interval': 5,\n",
        "\t'save_optimizer_state': True,\n",
        "\n",
        "\t# Eval:\n",
        "})\n",
        "\n",
        "\n",
        "# def hparams_debug_string():\n",
        "# \tvalues = hparams.values()\n",
        "# \thp = ['  %s: %s' % (name, values[name]) for name in sorted(values)]\n",
        "# \treturn 'Hyperparameters:\\n' + '\\n'.join(hp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oqr0SaMjmT2"
      },
      "outputs": [],
      "source": [
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOF9y4XucFVa"
      },
      "outputs": [],
      "source": [
        "def build_model(): #모델 생성\n",
        "\n",
        "    model = getattr(builder, hparams.builder)(\n",
        "        out_channels=hparams.out_channels,\n",
        "        layers=hparams.layers,\n",
        "        stacks=hparams.stacks,\n",
        "        residual_channels=hparams.residual_channels,\n",
        "        gate_channels=hparams.gate_channels,\n",
        "        skip_out_channels=hparams.skip_out_channels,\n",
        "        cin_channels=hparams.cin_channels,\n",
        "        gin_channels=hparams.gin_channels,\n",
        "        weight_normalization=hparams.weight_normalization,\n",
        "        n_speakers=hparams.n_speakers,\n",
        "        dropout=hparams.dropout,\n",
        "        kernel_size=hparams.kernel_size,\n",
        "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
        "        upsample_scales=hparams.upsample_scales,\n",
        "        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n",
        "        scalar_input=True,\n",
        "        legacy=hparams.legacy,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def wavegen(model, c=None, tqdm=tqdm): #파형 생성\n",
        "    \"\"\"Generate waveform samples by WaveNet.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.make_generation_fast_()\n",
        "\n",
        "    Tc = c.shape[0]\n",
        "    upsample_factor = hparams.hop_size\n",
        "    # Overwrite length according to feature size\n",
        "    length = Tc * upsample_factor\n",
        "\n",
        "    # B x C x T\n",
        "    c = torch.FloatTensor(c.T).unsqueeze(0)\n",
        "\n",
        "    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n",
        "\n",
        "    # Transform data to GPU\n",
        "    initial_input = initial_input.to(device)\n",
        "    c = None if c is None else c.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = model.incremental_forward(\n",
        "            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
        "            log_scale_min=hparams.log_scale_min)\n",
        "\n",
        "    y_hat = y_hat.view(-1).cpu().data.numpy()\n",
        "\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etqieCP-7Xtb"
      },
      "outputs": [],
      "source": [
        "#synthesis.py\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Synthesis waveform from trained WaveNet.\n",
        "\n",
        "Modified from https://github.com/r9y9/wavenet_vocoder\n",
        "\"\"\"\n",
        "\n",
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG1BpILHjppf",
        "outputId": "5eb36ccb-1526-44dd-e319-25b5fd9ac2fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "<ipython-input-26-a6ad4607df90>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p225xp225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23040/23040 [04:54<00:00, 78.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p225xp228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23040/23040 [04:50<00:00, 79.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p225xp256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23040/23040 [04:50<00:00, 79.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p225xp270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 13942/23040 [02:53<01:53, 80.21it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a6ad4607df90>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-dd546615aad5>\u001b[0m in \u001b[0;36mwavegen\u001b[0;34m(model, c, tqdm)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         y_hat = model.incremental_forward(\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             log_scale_min=hparams.log_scale_min)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wavenet_vocoder/wavenet.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, initial_input, c, g, T, test_inputs, tqdm, softmax, quantize, log_scale_min)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                     \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mskips\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, x, c, g)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x, c, g, is_incremental)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1x1c\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv1x1_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1x1c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplitdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wavenet_vocoder/modules.py\u001b[0m in \u001b[0;36m_conv1x1_forward\u001b[0;34m(conv, x, is_incremental)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_incremental\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wavenet_vocoder/conv.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
        "device = torch.device(\"cuda\")\n",
        "model = build_model().to(device)\n",
        "checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "for spect in spect_vc:\n",
        "    name = spect[0]\n",
        "    c = spect[1]\n",
        "    print(name)\n",
        "    waveform = wavegen(model, c=c)\n",
        "    sf.write(name+'.wav', waveform, samplerate=16000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1ekp16rP1qOj0y7N3PrfTMGKJFLoOwutO",
      "authorship_tag": "ABX9TyOK30nKmVqjSl2InyV4Fjm5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}