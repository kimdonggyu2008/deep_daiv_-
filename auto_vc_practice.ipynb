{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/auto_vc_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2Vzcr775Ib",
        "outputId": "3cc3eea8-31de-4619-eb1e-8cb7785f8fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAKmnK5n6Gez",
        "outputId": "8a36e1d2-0741-4e01-e141-c7eb74b9006e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wavenet_vocoder\n",
            "  Downloading wavenet_vocoder-0.1.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.13.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->wavenet_vocoder) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->wavenet_vocoder) (1.3.0)\n",
            "Building wheels for collected packages: wavenet_vocoder\n",
            "  Building wheel for wavenet_vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet_vocoder: filename=wavenet_vocoder-0.1.1-py3-none-any.whl size=12642 sha256=783c65761cf9fd15e40a228ef85151dcc4d4e521f0f19035add1b782b2b98828\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/a4/7b/f1d21f96be36a13e9c3948e8c28792bf8962da19781abd9dc8\n",
            "Successfully built wavenet_vocoder\n",
            "Installing collected packages: wavenet_vocoder\n",
            "Successfully installed wavenet_vocoder-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wavenet_vocoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8YKdCC3ciLI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V34cp_HHalfN"
      },
      "outputs": [],
      "source": [
        "#make_spect.py\n",
        "def butter_highpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq #차단할 주파수 대역 지정\n",
        "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    return b, a #버터워스 고역 필터 계수\n",
        "\n",
        "\n",
        "def pySTFT(x, fft_length=1024, hop_length=256): #stft실행\n",
        "\n",
        "    x = np.pad(x, int(fft_length//2), mode='reflect') #패딩, 불연속성 완화\n",
        "\n",
        "    noverlap = fft_length - hop_length #중첩된 샘플 수 계산\n",
        "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)#??\n",
        "    strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])#스트라이드 갯수 계산\n",
        "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "                                             strides=strides)#?? 윈도우변환??\n",
        "\n",
        "    fft_window = get_window('hann', fft_length, fftbins=True) #해닝 창 함수 적용\n",
        "    result = np.fft.rfft(fft_window * result, n=fft_length).T #ftf후 rftf적용\n",
        "\n",
        "    return np.abs(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc3uiuq89uh4",
        "outputId": "43e64dc1-1fa1-4c99-ed59-edcc0d24bc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs\n",
            "p225\n",
            "p226\n",
            "p227\n"
          ]
        }
      ],
      "source": [
        "mel_basis = mel(sr=16000,n_fft=1024,  n_mels=80, fmin=90, fmax=7600).T #멜 스펙트로그램 생성\n",
        "#신호의 저주파 성분 제거\n",
        "min_level = np.exp(-100 / 20 * np.log(10))\n",
        "#작은 신호를 0말고 최솟값으로 변환\n",
        "b, a = butter_highpass(30, 16000, order=5)\n",
        "#고역필터 계수 지정\n",
        "\n",
        "# audio file directory\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'\n",
        "\n",
        "# audio file directory\n",
        "# rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "# targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/results'\n",
        "\n",
        "#  audio file directory\n",
        "# rootDir = './wavs'\n",
        "# # spectrogram directory\n",
        "# targetDir = './spmel'\n",
        "\n",
        "\n",
        "\n",
        "dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "for subdir in sorted(subdirList): #데이터 파일 확인\n",
        "    print(subdir)\n",
        "    if not os.path.exists(os.path.join(targetDir, subdir)):#타겟 디렉토리 없으면 만듦\n",
        "        os.makedirs(os.path.join(targetDir, subdir))\n",
        "    _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))#순차적 방문\n",
        "    prng = RandomState(int(subdir[1:]))\n",
        "    for fileName in sorted(fileList):\n",
        "        # Read audio file\n",
        "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))#오디오파일 읽어오기\n",
        "        # Remove drifting noise\n",
        "        y = signal.filtfilt(b, a, x)#만든 고역필터 적용\n",
        "        # Ddd a little random noise for model roubstness\n",
        "        wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
        "        # Compute spect\n",
        "        D = pySTFT(wav).T #stft실행 후 스펙트로그램 생성\n",
        "        # Convert to mel and normalize\n",
        "        D_mel = np.dot(D, mel_basis)#스펙트로그램과 필터 뱅크 곱함 ??\n",
        "        D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16 #최소값 지정\n",
        "        S = np.clip((D_db + 100) / 100, 0, 1)#결과값을 0~1로 제한해서 클리핑 제한\n",
        "        # save spect\n",
        "        np.save(os.path.join(targetDir, subdir, fileName[:-4]),\n",
        "                S.astype(np.float32), allow_pickle=False)#멜스펙트로그램 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i515s4B4a9Jl"
      },
      "outputs": [],
      "source": [
        "#model_bl.py\n",
        "\n",
        "class D_VECTOR(nn.Module):\n",
        "    \"\"\"d vector speaker embedding.\"\"\"\n",
        "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
        "        super(D_VECTOR, self).__init__()#모듈의 d_vector 상속\n",
        "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()#batch_size, sequence_len, n_feature\n",
        "        lstm_out, _ = self.lstm(x)#batch_size,sequence_len,hidden_size\n",
        "        embeds = self.embedding(lstm_out[:,-1,:])#sequence_len의 마지막을 임베딩으로 변환\n",
        "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        embeds_normalized = embeds.div(norm)#batch_size, dim_emb\n",
        "        return embeds_normalized#batch_size, dim_Emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KziVBX5dbD9M"
      },
      "outputs": [],
      "source": [
        "#make_metadata.py\n",
        "\"\"\"\n",
        "Generate speaker embeddings and metadata for training\n",
        "\"\"\"\n",
        "\n",
        "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda() #화자별 임베딩\n",
        "c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n",
        "# c_checkpoint = torch.load('3000000-BL.ckpt')\n",
        "new_state_dict = OrderedDict()\n",
        "for key, val in c_checkpoint['model_b'].items():\n",
        "    new_key = key[7:]\n",
        "    new_state_dict[new_key] = val\n",
        "C.load_state_dict(new_state_dict)\n",
        "num_uttrs = 10\n",
        "len_crop = 128\n",
        "\n",
        "\n",
        "# Directory containing mel-spectrograms, 별도 데이터셋?\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/spmel'#?\n",
        "# rootDir = './spmel'\n",
        "dirName, subdirList, _ = next(os.walk(rootDir)) #데이터셋 파일 필요함\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "speakers = [] #화자 전체의 임베딩 목록\n",
        "for speaker in sorted(subdirList):\n",
        "    print('Processing speaker: %s' % speaker)\n",
        "    utterances = [] #단일 화자별 임베딩을 저장\n",
        "    utterances.append(speaker)\n",
        "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))#하나씩 추출개시\n",
        "\n",
        "    # make speaker embedding\n",
        "    assert len(fileList) >= num_uttrs\n",
        "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
        "    embs = []\n",
        "    for i in range(num_uttrs):\n",
        "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
        "        #디렉토리, 화자, 화자별 파일 순으로 로딩\n",
        "        candidates = np.delete(np.arange(len(fileList)), idx_uttrs)\n",
        "        # choose another utterance if the current one is too short\n",
        "        while tmp.shape[0] < len_crop:#지정 길이보다 짧으면 다른걸로 고름\n",
        "            idx_alt = np.random.choice(candidates)\n",
        "            tmp = np.load(os.path.join(dirName, speaker, fileList[idx_alt]))\n",
        "            candidates = np.delete(candidates, np.argwhere(candidates==idx_alt))\n",
        "        left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
        "        melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :]).cuda()\n",
        "        #주어진 간격(len_crop)만큼만 잘라서 학습에 활용\n",
        "        emb = C(melsp)#해당 부분을 임베딩\n",
        "        embs.append(emb.detach().squeeze().cpu().numpy())#해당 임베딩들 수집\n",
        "    utterances.append(np.mean(embs, axis=0))#평균값으로 묶어서 화자 정보로 추가\n",
        "\n",
        "    # create file list\n",
        "    for fileName in sorted(fileList):#각 화자에 대한 정보 파일로 저장\n",
        "        utterances.append(os.path.join(speaker,fileName))\n",
        "    speakers.append(utterances)\n",
        "\n",
        "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
        "    pickle.dump(speakers, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSI7SwAXbfj_"
      },
      "outputs": [],
      "source": [
        "#data_loader.py\n",
        "\n",
        "\n",
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, len_crop):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.len_crop = len_crop\n",
        "        self.step = 10\n",
        "\n",
        "        metaname = os.path.join(self.root_dir, \"train.pkl\") #메타데이터 읽어오기\n",
        "        meta = pickle.load(open(metaname, \"rb\"))\n",
        "\n",
        "        \"\"\"Load data using multiprocessing\"\"\"\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))#프로세스를 적용할 파일들\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:#프로세스 적용 후 저장\n",
        "            p.join()\n",
        "\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the dataset...')\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):#메타데이터 일부, 데이터셋 리스트, 인덱스 오프셋\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]#각 화자별로 넣을 수 있는 none칸 생성\n",
        "            for j, tmp in enumerate(sbmt):#화자별로 ID 임베딩 저장\n",
        "                if j < 2:  # fill in speaker id and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else: # load the mel-spectrograms\n",
        "\n",
        "                    #해당 화자의 멜스펙트로그램 읽어옴\n",
        "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp),allow_pickle=True)\n",
        "                    #uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
        "\n",
        "\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):#데이터셋의 인덱스\n",
        "        # pick a random speaker\n",
        "        dataset = self.train_dataset\n",
        "        list_uttrs = dataset[index] #데이터셋 상에서 해당 인덱스를 가지는 화자 검색\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        # pick random uttr with random crop\n",
        "        a = np.random.randint(2, len(list_uttrs)) #무작위 화자 선택\n",
        "        tmp = list_uttrs[a]\n",
        "        if tmp.shape[0] < self.len_crop: #추출 길이보다 샘플이 짧음\n",
        "            len_pad = self.len_crop - tmp.shape[0]#필요 길이까지 0으로 패딩 추가\n",
        "            uttr = np.pad(tmp, ((0,len_pad),(0,0)), 'constant')\n",
        "        elif tmp.shape[0] > self.len_crop:\n",
        "            left = np.random.randint(tmp.shape[0]-self.len_crop)#랜덤위치 잡고 LEN_CROP길이 가져옴\n",
        "            uttr = tmp[left:left+self.len_crop, :]\n",
        "        else:\n",
        "            uttr = tmp#같으면 전체 반환\n",
        "\n",
        "        return uttr, emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens #샘플의 토큰 갯수 반환\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size=16, len_crop=128, num_workers=0):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(root_dir, len_crop)\n",
        "\n",
        "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  worker_init_fn=worker_init_fn)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6zEd5YR_knf"
      },
      "source": [
        "# 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqCzLhYybRMT"
      },
      "outputs": [],
      "source": [
        "#model_vc.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):#가중치 설정용 선형 놈\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(#가중치 분산이 입력데이터 갯수에 반비례하게 초기화됨\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module): #합성곱 놈\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)#커널 사이즈에 따라 패딩 조정, 크기 유지\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels, #1D 컨볼루션 층, 시간따라 바뀌는것만 보면 됨\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(#초기화\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class Encoder(nn.Module): #인코더 모델\n",
        "    \"\"\"Encoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, freq):#보틀넥 구조 디멘션 갯수\n",
        "        #첫번째 컨볼루션은 512+화자 임베딩, 512로 이루어짐\n",
        "        #2, 3번째 컨볼루션은 512,512로 이루어짐\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_neck = dim_neck\n",
        "        self.freq = freq\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(#컨볼루션 3개 넣음\n",
        "                ConvNorm(80+dim_emb if i==0 else 512,#입력이 주파수 대역 갯수 + 화자임베딩\n",
        "                        #또는 원 크기?\n",
        "                         512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(512))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
        "        #쌍방 LSTM\n",
        "\n",
        "    def forward(self, x, c_org):#배치 사이즈, 1, 시간 프레임 T, 주파수대역 갯수 80\n",
        "        #C_ORG = 화자 임베딩 1차원 스칼라\n",
        "        x = x.squeeze(1).transpose(2,1)#배치사이즈, 주파수 대역, 시간 프래임\n",
        "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))#화자,화자, 시간프레임\n",
        "        x = torch.cat((x, c_org), dim=1)# 원 오디오+화자 임베딩\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x)) #컨볼루션 레이어가 끝나고 활성화함수 적용\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters() #lstm의 특징 추출\n",
        "        outputs, _ = self.lstm(x)\n",
        "        out_forward = outputs[:, :, :self.dim_neck] #순방향 lstm\n",
        "        out_backward = outputs[:, :, self.dim_neck:] #역방향 lstm\n",
        "\n",
        "        codes = []\n",
        "        for i in range(0, outputs.size(1), self.freq):#순방향 마지막, 역방향 첫번째 연결\n",
        "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
        "\n",
        "        return codes\n",
        "\n",
        "\n",
        "class Decoder(nn.Module): #디코더 모델\n",
        "    \"\"\"Decoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre):#인코더 잠재 크기, 화자 임베딩 크기, 중간층 맵 크기\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)#인코더 잠재표현+화자 임베딩\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential( #컨볼루션 층 3개 지정, 모두 중간맵 크기로만 지정됨\n",
        "                ConvNorm(dim_pre,\n",
        "                         dim_pre,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(dim_pre))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)#lstm 2번째\n",
        "\n",
        "        self.linear_projection = LinearNorm(1024, 80)\n",
        "\n",
        "    def forward(self, x): #학습 절차\n",
        "\n",
        "        #self.lstm1.flatten_parameters()\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        outputs, _ = self.lstm2(x)\n",
        "\n",
        "        decoder_output = self.linear_projection(outputs)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "class Postnet(nn.Module): #포스트넷, 후처리 담당\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(#컨볼루션 레이어 추가 80->512, 아마 주파수별 특징을 전체로 확장하는 것 같음\n",
        "            nn.Sequential(\n",
        "                ConvNorm(80, 512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(512))\n",
        "        )\n",
        "\n",
        "        for i in range(1, 5 - 1): #컨볼루션 3개, 512->512, 히든 레이어\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(512,\n",
        "                             512,\n",
        "                             kernel_size=5, stride=1,\n",
        "                             padding=2,\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(512))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(#마지막 512->80, 다시 줄임\n",
        "            nn.Sequential(\n",
        "                ConvNorm(512, 80,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(80))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):#시행\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = torch.tanh(self.convolutions[i](x))\n",
        "\n",
        "        x = self.convolutions[-1](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator(nn.Module): #생성기 = 인코더+디코더+포스트넷\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):# 히든 차원 갯수, 화자 임베딩, 최종 결과 차원\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
        "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, x, c_org, c_trg):#생성기 생성\n",
        "\n",
        "        codes = self.encoder(x, c_org)#batch_size, time_steps//freq, dim_neck\n",
        "        if c_trg is None:\n",
        "            return torch.cat(codes, dim=-1)\n",
        "\n",
        "        tmp = []\n",
        "        for code in codes:\n",
        "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
        "            #batch_size,1,dim_neck\n",
        "            #batch_size, time_Steps//len(codes),dim_neck\n",
        "        code_exp = torch.cat(tmp, dim=1)\n",
        "        #두번째 차원으로 묶음\n",
        "\n",
        "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
        "        #최종적으로는 batch_size,time_steps, dim_neck+dim_emb로 변형\n",
        "\n",
        "        mel_outputs = self.decoder(encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
        "        #batch_size, time_step, dim_pre에서 batch_size,dim_pre, time_Step으로 변형 후 포스트넷 적용\n",
        "\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
        "        #잔차 적용으로 다시 원본으로 바꾼 후 더함\n",
        "\n",
        "        mel_outputs = mel_outputs.unsqueeze(1)\n",
        "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)\n",
        "        #batch_size, time_Steps//freq, len(codes)*dim_neck?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAX8yLy_sDv"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnlPJX6JK57d"
      },
      "outputs": [],
      "source": [
        "#main.py\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ6SWuZZfU08"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/코딩공부/deep_daiv/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itRsZDeOg1f9"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmwCxNSLcN1e"
      },
      "outputs": [],
      "source": [
        "sys.argv=[''] #명령어 만들어서 실행\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Model configuration.\n",
        "parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
        "parser.add_argument('--dim_neck', type=int, default=16)\n",
        "parser.add_argument('--dim_emb', type=int, default=256)\n",
        "parser.add_argument('--dim_pre', type=int, default=512)\n",
        "parser.add_argument('--freq', type=int, default=16)\n",
        "\n",
        "# Training configuration.\n",
        "parser.add_argument('--data_dir', type=str, default='./spmel')\n",
        "parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
        "parser.add_argument('--num_iters', type=int, default=1000000, help='number of total iterations')\n",
        "parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
        "\n",
        "# Miscellaneous.\n",
        "parser.add_argument('--log_step', type=int, default=10)\n",
        "\n",
        "config = parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHF3fFgzQVL9"
      },
      "outputs": [],
      "source": [
        "# from easydict import EasyDict as edict\n",
        "\n",
        "# config = edict()\n",
        "\n",
        "# # Model configuration.\n",
        "# config.lambda_cd = 1\n",
        "# config.dim_neck = 16\n",
        "# config.dim_emb = 256\n",
        "# config.dim_pre = 512\n",
        "# config.freq = 16\n",
        "\n",
        "# # Training configuration.\n",
        "# config.data_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# config.batch_size = 2\n",
        "# config.num_iters = 1000000\n",
        "# config.len_crop = 128\n",
        "\n",
        "# # Miscellaneous.\n",
        "# config.log_step = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXgGFoXnb0De"
      },
      "outputs": [],
      "source": [
        "#solver_encoder.py\n",
        "\n",
        "class Solver(object): #음성신호 학습\n",
        "\n",
        "    def __init__(self, vcc_loader, config):#입력된 변수들 받아서 저장\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.vcc_loader = vcc_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.lambda_cd = config.lambda_cd\n",
        "        self.dim_neck = config.dim_neck\n",
        "        self.dim_emb = config.dim_emb\n",
        "        self.dim_pre = config.dim_pre\n",
        "        self.freq = config.freq\n",
        "\n",
        "        # Training configurations.\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
        "        self.log_step = config.log_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self): #모델 구축\n",
        "\n",
        "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)\n",
        "        #인코더, 디코더, 포스트넷으로 이루어진 생성기\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), 0.0001)\n",
        "        #옵티마이저 생성\n",
        "        self.G.to(self.device)\n",
        "        #지정된 장치로 수행\n",
        "\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()#초기화\n",
        "\n",
        "\n",
        "    #=====================================================================================================================================#\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # Set data loader.\n",
        "        data_loader = self.vcc_loader#데이터 로더 생성\n",
        "\n",
        "        # Print logs in specified order\n",
        "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(self.num_iters):#데이터에 대해 반복시작\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Fetch data.\n",
        "            try:#데이터 읽어오기\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, emb_org = next(data_iter)\n",
        "\n",
        "\n",
        "\n",
        "            x_real = x_real.to(self.device)#실제 음성\n",
        "            emb_org = emb_org.to(self.device)#\n",
        "\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               2. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            self.G = self.G.train()\n",
        "\n",
        "            # Identity mapping loss\n",
        "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
        "\n",
        "            g_loss_id = F.mse_loss(x_real, x_identic)\n",
        "            g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n",
        "            #원본과 생성된 음성에 대한 차이값 계산\n",
        "            #psnt는 포스트넷을 지난 데이터에 대해서 평가\n",
        "\n",
        "\n",
        "            # Code semantic loss.\n",
        "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
        "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
        "            #생성된 음성을 원본과 비교해서 손실 계산\n",
        "\n",
        "            # Backward and optimize.\n",
        "            g_loss = g_loss_id + g_loss_id_psnt + self.lambda_cd * g_loss_cd\n",
        "            # 원본과 생성값, 후처리된 원본과 생성값, 코드손실값*비율 계산\n",
        "\n",
        "            self.reset_grad()\n",
        "            g_loss.backward()#역전파로 가중치 수정\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['G/loss_id'] = g_loss_id.item()\n",
        "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
        "            loss['G/loss_cd'] = g_loss_cd.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag in keys:\n",
        "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
        "                print(log)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-tF8qGvMpXK"
      },
      "outputs": [],
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "    # Data loader.\n",
        "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
        "\n",
        "solver = Solver(vcc_loader, config)\n",
        "\n",
        "solver.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PerBW2C-hF6J"
      },
      "source": [
        "# 보코더\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PWLPDEzjfO_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import soundfile as sf\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from wavenet_vocoder import builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKvwjnd0bTi4"
      },
      "outputs": [],
      "source": [
        "#hparams.py\n",
        "# NOTE: If you want full control for model architecture. please take a look\n",
        "# at the code and change whatever you want. Some hyper parameters are hardcoded.\n",
        "\n",
        "\n",
        "class Map(dict): #보코더의 하이퍼 파라미터\n",
        "\t\"\"\"\n",
        "    Example:\n",
        "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
        "\n",
        "    Credits to epool:\n",
        "    https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, *args, **kwargs): #딕셔너리 확장버전\n",
        "\t\tsuper(Map, self).__init__(*args, **kwargs)\n",
        "\t\tfor arg in args:\n",
        "\t\t\tif isinstance(arg, dict):\n",
        "\t\t\t\tfor k, v in arg.items():\n",
        "\t\t\t\t\tself[k] = v\n",
        "\n",
        "\t\tif kwargs:\n",
        "\t\t\tfor k, v in kwargs.iteritems():\n",
        "\t\t\t\tself[k] = v\n",
        "\n",
        "\tdef __getattr__(self, attr): #각 변수를 불러오는 방법\n",
        "\t\treturn self.get(attr)\n",
        "\n",
        "\tdef __setattr__(self, key, value):\n",
        "\t\tself.__setitem__(key, value)\n",
        "\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tsuper(Map, self).__setitem__(key, value)\n",
        "\t\tself.__dict__.update({key: value})\n",
        "\n",
        "\tdef __delattr__(self, item):\n",
        "\t\tself.__delitem__(item)\n",
        "\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tsuper(Map, self).__delitem__(key)\n",
        "\t\tdel self.__dict__[key]\n",
        "\n",
        "\n",
        "# Default hyperparameters:\n",
        "hparams = Map({ #하이퍼 파라미터 묶음\n",
        "\t'name': \"wavenet_vocoder\",\n",
        "\n",
        "\t# Convenient model builder\n",
        "\t'builder': \"wavenet\",\n",
        "\n",
        "\t# Input type:\n",
        "\t# 1. raw [-1, 1]\n",
        "\t# 2. mulaw [-1, 1]\n",
        "\t# 3. mulaw-quantize [0, mu]\n",
        "\t# If input_type is raw or mulaw, network assumes scalar input and\n",
        "\t# discretized mixture of logistic distributions output, otherwise one-hot\n",
        "\t# input and softmax output are assumed.\n",
        "\t# **NOTE**: if you change the one of the two parameters below, you need to\n",
        "\t# re-run preprocessing before training.\n",
        "\t'input_type': \"raw\",\n",
        "\t'quantize_channels': 65536,  # 65536 or 256\n",
        "\n",
        "\t# Audio:\n",
        "\t'sample_rate': 16000,\n",
        "\t# this is only valid for mulaw is True\n",
        "\t'silence_threshold': 2,\n",
        "\t'num_mels': 80,\n",
        "\t'fmin': 125,\n",
        "\t'fmax': 7600,\n",
        "\t'fft_size': 1024,\n",
        "\t# shift can be specified by either hop_size or frame_shift_ms\n",
        "\t'hop_size': 256,\n",
        "\t'frame_shift_ms': None,\n",
        "\t'min_level_db': -100,\n",
        "\t'ref_level_db': 20,\n",
        "\t# whether to rescale waveform or not.\n",
        "\t# Let x is an input waveform, rescaled waveform y is given by:\n",
        "\t# y = x / np.abs(x).max() * rescaling_max\n",
        "\t'rescaling': True,\n",
        "\t'rescaling_max': 0.999,\n",
        "\t# mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n",
        "\t# happen depends on min_level_db and ref_level_db, causing clipping noise.\n",
        "\t# If False, assertion is added to ensure no clipping happens.o0\n",
        "\t'allow_clipping_in_normalization': True,\n",
        "\n",
        "\t# Mixture of logistic distributions:\n",
        "\t'log_scale_min': float(-32.23619130191664),\n",
        "\n",
        "\t# Model:\n",
        "\t# This should equal to `quantize_channels` if mu-law quantize enabled\n",
        "\t# otherwise num_mixture * 3 (pi, mean, log_scale)\n",
        "\t'out_channels': 10 * 3,\n",
        "\t'layers': 24,\n",
        "\t'stacks': 4,\n",
        "\t'residual_channels': 512,\n",
        "\t'gate_channels': 512,  # split into 2 gropus internally for gated activation\n",
        "\t'skip_out_channels': 256,\n",
        "\t'dropout': 1 - 0.95,\n",
        "\t'kernel_size': 3,\n",
        "\t# If True, apply weight normalization as same as DeepVoice3\n",
        "\t'weight_normalization': True,\n",
        "\t# Use legacy code or not. Default is True since we already provided a model\n",
        "\t# based on the legacy code that can generate high-quality audio.\n",
        "\t# Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n",
        "\t'legacy': True,\n",
        "\n",
        "\t# Local conditioning (set negative value to disable))\n",
        "\t'cin_channels': 80,\n",
        "\t# If True, use transposed convolutions to upsample conditional features,\n",
        "\t# otherwise repeat features to adjust time resolution\n",
        "\t'upsample_conditional_features': True,\n",
        "\t# should np.prod(upsample_scales) == hop_size\n",
        "\t'upsample_scales': [4, 4, 4, 4],\n",
        "\t# Freq axis kernel size for upsampling network\n",
        "\t'freq_axis_kernel_size': 3,\n",
        "\n",
        "\t# Global conditioning (set negative value to disable)\n",
        "\t# currently limited for speaker embedding\n",
        "\t# this should only be enabled for multi-speaker dataset\n",
        "\t'gin_channels': -1,  # i.e., speaker embedding dim\n",
        "\t'n_speakers': -1,\n",
        "\n",
        "\t# Data loader\n",
        "\t'pin_memory': True,\n",
        "\t'num_workers': 2,\n",
        "\n",
        "\t# train/test\n",
        "\t# test size can be specified as portion or num samples\n",
        "\t'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n",
        "\t'test_num_samples': None,\n",
        "\t'random_state': 1234,\n",
        "\n",
        "\t# Loss\n",
        "\n",
        "\t# Training:\n",
        "\t'batch_size': 2,\n",
        "\t'adam_beta1': 0.9,\n",
        "\t'adam_beta2': 0.999,\n",
        "\t'adam_eps': 1e-8,\n",
        "\t'amsgrad': False,\n",
        "\t'initial_learning_rate': 1e-3,\n",
        "\t# see lrschedule.py for available lr_schedule\n",
        "\t'lr_schedule': \"noam_learning_rate_decay\",\n",
        "\t'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n",
        "\t'nepochs': 2000,\n",
        "\t'weight_decay': 0.0,\n",
        "\t'clip_thresh': -1,\n",
        "\t# max time steps can either be specified as sec or steps\n",
        "\t# if both are None, then full audio samples are used in a batch\n",
        "\t'max_time_sec': None,\n",
        "\t'max_time_steps': 8000,\n",
        "\t# Hold moving averaged parameters and use them for evaluation\n",
        "\t'exponential_moving_average': True,\n",
        "\t# averaged = decay * averaged + (1 - decay) * x\n",
        "\t'ema_decay': 0.9999,\n",
        "\n",
        "\t# Save\n",
        "\t# per-step intervals\n",
        "\t'checkpoint_interval': 10000,\n",
        "\t'train_eval_interval': 10000,\n",
        "\t# per-epoch interval\n",
        "\t'test_eval_epoch_interval': 5,\n",
        "\t'save_optimizer_state': True,\n",
        "\n",
        "\t# Eval:\n",
        "})\n",
        "\n",
        "\n",
        "# def hparams_debug_string():\n",
        "# \tvalues = hparams.values()\n",
        "# \thp = ['  %s: %s' % (name, values[name]) for name in sorted(values)]\n",
        "# \treturn 'Hyperparameters:\\n' + '\\n'.join(hp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oqr0SaMjmT2"
      },
      "outputs": [],
      "source": [
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOF9y4XucFVa"
      },
      "outputs": [],
      "source": [
        "def build_model(): #모델 생성\n",
        "\n",
        "    model = getattr(builder, hparams.builder)(\n",
        "        out_channels=hparams.out_channels,\n",
        "        layers=hparams.layers,\n",
        "        stacks=hparams.stacks,\n",
        "        residual_channels=hparams.residual_channels,\n",
        "        gate_channels=hparams.gate_channels,\n",
        "        skip_out_channels=hparams.skip_out_channels,\n",
        "        cin_channels=hparams.cin_channels,\n",
        "        gin_channels=hparams.gin_channels,\n",
        "        weight_normalization=hparams.weight_normalization,\n",
        "        n_speakers=hparams.n_speakers,\n",
        "        dropout=hparams.dropout,\n",
        "        kernel_size=hparams.kernel_size,\n",
        "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
        "        upsample_scales=hparams.upsample_scales,\n",
        "        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n",
        "        scalar_input=True,\n",
        "        legacy=hparams.legacy,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def wavegen(model, c=None, tqdm=tqdm): #파형 생성\n",
        "    \"\"\"Generate waveform samples by WaveNet.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()#평가모드\n",
        "    model.make_generation_fast_() #빠른 생성?\n",
        "\n",
        "    Tc = c.shape[0] #조건부 입력 길이\n",
        "    upsample_factor = hparams.hop_size #업샘플링 비율\n",
        "    # Overwrite length according to feature size\n",
        "    length = Tc * upsample_factor #최종적 오디오 길이\n",
        "\n",
        "    # B x C x T\n",
        "    c = torch.FloatTensor(c.T).unsqueeze(0)#텐서 변환 후 차원 추가\n",
        "\n",
        "    initial_input = torch.zeros(1, 1, 1).fill_(0.0) # 초기 입력값, 처음 데이터는 전부 0으로 이루어짐, 첫번째 윈도우\n",
        "\n",
        "    # Transform data to GPU\n",
        "    initial_input = initial_input.to(device)\n",
        "    c = None if c is None else c.to(device)\n",
        "\n",
        "    with torch.no_grad():#기울기 계산 없애고\n",
        "        y_hat = model.incremental_forward(#오디오 한 샘플씩 생성\n",
        "            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
        "            log_scale_min=hparams.log_scale_min)\n",
        "        #initial - 처음 값\n",
        "        #c - 조건부 입력(멜 스펙트로그램 같은 것)\n",
        "        #t - 생성할 길이\n",
        "        #softmax=확률분포를 통한 각 샘플 생성\n",
        "        #quantization - 양자화 출력 여부\n",
        "        #log_scale - 로그 스케일 최소값, 이하는 0으로 전부 묶음\n",
        "\n",
        "    y_hat = y_hat.view(-1).cpu().data.numpy()#생성된 샘플을 1차원으로 변경, 합해서 후처리 시킴\n",
        "\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etqieCP-7Xtb"
      },
      "outputs": [],
      "source": [
        "#synthesis.py\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Synthesis waveform from trained WaveNet.\n",
        "\n",
        "Modified from https://github.com/r9y9/wavenet_vocoder\n",
        "\"\"\"\n",
        "\n",
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG1BpILHjppf"
      },
      "outputs": [],
      "source": [
        "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
        "device = torch.device(\"cuda\")\n",
        "model = build_model().to(device)\n",
        "checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "for spect in spect_vc:\n",
        "    name = spect[0]\n",
        "    c = spect[1]\n",
        "    print(name)\n",
        "    waveform = wavegen(model, c=c)\n",
        "    sf.write(name+'.wav', waveform, samplerate=16000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1ekp16rP1qOj0y7N3PrfTMGKJFLoOwutO",
      "authorship_tag": "ABX9TyPMXWGBAWcJR+3FfSJajWYG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}