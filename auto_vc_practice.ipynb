{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ekp16rP1qOj0y7N3PrfTMGKJFLoOwutO",
      "authorship_tag": "ABX9TyPZQYM3i0qRFo28SPUJcL/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/auto_vc_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2Vzcr775Ib",
        "outputId": "ff5caf75-dd17-422d-972e-819dabcabef1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wavenet_vocoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAKmnK5n6Gez",
        "outputId": "71901919-9a88-4ce0-d02d-1823ac8a83e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wavenet_vocoder\n",
            "  Downloading wavenet_vocoder-0.1.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.11.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->wavenet_vocoder)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->wavenet_vocoder)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->wavenet_vocoder) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->wavenet_vocoder) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wavenet_vocoder\n",
            "  Building wheel for wavenet_vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet_vocoder: filename=wavenet_vocoder-0.1.1-py3-none-any.whl size=12647 sha256=0183a1aa7d9f9801eababf76b9a445c5389b3efaaf50da952a981630f198188f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/a4/7b/f1d21f96be36a13e9c3948e8c28792bf8962da19781abd9dc8\n",
            "Successfully built wavenet_vocoder\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, wavenet_vocoder\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 wavenet_vocoder-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import soundfile as sf\n",
        "from torch.backends import cudnn\n",
        "from wavenet_vocoder import builder\n",
        "\"\"\"\n",
        "from model_bl import D_VECTOR\n",
        "from model_vc import Generator\n",
        "from hparams import hparams\n",
        "from solver_encoder import Solver\n",
        "from data_loader import get_loader\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "s8YKdCC3ciLI",
        "outputId": "37f7b21a-25b5-40bf-bdef-64593bdc21ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom model_bl import D_VECTOR\\nfrom model_vc import Generator\\nfrom hparams import hparams\\nfrom solver_encoder import Solver\\nfrom data_loader import get_loader\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V34cp_HHalfN"
      },
      "outputs": [],
      "source": [
        "#make_spect.py\n",
        "def butter_highpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def pySTFT(x, fft_length=1024, hop_length=256):\n",
        "\n",
        "    x = np.pad(x, int(fft_length//2), mode='reflect')\n",
        "\n",
        "    noverlap = fft_length - hop_length\n",
        "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)\n",
        "    strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])\n",
        "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "                                             strides=strides)\n",
        "\n",
        "    fft_window = get_window('hann', fft_length, fftbins=True)\n",
        "    result = np.fft.rfft(fft_window * result, n=fft_length).T\n",
        "\n",
        "    return np.abs(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_basis = mel(sr=16000,n_fft=1024,  n_mels=80, fmin=90, fmax=7600).T #멜 스펙트로그램 생성\n",
        "min_level = np.exp(-100 / 20 * np.log(10))\n",
        "b, a = butter_highpass(30, 16000, order=5)\n",
        "\n",
        "\"\"\"\n",
        "# audio file directory\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset'\n",
        "# spectrogram directory\n",
        "targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/results'\n",
        "\"\"\"\n",
        "# audio file directory\n",
        "rootDir = '/content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs'\n",
        "# spectrogram directory\n",
        "targetDir = '/content/drive/MyDrive/코딩공부/deep_daiv/results'\n",
        "\n",
        "dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "for subdir in sorted(subdirList):\n",
        "    print(subdir)\n",
        "    if not os.path.exists(os.path.join(targetDir, subdir)):\n",
        "        os.makedirs(os.path.join(targetDir, subdir))\n",
        "    _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))\n",
        "    prng = RandomState(int(subdir[1:]))\n",
        "    for fileName in sorted(fileList):\n",
        "        # Read audio file\n",
        "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))\n",
        "        # Remove drifting noise\n",
        "        y = signal.filtfilt(b, a, x)\n",
        "        # Ddd a little random noise for model roubstness\n",
        "        wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
        "        # Compute spect\n",
        "        D = pySTFT(wav).T\n",
        "        # Convert to mel and normalize\n",
        "        D_mel = np.dot(D, mel_basis)\n",
        "        D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16\n",
        "        S = np.clip((D_db + 100) / 100, 0, 1)\n",
        "        # save spect\n",
        "        np.save(os.path.join(targetDir, subdir, fileName[:-4]),\n",
        "                S.astype(np.float32), allow_pickle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc3uiuq89uh4",
        "outputId": "3674fae8-2c43-44da-e0ed-04263cbb7c5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: /content/drive/MyDrive/코딩공부/deep_daiv/dataset/wavs\n",
            "p225\n",
            "p226\n",
            "p227\n",
            "p228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_bl.py\n",
        "\n",
        "class D_VECTOR(nn.Module):\n",
        "    \"\"\"d vector speaker embedding.\"\"\"\n",
        "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
        "        super(D_VECTOR, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        embeds = self.embedding(lstm_out[:,-1,:])\n",
        "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        embeds_normalized = embeds.div(norm)\n",
        "        return embeds_normalized\n"
      ],
      "metadata": {
        "id": "i515s4B4a9Jl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make_metadata.py\n",
        "\"\"\"\n",
        "Generate speaker embeddings and metadata for training\n",
        "\"\"\"\n",
        "\n",
        "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda()\n",
        "c_checkpoint = torch.load('/content/drive/MyDrive/코딩공부/deep_daiv/dataset/3000000-BL.ckpt')\n",
        "new_state_dict = OrderedDict()\n",
        "for key, val in c_checkpoint['model_b'].items():\n",
        "    new_key = key[7:]\n",
        "    new_state_dict[new_key] = val\n",
        "C.load_state_dict(new_state_dict)\n",
        "num_uttrs = 10\n",
        "len_crop = 128\n",
        "\n",
        "\n",
        "# Directory containing mel-spectrograms, 별도 데이터셋?\n",
        "rootDir = './spmel'#?\n",
        "dirName, subdirList, _ = next(os.walk(rootDir)) #데이터셋 파일 필요함\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "speakers = []\n",
        "for speaker in sorted(subdirList):\n",
        "    print('Processing speaker: %s' % speaker)\n",
        "    utterances = []\n",
        "    utterances.append(speaker)\n",
        "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
        "\n",
        "    # make speaker embedding\n",
        "    assert len(fileList) >= num_uttrs\n",
        "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
        "    embs = []\n",
        "    for i in range(num_uttrs):\n",
        "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
        "        candidates = np.delete(np.arange(len(fileList)), idx_uttrs)\n",
        "        # choose another utterance if the current one is too short\n",
        "        while tmp.shape[0] < len_crop:\n",
        "            idx_alt = np.random.choice(candidates)\n",
        "            tmp = np.load(os.path.join(dirName, speaker, fileList[idx_alt]))\n",
        "            candidates = np.delete(candidates, np.argwhere(candidates==idx_alt))\n",
        "        left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
        "        melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :]).cuda()\n",
        "        emb = C(melsp)\n",
        "        embs.append(emb.detach().squeeze().cpu().numpy())\n",
        "    utterances.append(np.mean(embs, axis=0))\n",
        "\n",
        "    # create file list\n",
        "    for fileName in sorted(fileList):\n",
        "        utterances.append(os.path.join(speaker,fileName))\n",
        "    speakers.append(utterances)\n",
        "\n",
        "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
        "    pickle.dump(speakers, handle)\n"
      ],
      "metadata": {
        "id": "KziVBX5dbD9M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "5c866759-0f50-457b-c87b-cb0e8dfb98cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7389aa433a9d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Directory containing mel-spectrograms, 별도 데이터셋?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mrootDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./spmel'\u001b[0m\u001b[0;31m#?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdirName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdirList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#데이터셋 파일 필요함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found directory: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdirName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loader.py\n",
        "\n",
        "\n",
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, len_crop):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.len_crop = len_crop\n",
        "        self.step = 10\n",
        "\n",
        "        metaname = os.path.join(self.root_dir, \"train.pkl\")\n",
        "        meta = pickle.load(open(metaname, \"rb\"))\n",
        "\n",
        "        \"\"\"Load data using multiprocessing\"\"\"\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the dataset...')\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]\n",
        "            for j, tmp in enumerate(sbmt):\n",
        "                if j < 2:  # fill in speaker id and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else: # load the mel-spectrograms\n",
        "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # pick a random speaker\n",
        "        dataset = self.train_dataset\n",
        "        list_uttrs = dataset[index]\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        # pick random uttr with random crop\n",
        "        a = np.random.randint(2, len(list_uttrs))\n",
        "        tmp = list_uttrs[a]\n",
        "        if tmp.shape[0] < self.len_crop:\n",
        "            len_pad = self.len_crop - tmp.shape[0]\n",
        "            uttr = np.pad(tmp, ((0,len_pad),(0,0)), 'constant')\n",
        "        elif tmp.shape[0] > self.len_crop:\n",
        "            left = np.random.randint(tmp.shape[0]-self.len_crop)\n",
        "            uttr = tmp[left:left+self.len_crop, :]\n",
        "        else:\n",
        "            uttr = tmp\n",
        "\n",
        "        return uttr, emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size=16, len_crop=128, num_workers=0):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(root_dir, len_crop)\n",
        "\n",
        "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  worker_init_fn=worker_init_fn)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QSI7SwAXbfj_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델"
      ],
      "metadata": {
        "id": "G6zEd5YR_knf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_vc.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module): #합성곱 놈\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class Encoder(nn.Module): #인코더 모델\n",
        "    \"\"\"Encoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, freq):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_neck = dim_neck\n",
        "        self.freq = freq\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(80+dim_emb if i==0 else 512,\n",
        "                         512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(512))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, c_org):\n",
        "        x = x.squeeze(1).transpose(2,1)\n",
        "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
        "        x = torch.cat((x, c_org), dim=1)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "        out_forward = outputs[:, :, :self.dim_neck]\n",
        "        out_backward = outputs[:, :, self.dim_neck:]\n",
        "\n",
        "        codes = []\n",
        "        for i in range(0, outputs.size(1), self.freq):\n",
        "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
        "\n",
        "        return codes\n",
        "\n",
        "\n",
        "class Decoder(nn.Module): #디코더 모델\n",
        "    \"\"\"Decoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
        "\n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(dim_pre,\n",
        "                         dim_pre,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(dim_pre))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
        "\n",
        "        self.linear_projection = LinearNorm(1024, 80)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #self.lstm1.flatten_parameters()\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        outputs, _ = self.lstm2(x)\n",
        "\n",
        "        decoder_output = self.linear_projection(outputs)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "class Postnet(nn.Module): #포스트넷?\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(80, 512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(512))\n",
        "        )\n",
        "\n",
        "        for i in range(1, 5 - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(512,\n",
        "                             512,\n",
        "                             kernel_size=5, stride=1,\n",
        "                             padding=2,\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(512))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(512, 80,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(80))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = torch.tanh(self.convolutions[i](x))\n",
        "\n",
        "        x = self.convolutions[-1](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator(nn.Module): #생성기 = 인코더+디코더+포스트넷\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
        "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, x, c_org, c_trg):\n",
        "\n",
        "        codes = self.encoder(x, c_org)\n",
        "        if c_trg is None:\n",
        "            return torch.cat(codes, dim=-1)\n",
        "\n",
        "        tmp = []\n",
        "        for code in codes:\n",
        "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
        "        code_exp = torch.cat(tmp, dim=1)\n",
        "\n",
        "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
        "\n",
        "        mel_outputs = self.decoder(encoder_outputs)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
        "\n",
        "        mel_outputs = mel_outputs.unsqueeze(1)\n",
        "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
        "\n",
        "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "hqCzLhYybRMT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "LLAX8yLy_sDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n"
      ],
      "metadata": {
        "id": "lnlPJX6JK57d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Model configuration.\n",
        "parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
        "parser.add_argument('--dim_neck', type=int, default=16)\n",
        "parser.add_argument('--dim_emb', type=int, default=256)\n",
        "parser.add_argument('--dim_pre', type=int, default=512)\n",
        "parser.add_argument('--freq', type=int, default=16)\n",
        "\n",
        "# Training configuration.\n",
        "parser.add_argument('--data_dir', type=str, default='./spmel')\n",
        "parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
        "parser.add_argument('--num_iters', type=int, default=1000000, help='number of total iterations')\n",
        "parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
        "\n",
        "# Miscellaneous.\n",
        "parser.add_argument('--log_step', type=int, default=10)\n",
        "\n",
        "config = parser.parse_args()"
      ],
      "metadata": {
        "id": "kmwCxNSLcN1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "720f93c2-e3ca-41cc-d304-ea7938975df2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--lambda_cd LAMBDA_CD] [--dim_neck DIM_NECK]\n",
            "                                [--dim_emb DIM_EMB] [--dim_pre DIM_PRE] [--freq FREQ]\n",
            "                                [--data_dir DATA_DIR] [--batch_size BATCH_SIZE]\n",
            "                                [--num_iters NUM_ITERS] [--len_crop LEN_CROP]\n",
            "                                [--log_step LOG_STEP]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-414ea9ed-810e-4007-9fb8-24828588bc87.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solver_encoder.py\n",
        "\n",
        "class Solver(object): #음성신호 학습\n",
        "\n",
        "    def __init__(self, vcc_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.vcc_loader = vcc_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.lambda_cd = config.lambda_cd\n",
        "        self.dim_neck = config.dim_neck\n",
        "        self.dim_emb = config.dim_emb\n",
        "        self.dim_pre = config.dim_pre\n",
        "        self.freq = config.freq\n",
        "\n",
        "        # Training configurations.\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
        "        self.log_step = config.log_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), 0.0001)\n",
        "\n",
        "        self.G.to(self.device)\n",
        "\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    #=====================================================================================================================================#\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        # Set data loader.\n",
        "        data_loader = self.vcc_loader\n",
        "\n",
        "        # Print logs in specified order\n",
        "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Fetch data.\n",
        "            try:\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, emb_org = next(data_iter)\n",
        "\n",
        "\n",
        "            x_real = x_real.to(self.device)\n",
        "            emb_org = emb_org.to(self.device)\n",
        "\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               2. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            self.G = self.G.train()\n",
        "\n",
        "            # Identity mapping loss\n",
        "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
        "            g_loss_id = F.mse_loss(x_real, x_identic)\n",
        "            g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)\n",
        "\n",
        "            # Code semantic loss.\n",
        "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
        "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
        "\n",
        "\n",
        "            # Backward and optimize.\n",
        "            g_loss = g_loss_id + g_loss_id_psnt + self.lambda_cd * g_loss_cd\n",
        "            self.reset_grad()\n",
        "            g_loss.backward()\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['G/loss_id'] = g_loss_id.item()\n",
        "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
        "            loss['G/loss_cd'] = g_loss_cd.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag in keys:\n",
        "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
        "                print(log)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXgGFoXnb0De"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "    # Data loader.\n",
        "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
        "\n",
        "solver = Solver(vcc_loader, config)\n",
        "\n",
        "solver.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "C-tF8qGvMpXK",
        "outputId": "361f35d1-03b1-4d45-c13f-599dde60294d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-04e12e321f52>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Data loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvcc_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hparams.py\n",
        "# NOTE: If you want full control for model architecture. please take a look\n",
        "# at the code and change whatever you want. Some hyper parameters are hardcoded.\n",
        "\n",
        "\n",
        "class Map(dict):\n",
        "\t\"\"\"\n",
        "    Example:\n",
        "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
        "\n",
        "    Credits to epool:\n",
        "    https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, *args, **kwargs):\n",
        "\t\tsuper(Map, self).__init__(*args, **kwargs)\n",
        "\t\tfor arg in args:\n",
        "\t\t\tif isinstance(arg, dict):\n",
        "\t\t\t\tfor k, v in arg.items():\n",
        "\t\t\t\t\tself[k] = v\n",
        "\n",
        "\t\tif kwargs:\n",
        "\t\t\tfor k, v in kwargs.iteritems():\n",
        "\t\t\t\tself[k] = v\n",
        "\n",
        "\tdef __getattr__(self, attr):\n",
        "\t\treturn self.get(attr)\n",
        "\n",
        "\tdef __setattr__(self, key, value):\n",
        "\t\tself.__setitem__(key, value)\n",
        "\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tsuper(Map, self).__setitem__(key, value)\n",
        "\t\tself.__dict__.update({key: value})\n",
        "\n",
        "\tdef __delattr__(self, item):\n",
        "\t\tself.__delitem__(item)\n",
        "\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tsuper(Map, self).__delitem__(key)\n",
        "\t\tdel self.__dict__[key]\n",
        "\n",
        "\n",
        "# Default hyperparameters:\n",
        "hparams = Map({ #하이퍼 파라미터 묶음\n",
        "\t'name': \"wavenet_vocoder\",\n",
        "\n",
        "\t# Convenient model builder\n",
        "\t'builder': \"wavenet\",\n",
        "\n",
        "\t# Input type:\n",
        "\t# 1. raw [-1, 1]\n",
        "\t# 2. mulaw [-1, 1]\n",
        "\t# 3. mulaw-quantize [0, mu]\n",
        "\t# If input_type is raw or mulaw, network assumes scalar input and\n",
        "\t# discretized mixture of logistic distributions output, otherwise one-hot\n",
        "\t# input and softmax output are assumed.\n",
        "\t# **NOTE**: if you change the one of the two parameters below, you need to\n",
        "\t# re-run preprocessing before training.\n",
        "\t'input_type': \"raw\",\n",
        "\t'quantize_channels': 65536,  # 65536 or 256\n",
        "\n",
        "\t# Audio:\n",
        "\t'sample_rate': 16000,\n",
        "\t# this is only valid for mulaw is True\n",
        "\t'silence_threshold': 2,\n",
        "\t'num_mels': 80,\n",
        "\t'fmin': 125,\n",
        "\t'fmax': 7600,\n",
        "\t'fft_size': 1024,\n",
        "\t# shift can be specified by either hop_size or frame_shift_ms\n",
        "\t'hop_size': 256,\n",
        "\t'frame_shift_ms': None,\n",
        "\t'min_level_db': -100,\n",
        "\t'ref_level_db': 20,\n",
        "\t# whether to rescale waveform or not.\n",
        "\t# Let x is an input waveform, rescaled waveform y is given by:\n",
        "\t# y = x / np.abs(x).max() * rescaling_max\n",
        "\t'rescaling': True,\n",
        "\t'rescaling_max': 0.999,\n",
        "\t# mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n",
        "\t# happen depends on min_level_db and ref_level_db, causing clipping noise.\n",
        "\t# If False, assertion is added to ensure no clipping happens.o0\n",
        "\t'allow_clipping_in_normalization': True,\n",
        "\n",
        "\t# Mixture of logistic distributions:\n",
        "\t'log_scale_min': float(-32.23619130191664),\n",
        "\n",
        "\t# Model:\n",
        "\t# This should equal to `quantize_channels` if mu-law quantize enabled\n",
        "\t# otherwise num_mixture * 3 (pi, mean, log_scale)\n",
        "\t'out_channels': 10 * 3,\n",
        "\t'layers': 24,\n",
        "\t'stacks': 4,\n",
        "\t'residual_channels': 512,\n",
        "\t'gate_channels': 512,  # split into 2 gropus internally for gated activation\n",
        "\t'skip_out_channels': 256,\n",
        "\t'dropout': 1 - 0.95,\n",
        "\t'kernel_size': 3,\n",
        "\t# If True, apply weight normalization as same as DeepVoice3\n",
        "\t'weight_normalization': True,\n",
        "\t# Use legacy code or not. Default is True since we already provided a model\n",
        "\t# based on the legacy code that can generate high-quality audio.\n",
        "\t# Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n",
        "\t'legacy': True,\n",
        "\n",
        "\t# Local conditioning (set negative value to disable))\n",
        "\t'cin_channels': 80,\n",
        "\t# If True, use transposed convolutions to upsample conditional features,\n",
        "\t# otherwise repeat features to adjust time resolution\n",
        "\t'upsample_conditional_features': True,\n",
        "\t# should np.prod(upsample_scales) == hop_size\n",
        "\t'upsample_scales': [4, 4, 4, 4],\n",
        "\t# Freq axis kernel size for upsampling network\n",
        "\t'freq_axis_kernel_size': 3,\n",
        "\n",
        "\t# Global conditioning (set negative value to disable)\n",
        "\t# currently limited for speaker embedding\n",
        "\t# this should only be enabled for multi-speaker dataset\n",
        "\t'gin_channels': -1,  # i.e., speaker embedding dim\n",
        "\t'n_speakers': -1,\n",
        "\n",
        "\t# Data loader\n",
        "\t'pin_memory': True,\n",
        "\t'num_workers': 2,\n",
        "\n",
        "\t# train/test\n",
        "\t# test size can be specified as portion or num samples\n",
        "\t'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n",
        "\t'test_num_samples': None,\n",
        "\t'random_state': 1234,\n",
        "\n",
        "\t# Loss\n",
        "\n",
        "\t# Training:\n",
        "\t'batch_size': 2,\n",
        "\t'adam_beta1': 0.9,\n",
        "\t'adam_beta2': 0.999,\n",
        "\t'adam_eps': 1e-8,\n",
        "\t'amsgrad': False,\n",
        "\t'initial_learning_rate': 1e-3,\n",
        "\t# see lrschedule.py for available lr_schedule\n",
        "\t'lr_schedule': \"noam_learning_rate_decay\",\n",
        "\t'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n",
        "\t'nepochs': 2000,\n",
        "\t'weight_decay': 0.0,\n",
        "\t'clip_thresh': -1,\n",
        "\t# max time steps can either be specified as sec or steps\n",
        "\t# if both are None, then full audio samples are used in a batch\n",
        "\t'max_time_sec': None,\n",
        "\t'max_time_steps': 8000,\n",
        "\t# Hold moving averaged parameters and use them for evaluation\n",
        "\t'exponential_moving_average': True,\n",
        "\t# averaged = decay * averaged + (1 - decay) * x\n",
        "\t'ema_decay': 0.9999,\n",
        "\n",
        "\t# Save\n",
        "\t# per-step intervals\n",
        "\t'checkpoint_interval': 10000,\n",
        "\t'train_eval_interval': 10000,\n",
        "\t# per-epoch interval\n",
        "\t'test_eval_epoch_interval': 5,\n",
        "\t'save_optimizer_state': True,\n",
        "\n",
        "\t# Eval:\n",
        "})\n",
        "\n",
        "\n",
        "def hparams_debug_string():\n",
        "\tvalues = hparams.values()\n",
        "\thp = ['  %s: %s' % (name, values[name]) for name in sorted(values)]\n",
        "\treturn 'Hyperparameters:\\n' + '\\n'.join(hp)"
      ],
      "metadata": {
        "id": "IKvwjnd0bTi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#synthesis.py\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Synthesis waveform from trained WaveNet.\n",
        "\n",
        "Modified from https://github.com/r9y9/wavenet_vocoder\n",
        "\"\"\"\n",
        "\n",
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
      ],
      "metadata": {
        "id": "etqieCP-7Xtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(): #모델 생성\n",
        "\n",
        "    model = getattr(builder, hparams.builder)(\n",
        "        out_channels=hparams.out_channels,\n",
        "        layers=hparams.layers,\n",
        "        stacks=hparams.stacks,\n",
        "        residual_channels=hparams.residual_channels,\n",
        "        gate_channels=hparams.gate_channels,\n",
        "        skip_out_channels=hparams.skip_out_channels,\n",
        "        cin_channels=hparams.cin_channels,\n",
        "        gin_channels=hparams.gin_channels,\n",
        "        weight_normalization=hparams.weight_normalization,\n",
        "        n_speakers=hparams.n_speakers,\n",
        "        dropout=hparams.dropout,\n",
        "        kernel_size=hparams.kernel_size,\n",
        "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
        "        upsample_scales=hparams.upsample_scales,\n",
        "        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n",
        "        scalar_input=True,\n",
        "        legacy=hparams.legacy,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def wavegen(model, c=None, tqdm=tqdm): #파형 생성\n",
        "    \"\"\"Generate waveform samples by WaveNet.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.make_generation_fast_()\n",
        "\n",
        "    Tc = c.shape[0]\n",
        "    upsample_factor = hparams.hop_size\n",
        "    # Overwrite length according to feature size\n",
        "    length = Tc * upsample_factor\n",
        "\n",
        "    # B x C x T\n",
        "    c = torch.FloatTensor(c.T).unsqueeze(0)\n",
        "\n",
        "    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n",
        "\n",
        "    # Transform data to GPU\n",
        "    initial_input = initial_input.to(device)\n",
        "    c = None if c is None else c.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = model.incremental_forward(\n",
        "            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
        "            log_scale_min=hparams.log_scale_min)\n",
        "\n",
        "    y_hat = y_hat.view(-1).cpu().data.numpy()\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "dOF9y4XucFVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxsT_Wq5cUTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}