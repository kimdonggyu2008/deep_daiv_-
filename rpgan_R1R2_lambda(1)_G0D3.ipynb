{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/rpgan_R1R2_lambda(1)_G0D3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7NiSG0e9Ce8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHgPVpTkbfUa"
      },
      "outputs": [],
      "source": [
        "!pip install wavenet_vocoder\n",
        "!pip install librosa==0.9.1\n",
        "!pip install wandb\n",
        "!pip install \"pip<24.1\"\n",
        "!pip install git+https://github.com/One-sixth/fairseq.git\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-UlSgqBb1NN"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/코딩공부/deep_daiv/PAPER_SUBMIT\n",
        "# 본인 폴더 경로로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6DJFPig4vrs"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "!pip install --upgrade scoreq\n",
        "!pip install torch-pesq\n",
        "!pip install praat-parselmouth\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8M5ZQxcbwrH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from multiprocessing import Process, Manager\n",
        "import easydict\n",
        "from torch.backends import cudnn\n",
        "import time\n",
        "import datetime\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from wavenet_vocoder import builder\n",
        "from torch_pesq import PesqLoss\n",
        "import scoreq\n",
        "from torchinfo import summary\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcaXmCjQcii6"
      },
      "outputs": [],
      "source": [
        "#utils\n",
        "import glob\n",
        "import os\n",
        "import matplotlib\n",
        "import torch\n",
        "from torch.nn.utils import weight_norm\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "def plot_spectrogram(spectrogram):\n",
        "    fig, ax = plt.subplots(figsize=(10, 2))\n",
        "    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n",
        "                   interpolation='none')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    plt.close()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "\n",
        "def apply_weight_norm(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        weight_norm(m)\n",
        "\n",
        "\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def save_checkpoint(filepath, obj):\n",
        "    print(\"Saving checkpoint to {}\".format(filepath))\n",
        "    torch.save(obj, filepath)\n",
        "    print(\"Complete.\")\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '????????')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return None\n",
        "    return sorted(cp_list)[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsFDAKr8c_z9"
      },
      "outputs": [],
      "source": [
        "#env\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def build_env(config, config_name, path): # env_path : cp_hifigan/config_v1.json에 저장\n",
        "    t_path = os.path.join(path, config_name)\n",
        "    if config != t_path:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        shutil.copyfile(config, os.path.join(path, config_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpdzGeDOdSpl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from librosa.util import normalize\n",
        "from scipy.io.wavfile import read\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    return np.exp(x) / C\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "mel_basis = {}\n",
        "hann_window = {}\n",
        "\n",
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    # Apply padding and calculate STFT\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "\n",
        "    # STFT의 결과에서 절대값을 취해 크기만 남김\n",
        "    spec = spec.abs()\n",
        "\n",
        "    # spec의 차원이 (n_frames, frequency_bins) 형식으로 맞는지 확인\n",
        "    if len(spec.shape) == 3:\n",
        "        spec = spec.squeeze(1)\n",
        "\n",
        "    # mel_basis와 spec의 차원이 맞는지 확인한 후 matmul 실행\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n",
        "\n",
        "def get_dataset_filelist(a):\n",
        "    with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n",
        "\n",
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 device=None, fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split\n",
        "        self.n_fft = n_fft\n",
        "        self.num_mels = num_mels\n",
        "        self.hop_size = hop_size\n",
        "        self.win_size = win_size\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "        self.fmax_loss = fmax_loss\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.device = device\n",
        "        self.fine_tuning = fine_tuning\n",
        "        self.base_mels_path = base_mels_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename)\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning:\n",
        "                audio = normalize(audio) * 0.95\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate:\n",
        "                raise ValueError(f\"{sampling_rate} SR doesn't match target {self.sampling_rate} SR\")\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio).unsqueeze(0)\n",
        "\n",
        "        if not self.fine_tuning:  # self.fine_tuning : False\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "        else:\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "\n",
        "            if len(mel.shape) < 3:\n",
        "                mel = mel.unsqueeze(0)\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n",
        "\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj_JJYj-dTdw"
      },
      "source": [
        "## hifi-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NnNmsw9dNvA"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "\n",
        "LRELU_SLOPE = 0.2\n",
        "\n",
        "\n",
        "class ResBlock1(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock1, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs1:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.convs2:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class ResBlock2(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1])))\n",
        "        ])\n",
        "        self.convs.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c in self.convs:\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
        "                                k, u, padding=(k-u)//2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = h.upsample_initial_channel//(2**(i+1))\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(h, ch, k, d))\n",
        "\n",
        "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_pre(x)\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        remove_weight_norm(self.conv_post)\n",
        "\n",
        "\n",
        "    def summary(self, input_shape):\n",
        "        \"\"\" 모델 구조를 출력하는 summary 메서드 추가 \"\"\"\n",
        "        return summary(self, input_size=input_shape, verbose=2)\n",
        "\n",
        "\n",
        "class DiscriminatorP(torch.nn.Module):\n",
        "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
        "        super(DiscriminatorP, self).__init__()\n",
        "        self.period = period\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        # 1d to 2d\n",
        "        b, c, t = x.shape\n",
        "        if t % self.period != 0: # pad first\n",
        "            n_pad = self.period - (t % self.period)\n",
        "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
        "            t = t + n_pad\n",
        "        x = x.view(b, c, t // self.period, self.period)\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "class MultiPeriodDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiPeriodDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorP(2),\n",
        "            DiscriminatorP(3),\n",
        "            DiscriminatorP(5),\n",
        "            DiscriminatorP(7),\n",
        "            DiscriminatorP(11),\n",
        "            #레이어 추가\n",
        "            DiscriminatorP(13), #1600만\n",
        "            DiscriminatorP(17),\n",
        "            DiscriminatorP(19),\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "    def summary(self, input_shape):\n",
        "        \"\"\" MPD 모델 구조를 출력하는 summary 메서드 추가 \"\"\"\n",
        "        return summary(self, input_size=[input_shape, input_shape], verbose=2)\n",
        "\n",
        "class DiscriminatorS(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(DiscriminatorS, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n",
        "            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
        "            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "\n",
        "class MultiScaleDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorS(use_spectral_norm=True),\n",
        "            DiscriminatorS(),\n",
        "            DiscriminatorS(),\n",
        "            #레이어 3개 추가\n",
        "            DiscriminatorS(),# 3000만\n",
        "            DiscriminatorS(),\n",
        "            DiscriminatorS(),\n",
        "\n",
        "        ])\n",
        "        self.meanpools = nn.ModuleList([\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            # #레이어 3개 추가\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            if i != 0:\n",
        "                y = self.meanpools[i-1](y)\n",
        "                y_hat = self.meanpools[i-1](y_hat)\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "    def summary(self, input_shape):\n",
        "        \"\"\" MSD 모델 구조를 출력하는 summary 메서드 추가 \"\"\"\n",
        "        return summary(self, input_size=[input_shape, input_shape], verbose=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khEcCvZMdXoX"
      },
      "source": [
        "# hifi - train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6VRv4S6vWBn"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"4c7d91ca2cd073dc0f1c148b6e4bacff713df5c6\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ayMpwEBn8Ak"
      },
      "outputs": [],
      "source": [
        "# WandB 프로젝트 초기화\n",
        "name = \"RPGAN_R1R2_lambda(1)_G0D3\"   # ex) WGAN (노션 참고)\n",
        "id = 'rpgan_r1r2_lambda1_G0D3'     # 프로젝트마다 고유 id 부여 (실험마다 다르게 설정해야함, 만약 전 실험을 이어서 진행하고 싶다면 같은 id 기재)\n",
        "wandb.init(project=\"RPGAN_R1R2_G0D3\", name=name ,id = id, resume = 'allow',settings=wandb.Settings(init_timeout=1200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkeNI83klNpR"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oNUYwggo6Kr"
      },
      "source": [
        "RPGAN Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDfcnyJnlLkf"
      },
      "outputs": [],
      "source": [
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            loss += torch.mean(torch.abs(rl - gl))\n",
        "\n",
        "    return loss*2\n",
        "\n",
        "'''\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses\n",
        "'''\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs): #모양 바꿈\n",
        "     loss = 0\n",
        "     for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "         relativistic_logits = dr - dg\n",
        "         loss += torch.mean(F.softplus(-relativistic_logits))\n",
        "     return loss\n",
        "\n",
        "def generator_loss(disc_real_outputs, disc_generated_outputs): #discriminator가 뱉은 값을 받아서 업뎃을 해야지\n",
        "     loss = 0\n",
        "     for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "         relativistic_logits = dr - dg\n",
        "         loss += torch.mean(F.softplus(relativistic_logits))\n",
        "     return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy0NS8HHtotj"
      },
      "outputs": [],
      "source": [
        "from torchaudio.transforms import Resample\n",
        "import tempfile\n",
        "import librosa\n",
        "\n",
        "def lsd(est ,target):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim = 1) ** 0.5\n",
        "    lsd = torch.mean(squared_diff, dim = 0)\n",
        "    return lsd\n",
        "\n",
        "def lsd_hf(est, target, hf_ratio=0.25):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "\n",
        "    # Define high-frequency range\n",
        "    num_freq_bins = est.shape[0]\n",
        "    hf_start = int(num_freq_bins * (1 - hf_ratio))  # Starting index for high frequencies\n",
        "\n",
        "    # Focus on high-frequency bands\n",
        "    est_hf = est[hf_start:, :]\n",
        "    target_hf = target[hf_start:, :]\n",
        "\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target_hf + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est_hf + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim=1) ** 0.5\n",
        "    lsd_hf = torch.mean(squared_diff, dim=0)\n",
        "\n",
        "    return lsd_hf\n",
        "\n",
        "def extract_f0_from_audio(audio, sr, fmin=50, fmax=500):\n",
        "    audio_np = audio.cpu().numpy()  # Convert to numpy for librosa\n",
        "    f0, voiced_flag, _ = librosa.pyin(audio_np, fmin=fmin, fmax=fmax, sr=sr)\n",
        "    f0 = torch.tensor(f0, dtype=torch.float32)  # Convert back to tensor\n",
        "    f0[~torch.tensor(voiced_flag, dtype=torch.bool)] = 0  # Set unvoiced regions to 0\n",
        "    return f0\n",
        "\n",
        "def f0_rmse(f0_pred, f0_target):\n",
        "    assert f0_pred.shape == f0_target.shape, \"F0 shapes must match.\"\n",
        "    squared_error = (f0_pred - f0_target) ** 2\n",
        "    mse = torch.mean(squared_error)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A63eywq8lO-E"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f9CkiPrUdZAe"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DistributedSampler, DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import easydict\n",
        "import torchaudio\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#penalty 계산 함수\n",
        "def compute_r1_penalty(discriminators, real_samples, device, gamma=10):\n",
        "    \"\"\"\n",
        "    Computes the R1 regularization penalty for each discriminator in the given discriminators list.\n",
        "    Args:\n",
        "        discriminators: A list of individual discriminators (DiscriminatorP or DiscriminatorS).\n",
        "        real_samples: The real samples (y).\n",
        "        device: The device (cuda or cpu).\n",
        "        gamma: The regularization weight (default: 10.0).\n",
        "    Returns:\n",
        "        r1_penalty: The computed R1 penalty.\n",
        "    \"\"\"\n",
        "    real_samples = real_samples.requires_grad_(True)  # Enable gradient tracking\n",
        "    r1_penalties = []\n",
        "\n",
        "    for d in discriminators:\n",
        "        real_outputs, _ = d(real_samples)  # Get discriminator outputs for real samples\n",
        "        grad_real = torch.autograd.grad(\n",
        "            outputs=real_outputs.sum(),\n",
        "            inputs=real_samples,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]  # Compute gradients\n",
        "\n",
        "        r1_penalty = (grad_real.norm(2, dim=(1, 2)) ** 2).mean()  # Compute gradient norm squared and take mean\n",
        "        r1_penalties.append(r1_penalty)\n",
        "\n",
        "    return (gamma / 2) * sum(r1_penalties)  # Apply weighting factor\n",
        "\n",
        "def compute_r2_penalty(discriminators, fake_samples, device, gamma=10):\n",
        "    \"\"\"\n",
        "    Computes the R2 regularization penalty for each discriminator in the given discriminators list.\n",
        "    Args:\n",
        "        discriminators: A list of individual discriminators (DiscriminatorP or DiscriminatorS).\n",
        "        fake_samples: The fake samples (generated by the generator).\n",
        "        device: The device (cuda or cpu).\n",
        "        gamma: The regularization weight (default: 10.0).\n",
        "    Returns:\n",
        "        r2_penalty: The computed R2 penalty.\n",
        "    \"\"\"\n",
        "    fake_samples = fake_samples.requires_grad_(True)  # Enable gradient tracking\n",
        "    r2_penalties = []\n",
        "\n",
        "    for d in discriminators:\n",
        "        fake_outputs, _ = d(fake_samples)  # Get discriminator outputs for fake samples\n",
        "        grad_fake = torch.autograd.grad(\n",
        "            outputs=fake_outputs.sum(),\n",
        "            inputs=fake_samples,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]  # Compute gradients\n",
        "\n",
        "        r2_penalty = (grad_fake.norm(2, dim=(1, 2)) ** 2).mean()  # Compute gradient norm squared and take mean\n",
        "        r2_penalties.append(r2_penalty)\n",
        "\n",
        "    return (gamma / 2) * sum(r2_penalties)  # Apply weighting factor\n",
        "\n",
        "def train(rank, a, h): # rank : 0\n",
        "    # SCOREQ 초기화\n",
        "    scoreq_nr = scoreq.Scoreq(data_domain='natural', mode='nr')  # No-reference\n",
        "\n",
        "    # PESQ 초기화\n",
        "    pesq = PesqLoss(0.5,h.sampling_rate).eval()\n",
        "    for param in pesq.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # # lambda 값 설정\n",
        "    # ex) loss_scale_formant = 0.001\n",
        "    # ex) loss_scale_r1r2 = #\n",
        "\n",
        "    if h.num_gpus > 1:\n",
        "        init_process_group(backend=h.dist_config['dist_backend'], init_method=h.dist_config['dist_url'],\n",
        "                           world_size=h.dist_config['world_size'] * h.num_gpus, rank=rank)\n",
        "\n",
        "    torch.cuda.manual_seed(h.seed)\n",
        "    device = torch.device('cuda:{:d}'.format(rank))\n",
        "\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    mpd = MultiPeriodDiscriminator().to(device)\n",
        "    msd = MultiScaleDiscriminator().to(device)\n",
        "\n",
        "    if rank == 0:\n",
        "        os.makedirs(a.checkpoint_path, exist_ok=True)\n",
        "        print(\"checkpoints directory : \", a.checkpoint_path)\n",
        "        #크기 보기\n",
        "        # print(\"\\n========== Generator Model ==========\")\n",
        "        # generator.summary(input_shape=(1, h.num_mels, h.segment_size // h.hop_size))  # ✅ model.summary()처럼 사용 가능!\n",
        "\n",
        "        # print(\"\\n========== Multi-Period Discriminator (MPD) ==========\")\n",
        "        # mpd.summary(input_shape=(1, 1, h.segment_size))  # ✅ 입력 2개 전달\n",
        "\n",
        "        # print(\"\\n========== Multi-Scale Discriminator (MSD) ==========\")\n",
        "        # msd.summary(input_shape=(1, 1, h.segment_size))  # ✅ 입력 2개 전달\n",
        "\n",
        "    if os.path.isdir(a.checkpoint_path):                 # 만약 체크포인트 폴더에 g_ 또는 do_로 시작하는 파일이 있다면 체크포인트 로드됨.\n",
        "        cp_g = scan_checkpoint(a.checkpoint_path, 'g_')\n",
        "        cp_do = scan_checkpoint(a.checkpoint_path, 'do_')\n",
        "\n",
        "    steps = 0\n",
        "    if cp_g is None or cp_do is None:         # 처음부터 학습\n",
        "        state_dict_do = None\n",
        "        last_epoch = -1\n",
        "    else:                                     # 체크포인트부터 학습\n",
        "        state_dict_g = load_checkpoint(cp_g, device)\n",
        "        state_dict_do = load_checkpoint(cp_do, device)\n",
        "        generator.load_state_dict(state_dict_g['generator'])\n",
        "        mpd.load_state_dict(state_dict_do['mpd'])\n",
        "        msd.load_state_dict(state_dict_do['msd'])\n",
        "        steps = state_dict_do['steps'] + 1\n",
        "        last_epoch = state_dict_do['epoch']\n",
        "\n",
        "    if h.num_gpus > 1:\n",
        "        generator = DistributedDataParallel(generator, device_ids=[rank]).to(device)\n",
        "        mpd = DistributedDataParallel(mpd, device_ids=[rank]).to(device)\n",
        "        msd = DistributedDataParallel(msd, device_ids=[rank]).to(device)\n",
        "\n",
        "\n",
        "    # 옵티마이저 변경(?) - 후보 SGD, MOMENTUM, ADAM\n",
        "    optim_g = torch.optim.AdamW(generator.parameters(), h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "    optim_d = torch.optim.AdamW(itertools.chain(msd.parameters(), mpd.parameters()),\n",
        "                                h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "\n",
        "    if state_dict_do is not None:\n",
        "        optim_g.load_state_dict(state_dict_do['optim_g'])\n",
        "        optim_d.load_state_dict(state_dict_do['optim_d'])\n",
        "\n",
        "    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=h.lr_decay, last_epoch=last_epoch)\n",
        "    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=h.lr_decay, last_epoch=last_epoch)\n",
        "\n",
        "    training_filelist, validation_filelist = get_dataset_filelist(a)\n",
        "\n",
        "    trainset = MelDataset(training_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
        "                          h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, n_cache_reuse=0,\n",
        "                          shuffle=False if h.num_gpus > 1 else True, fmax_loss=h.fmax_for_loss, device=device,\n",
        "                          fine_tuning=a.fine_tuning, base_mels_path=a.input_mels_dir)\n",
        "\n",
        "    train_sampler = DistributedSampler(trainset) if h.num_gpus > 1 else None\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=h.num_workers, shuffle=False,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=h.batch_size,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    if rank == 0:\n",
        "        validset = MelDataset(validation_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
        "                              h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, False, False, n_cache_reuse=0,\n",
        "                              fmax_loss=h.fmax_for_loss, device=device, fine_tuning=a.fine_tuning,\n",
        "                              base_mels_path=a.input_mels_dir)\n",
        "        validation_loader = DataLoader(validset, num_workers=1, shuffle=False,\n",
        "                                       sampler=None,\n",
        "                                       batch_size=1,\n",
        "                                       pin_memory=True,\n",
        "                                       drop_last=True)\n",
        "\n",
        "        sw = SummaryWriter(os.path.join(a.checkpoint_path, 'logs'))\n",
        "\n",
        "\n",
        "    generator.train()\n",
        "    mpd.train()\n",
        "    msd.train()\n",
        "    for epoch in range(max(0, last_epoch), a.training_epochs):  # 200 epoch\n",
        "        if rank == 0:\n",
        "            start = time.time()\n",
        "            print(\"Epoch: {}\".format(epoch+1))\n",
        "\n",
        "        if h.num_gpus > 1:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if rank == 0:\n",
        "                start_b = time.time()\n",
        "            x, y, _, y_mel = batch\n",
        "            x = torch.autograd.Variable(x.to(device, non_blocking=True))\n",
        "            y = torch.autograd.Variable(y.to(device, non_blocking=True))\n",
        "            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n",
        "            y = y.unsqueeze(1)\n",
        "\n",
        "            y_g_hat = generator(x)\n",
        "            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size,\n",
        "                                          h.fmin, h.fmax_for_loss)\n",
        "\n",
        "            optim_d.zero_grad()\n",
        "\n",
        "            # MPD\n",
        "            y_df_hat_r, y_df_hat_g, _, _ = mpd(y, y_g_hat.detach())\n",
        "            loss_disc_f = discriminator_loss(y_df_hat_r, y_df_hat_g)\n",
        "\n",
        "            # MSD\n",
        "            y_ds_hat_r, y_ds_hat_g, _, _ = msd(y, y_g_hat.detach())\n",
        "            loss_disc_s = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "            #R1 Penalty 추가\n",
        "            # ✅ MPD & MSD 각각의 Discriminators 리스트를 전달\n",
        "            r1_penalty_mpd = compute_r1_penalty(mpd.discriminators, y, device, gamma=1)\n",
        "            r1_penalty_msd = compute_r1_penalty(msd.discriminators, y, device, gamma=1)\n",
        "\n",
        "            r2_penalty_mpd = compute_r2_penalty(mpd.discriminators, y_g_hat.detach(), device, gamma=1)\n",
        "            r2_penalty_msd = compute_r2_penalty(msd.discriminators, y_g_hat.detach(), device, gamma=1)\n",
        "\n",
        "            # ✅ 최종 Discriminator Loss 계산 (R1 + R2 Penalty 포함)\n",
        "            loss_disc_all = loss_disc_s + loss_disc_f + r1_penalty_mpd + r1_penalty_msd + r2_penalty_mpd + r2_penalty_msd\n",
        "            wandb.log({\"train/loss_disc_all\" : loss_disc_all , \"train/loss_disc_s\" : loss_disc_s , \"train/loss_disc_f\" : loss_disc_f})\n",
        "\n",
        "            loss_disc_all.backward()\n",
        "            optim_d.step()\n",
        "\n",
        "            # Generator\n",
        "            optim_g.zero_grad()\n",
        "\n",
        "            # L1 Mel-Spectrogram Loss\n",
        "            loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * 45\n",
        "\n",
        "            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(y, y_g_hat)\n",
        "            y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msd(y, y_g_hat)\n",
        "            loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n",
        "            loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n",
        "            loss_gen_f = generator_loss(y_df_hat_r,y_df_hat_g)\n",
        "            loss_gen_s = generator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "            loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel   # generator loss 계산\n",
        "            wandb.log({\"train/loss_gen_all\" : loss_gen_all , \"train/loss_gen_s\" : loss_gen_s , \"train/loss_gen_f\" : loss_gen_f,\n",
        "                       \"train/loss_fm_s\" : loss_fm_s , \"train/loss_fm_f\" : loss_fm_f , \"train/loss_mel\" : loss_mel})\n",
        "\n",
        "            loss_gen_all.backward()\n",
        "            optim_g.step()\n",
        "\n",
        "            if rank == 0:\n",
        "                # STDOUT logging\n",
        "                if steps % a.stdout_interval == 0:\n",
        "                    with torch.no_grad():\n",
        "                        mel_error = F.l1_loss(y_mel, y_g_hat_mel).item()\n",
        "\n",
        "                    print('Steps : {:d}, Gen Loss Total : {:4.3f}, Mel-Spec. Error : {:4.3f}, s/b : {:4.3f}'.\n",
        "                          format(steps, loss_gen_all, mel_error, time.time() - start_b))\n",
        "\n",
        "                # checkpointing\n",
        "                if steps % a.checkpoint_interval == 0 and steps != 0:\n",
        "                    checkpoint_path = \"{}/g_{:08d}\".format(a.checkpoint_path, steps)\n",
        "                    save_checkpoint(checkpoint_path,\n",
        "                                    {'generator': (generator.module if h.num_gpus > 1 else generator).state_dict()})\n",
        "                    checkpoint_path = \"{}/do_{:08d}\".format(a.checkpoint_path, steps)\n",
        "                    save_checkpoint(checkpoint_path,\n",
        "                                    {'mpd': (mpd.module if h.num_gpus > 1\n",
        "                                                         else mpd).state_dict(),\n",
        "                                     'msd': (msd.module if h.num_gpus > 1\n",
        "                                                         else msd).state_dict(),\n",
        "                                     'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': steps,\n",
        "                                     'epoch': epoch})\n",
        "\n",
        "                # # Tensorboard summary logging\n",
        "                # if steps % a.summary_interval == 0:\n",
        "                #     sw.add_scalar(\"training/gen_loss_total\", loss_gen_all, steps)\n",
        "                #     sw.add_scalar(\"training/mel_spec_error\", mel_error, steps)\n",
        "\n",
        "                # Validation\n",
        "                if steps % a.validation_interval == 0 and steps != 0:\n",
        "                    generator.eval()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    val_err_tot = 0\n",
        "                    val_err_tot_rmse = 0\n",
        "                    val_err_tot_f0rmse = 0\n",
        "                    val_err_tot_lsd = 0\n",
        "                    val_err_tot_lsd_hf = 0\n",
        "                    val_err_tot_mos = 0\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        for j, batch in enumerate(validation_loader):\n",
        "                            x, y, _, y_mel = batch\n",
        "                            y_g_hat = generator(x.to(device))\n",
        "                            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n",
        "                            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate,\n",
        "                                                          h.hop_size, h.win_size,\n",
        "                                                          h.fmin, h.fmax_for_loss)\n",
        "                            # L1 loss 계산\n",
        "                            val_err_tot += F.l1_loss(y_mel, y_g_hat_mel).item()\n",
        "\n",
        "                            # RMSE 계산\n",
        "                            val_err_tot_rmse += torch.sqrt(F.mse_loss(y_g_hat_mel, y_mel, reduction='mean')).item()\n",
        "\n",
        "                            # F0_RMSE 계산\n",
        "                            f0_y = extract_f0_from_audio(y.squeeze(1), h.sampling_rate)\n",
        "                            f0_y_g_hat = extract_f0_from_audio(y_g_hat.squeeze(1), h.sampling_rate)\n",
        "                            val_err_tot_f0rmse += f0_rmse(f0_y_g_hat, f0_y).item()\n",
        "\n",
        "                            # LSD\n",
        "                            val_err_tot_lsd += lsd(y_g_hat_mel,  y_mel).item()\n",
        "\n",
        "                            # LSD_HF\n",
        "                            val_err_tot_lsd_hf += lsd_hf(y_g_hat_mel,  y_mel).item()\n",
        "\n",
        "                            # SCOREQ 계산\n",
        "                            # 참조 및 생성된 오디오를 파일로 저장 (SCOREQ는 파일 경로로 입력 받음)\n",
        "                            test_audio_path = f\"temp_test.wav\"\n",
        "                            torchaudio.save(test_audio_path, y_g_hat.squeeze(1).cpu(), h.sampling_rate)\n",
        "\n",
        "                            # No-reference 모드에서 품질 평가\n",
        "                            val_err_tot_mos += scoreq_nr.predict(test_path=test_audio_path).item()\n",
        "\n",
        "                            if j <= 4:\n",
        "                                if steps != 0:\n",
        "                                    wandb.log({\n",
        "                                        \"Predicted Audio\": wandb.Audio(\n",
        "                                        y_g_hat.squeeze().cpu().numpy(),\n",
        "                                        sample_rate=h.sampling_rate,\n",
        "                                        caption=\"Predicted Audio\"\n",
        "                                    ),\n",
        "                                    \"Ground Truth Audio\": wandb.Audio(\n",
        "                                        y[0].squeeze().cpu().numpy(),\n",
        "                                        sample_rate=h.sampling_rate,\n",
        "                                        caption=\"Ground Truth Audio\"\n",
        "                                    )})\n",
        "\n",
        "                                # sw.add_audio('generated/y_hat_{}'.format(j), y_g_hat[0], steps, h.sampling_rate)\n",
        "                                y_hat_spec = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels,\n",
        "                                                             h.sampling_rate, h.hop_size, h.win_size,\n",
        "                                                             h.fmin, h.fmax)\n",
        "                                wandb.log({'generated/y_hat_spec_{}'.format(j):\n",
        "                                              plot_spectrogram(y_hat_spec.squeeze(0).cpu().numpy())})\n",
        "\n",
        "                        val_err_l1 = val_err_tot / (j+1)\n",
        "                        val_err_rmse = val_err_tot_rmse / (j+1)\n",
        "                        val_err_f0rmse = val_err_tot_f0rmse / (j+1)\n",
        "                        val_err_lsd = val_err_tot_lsd / (j+1)\n",
        "                        val_err_lsd_hf = val_err_tot_lsd_hf / (j+1)\n",
        "                        val_err_mos = val_err_tot_mos / (j+1)\n",
        "\n",
        "                        wandb.log({\"validation/val_err_l1\": val_err_l1, \"validation/val_err_rmse\": val_err_rmse, \"validation/val_err_f0rmse\": val_err_f0rmse,\n",
        "                                   \"validation/val_err_lsd\": val_err_lsd, \"validation/val_err_lsd_hf\": val_err_lsd_hf, \"validation/val_err_mos\": val_err_mos})\n",
        "\n",
        "                    generator.train()\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "        scheduler_g.step()\n",
        "        scheduler_d.step()\n",
        "\n",
        "        if rank == 0:\n",
        "            print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, int(time.time() - start)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Training Process..')\n",
        "\n",
        "    a = easydict.EasyDict({\n",
        "      \"group_name\" : None,\n",
        "      \"input_wavs_dir\" : 'LJSpeech-1.1/wavs',\n",
        "      \"input_mels_dir\" : None,                              # fine-tuning 안할거면 필요 없음\n",
        "      \"input_training_file\" : 'LJSpeech-1.1/training.txt',\n",
        "      \"input_validation_file\" : 'LJSpeech-1.1/validation.txt',\n",
        "      \"checkpoint_path\" : 'cp_RPGAN_R1R2_lambda(1)_G0D3',                     # 체크포인트 폴더명 지정 (cp_Wandb name이랑 똑같이 설정 -> ex) cp_WGAN )\n",
        "      \"config\" : 'config_v3.json',\n",
        "      \"training_epochs\" : 50,\n",
        "      \"stdout_interval\" : 50,\n",
        "      \"checkpoint_interval\" : 1000,\n",
        "      \"summary_interval\" : 100,\n",
        "      \"validation_interval\" : 1000,\n",
        "      \"fine_tuning\" : False\n",
        "      })\n",
        "\n",
        "\n",
        "    with open(a.config) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "    build_env(a.config, 'config.json', a.checkpoint_path)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        h.num_gpus = torch.cuda.device_count()\n",
        "        h.batch_size = int(h.batch_size / h.num_gpus)\n",
        "        print('Batch size per GPU :', h.batch_size)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    if h.num_gpus > 1:\n",
        "        mp.spawn(train, nprocs=h.num_gpus, args=(a, h,))\n",
        "    else:\n",
        "        train(0, a, h)\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUvn1oDK-b-d"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucYJHHPWpPPv"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  # a = Args()\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGBeq_8kYop7"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl0c8fiydd7y"
      },
      "source": [
        "# hifi-inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5lGKIU0vS7p"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "        group_name = None\n",
        "        input_wavs_dir = 'test_files'\n",
        "        output_dir = 'generated_files'\n",
        "        checkpoint_file = 'cp_in_hifigan/generator_v1.pth'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS6SxYxedfWU"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "# from env import AttrDict\n",
        "# from meldataset import mel_spectrogram, MAX_WAV_VALUE, load_wav\n",
        "# from models import Generator\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(x, h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size, h.fmin, h.fmax)\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = os.listdir(a.input_wavs_dir)\n",
        "\n",
        "    os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "    with torch.no_grad():\n",
        "        for i, filname in enumerate(filelist):\n",
        "            wav, sr = load_wav(os.path.join(a.input_wavs_dir, filname))\n",
        "            wav = wav / MAX_WAV_VALUE\n",
        "            wav = torch.FloatTensor(wav).to(device)\n",
        "            x = get_mel(wav.unsqueeze(0))\n",
        "            y_g_hat = generator(x)\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            output_file = os.path.join(a.output_dir, os.path.splitext(filname)[0] + '_generated.wav')\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--input_wavs_dir', default='test_files')\n",
        "    # parser.add_argument('--output_dir', default='generated_files')\n",
        "    # parser.add_argument('--checkpoint_file', required=True)\n",
        "    # a = parser.parse_args()\n",
        "\n",
        "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    a = Args()\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g1HJtiHdf84"
      },
      "outputs": [],
      "source": [
        "#inference e2e\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from scipy.io.wavfile import write\n",
        "# from env import AttrDict\n",
        "# from meldataset import MAX_WAV_VALUE\n",
        "# from models import Generator\n",
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = os.listdir(a.input_mels_dir)\n",
        "\n",
        "    os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "    with torch.no_grad():\n",
        "        for i, filname in enumerate(filelist):\n",
        "            x = np.load(os.path.join(a.input_mels_dir, filname))\n",
        "            x = torch.FloatTensor(x).to(device)\n",
        "            y_g_hat = generator(x)\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            output_file = os.path.join(a.output_dir, os.path.splitext(filname)[0] + '_generated_e2e.wav')\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_mels_dir', default='test_mel_files')\n",
        "    parser.add_argument('--output_dir', default='generated_files_from_mel')\n",
        "    parser.add_argument('--checkpoint_file', required=True)\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hl0c8fiydd7y"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}