{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline_Perception%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%B8_Formant_WAVLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZk8X1Cx3G3_"
      },
      "source": [
        "#원본 코드\n",
        "\n",
        "https://github.com/ncsoft/avocodo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7waehhoSRy"
      },
      "source": [
        "#Avocodo 사전 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE6aC-JGFmw_",
        "outputId": "c8f5713f-af7a-4b65-8bdc-23a07daa4081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output"
      ],
      "metadata": {
        "id": "gRDgRP90jCQv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MqwXdH8ARxSb"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install pytorch_lightning\n",
        "!pip install OmegaConf\n",
        "!pip install torchinfo\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pip<24.1\"\n",
        "!pip install scoreq\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "HFLo1_Sojlqf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "outputs": [],
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "from scipy.signal.windows import kaiser\n",
        "from omegaconf import OmegaConf\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import torchaudio\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY7VggLAf8cx"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"4c7d91ca2cd073dc0f1c148b6e4bacff713df5c6\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuKV0hhkiCFp"
      },
      "source": [
        "#meldataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkgki34ooeUR"
      },
      "outputs": [],
      "source": [
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5): #튀는 부분 처리\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1): #작은 부분 키우기\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5): #토치버전 튀는 부분 처리\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1): #토치버전 작은 부분 키우기\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes): #토치 버전 스펙트로그램 정규화\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes): #토치버전 스펙트로그램 비정규화\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKv2Mxbvoiba"
      },
      "outputs": [],
      "source": [
        "mel_basis = {}\n",
        "hann_window = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdi0Xs8BohoR"
      },
      "outputs": [],
      "source": [
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.: #정규화 여부\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcaNp8vHokMx"
      },
      "outputs": [],
      "source": [
        "def get_dataset_filelist(\n",
        "    input_wavs_dir,\n",
        "    input_training_file,\n",
        "    input_validation_file\n",
        "):\n",
        "    with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcNWVMHiD3R"
      },
      "outputs": [],
      "source": [
        "#원본 meldataset\n",
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size #1개의 샘플 내에서 묶을 신호의 갯수\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split #segment_size로 자를지에 대한 여부\n",
        "        self.n_fft = n_fft #fft에서 나눌 주파수 대역의 갯수\n",
        "        self.num_mels = num_mels #멜 필터 사용 갯수\n",
        "        self.hop_size = hop_size #나눠진 각 구간에 대한\n",
        "        self.win_size = win_size #stft에서 분석할 샘플 갯수\n",
        "        self.fmin = fmin #사용할 최소 주파수\n",
        "        self.fmax = fmax #사용할 최대 주파수\n",
        "        self.fmax_loss = fmax_loss #멜 손실 최댓값\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.fine_tuning = fine_tuning #파인튜닝 여부\n",
        "        self.base_mels_path = base_mels_path# 저장된 멜 스펙트로그램 데이터경로\n",
        "\n",
        "    def __getitem__(self, index): #오디오 파일들 가져옴\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename) #[num_samples]\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning: #파인 튜닝 여부에 따라 정규화, 첫 학습인 경우 npy\n",
        "                audio = normalize(audio) * 0.95 #[num_samples]\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate: #샘플레이트 맞추기\n",
        "                raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio)#텐서화\n",
        "        audio = audio.unsqueeze(0)#[1, num_samples]\n",
        "\n",
        "        if not self.fine_tuning:\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size: #샘플 길이가 세그먼트 갯수보다 긴 경우\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                    #랜덤 시작위치에서 세그먼트 길이까지 자름,[1,segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    #짧으면 오디오에 제로 패딩 추가\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "            #num_mels랑 hop_size, win_size로 멜로 만듦, [num_mels, num_frames]\n",
        "        else:\n",
        "\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "                # [num_mels, num_frames]\n",
        "            if len(mel.shape) < 3: #3개 값 안가지면, 차원 증강\n",
        "                mel = mel.unsqueeze(0)\n",
        "                # [1, num_mels,num_frames]\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size) #프레임 갯수 계산\n",
        "                # [1, num_mels, num_frames]\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg] #[ 1,num_mels,frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                    # [1,segment_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant') #[1,num_mels,num_frames]\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    # [1,num_mels, frames_per_seg]\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False) #Ground-truth와 Generated 의 차이\n",
        "        # [1, segment_size] -> [num_mels, frames_per_seg]\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owc_vcWhiAC"
      },
      "source": [
        "#utils.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxozRiQYhj-y"
      },
      "outputs": [],
      "source": [
        "def get_padding(kernel_size, dilation=1): #제로 패딩 추가\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01): #가중치 초기화\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WGAN"
      ],
      "metadata": {
        "id": "WxZ-P1pZhbKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator\n"
      ],
      "metadata": {
        "id": "kfdIJLwGhdfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wgan_discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = -torch.mean(dr)\n",
        "        g_loss = torch.mean(dg)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses"
      ],
      "metadata": {
        "id": "2uVDhjbMDk2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "mVz8O6SuDxUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wgan_generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = -torch.mean(dg)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ],
      "metadata": {
        "id": "BIfEPe9ADy-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98DxXGMdhmT5"
      },
      "source": [
        "#losses.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VpuHS2KhoVJ"
      },
      "outputs": [],
      "source": [
        "def feature_loss(fmap_r, fmap_g): #생성된 특성 맵, 실제 특성 맵 비교 #generator의 생성에 도움\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    for dr, dg in zip(fmap_r, fmap_g): #각각에 대해 절댓값으로 비교\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            _loss = torch.mean(torch.abs(rl - gl))\n",
        "            loss += _loss\n",
        "        losses.append(_loss)\n",
        "\n",
        "    return loss*2, losses\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs): #실제값, 생성값 로스값 비교\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs): #생성기 로스값\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-6kqMPP1G7a"
      },
      "source": [
        "##formant based loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gbPDoS41Imn"
      },
      "outputs": [],
      "source": [
        "def extract_formants_torch(signal, sr=22050, n_fft=2048, hop_length=512, num_formants=5):\n",
        "    if signal.dim() == 1:\n",
        "        signal = signal.unsqueeze(0)\n",
        "\n",
        "    spec = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
        "    mag = torch.abs(spec)\n",
        "\n",
        "    # Kernel size and padding validation\n",
        "    kernel_size = 3\n",
        "    if kernel_size > mag.shape[2]:\n",
        "        kernel_size = mag.shape[2]\n",
        "    padding = kernel_size // 2\n",
        "\n",
        "    # Prevent padding issues\n",
        "    if mag.shape[2] < kernel_size:\n",
        "        raise ValueError(\"Input signal is too short for the given kernel size.\")\n",
        "\n",
        "    smoothed = F.avg_pool1d(\n",
        "        mag.transpose(1, 2),\n",
        "        kernel_size=kernel_size,\n",
        "        padding=padding,\n",
        "        stride=1\n",
        "    ).transpose(1, 2)\n",
        "\n",
        "    # Ensure sufficient frequency bins\n",
        "    if mag.shape[1] < 3:\n",
        "        raise ValueError(\"Frequency dimension too small for peak detection.\")\n",
        "\n",
        "    peaks = (mag[:, 1:-1] > mag[:, :-2]) & (mag[:, 1:-1] > mag[:, 2:])\n",
        "    peaks = F.pad(peaks, (1, 1), value=False)\n",
        "\n",
        "    freqs = torch.linspace(0, sr / 2, mag.shape[1], device=signal.device)\n",
        "    formant_freqs = []\n",
        "\n",
        "    for b in range(mag.shape[0]):\n",
        "        frame_formants = []\n",
        "        for t in range(mag.shape[2]):\n",
        "            peak_freqs = freqs[peaks[b, :, t]]\n",
        "            peak_mags = mag[b, peaks[b, :, t], t]\n",
        "\n",
        "            if len(peak_freqs) >= num_formants:\n",
        "                _, top_k_idx = torch.topk(peak_mags, num_formants)\n",
        "                frame_formants.append(peak_freqs[top_k_idx])\n",
        "            else:\n",
        "                padding = torch.zeros(num_formants - len(peak_freqs), device=signal.device)\n",
        "                frame_formants.append(torch.cat([peak_freqs, padding]))\n",
        "\n",
        "        formant_freqs.append(torch.stack(frame_formants))\n",
        "\n",
        "    formants = torch.stack(formant_freqs).transpose(1, 2)\n",
        "    return formants.squeeze(0) if signal.dim() == 1 else formants\n",
        "\n",
        "\n",
        "\n",
        "def formant_loss(original_signal, reconstructed_signal, sr=22050, n_fft=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    미분 가능한 formant 기반 loss 함수\n",
        "\n",
        "    Args:\n",
        "        original_signal: 원본 오디오 (B, T)\n",
        "        reconstructed_signal: 생성된 오디오 (B, T)\n",
        "    \"\"\"\n",
        "    # Calculate padding based on signal length and n_fft\n",
        "    padding = (n_fft - original_signal.shape[-1] % n_fft) % n_fft\n",
        "\n",
        "    # Pad the signal to a multiple of n_fft\n",
        "    original_signal = F.pad(original_signal, (0, padding))\n",
        "    reconstructed_signal = F.pad(reconstructed_signal, (0, padding))\n",
        "\n",
        "    # STFT 계산\n",
        "    orig_spec = torch.stft(original_signal,\n",
        "                          n_fft=n_fft,\n",
        "                          hop_length=hop_length,\n",
        "                          return_complex=True)\n",
        "    recon_spec = torch.stft(reconstructed_signal,\n",
        "                           n_fft=n_fft,\n",
        "                           hop_length=hop_length,\n",
        "                           return_complex=True)\n",
        "\n",
        "    # 스펙트럼 진폭\n",
        "    orig_mag = torch.abs(orig_spec)\n",
        "    recon_mag = torch.abs(recon_spec)\n",
        "\n",
        "    # Ensure both STFT outputs have the same time dimension\n",
        "    min_time_dim = min(orig_mag.shape[2], recon_mag.shape[2])\n",
        "    orig_mag = orig_mag[:, :, :min_time_dim]\n",
        "    recon_mag = recon_mag[:, :, :min_time_dim]\n",
        "\n",
        "\n",
        "    # Formant 영역에 가중치를 둔 loss (예: 0-5kHz 영역)\n",
        "    freq_weights = torch.linspace(1.0, 0.5, orig_mag.shape[1], device=orig_mag.device)\n",
        "    weighted_diff = (orig_mag - recon_mag) * freq_weights.view(1, -1, 1)\n",
        "\n",
        "    loss = torch.mean(weighted_diff ** 2)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wavlm loss\n"
      ],
      "metadata": {
        "id": "ZBWGDS_TwA1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransposeLast(nn.Module):\n",
        "    def __init__(self, deconstruct_idx=None):\n",
        "        super().__init__()\n",
        "        self.deconstruct_idx = deconstruct_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deconstruct_idx is not None:\n",
        "            x = x[self.deconstruct_idx]\n",
        "        return x.transpose(-2, -1)#마지막 차원 두개 바꿈\n",
        "\n",
        "\n",
        "class Fp32LayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.layer_norm(\n",
        "            input.float(),\n",
        "            self.normalized_shape,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None #동일 형태\n",
        "\n",
        "\n",
        "class SamePad(nn.Module): #패딩 추가 또는 빼기\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x): # [batch_size, channel, time]\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x #[batch_size, channel, time-padding]\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x) # 동일 차원\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module): #레이어 정규화\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x #동일 차원\n",
        "\n",
        "\n",
        "def gelu_accurate(x): #계산식\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor: #계산식\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size): #양자화 노이즈 추가\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training: #학습하는 경우\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else: #체크하는 경우\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim, #임베딩 차원\n",
        "            num_heads, #MULTIHEAD ATTENTION 헤드 수\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise( #노이즈 양자화\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): #파라미터 리셋\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional: #blstm\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length): #바이에스 계산\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None] #query_length-1까지\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :] #key_length-1까지\n",
        "        relative_position = memory_position - context_position #길이 차이 만큼의 상대적 위치 계산 5-3=2 식으로\n",
        "        relative_position_bucket = self._relative_positions_bucket( #범주화\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device) #상대적 위치 편향값\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1]) # [num_heads, query_length, key_length]\n",
        "        return values\n",
        "\n",
        "    def forward( #[time, batch, channel]\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights: #가중치 필요한 경우 확인\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size() #입력 데이터 분리\n",
        "        src_len = tgt_len #key 길이 설정\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None: #상대적 위치 편향 계산\n",
        "            position_bias = self.compute_bias(tgt_len, src_len) #원본과 타겟의 상대적 위치차이 계산\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "            #batch*heads, target_length, source_length\n",
        "\n",
        "        if (\n",
        "                not is_tpu  # don't use PyTorch version on TPUs\n",
        "                and incremental_state is None\n",
        "                and not static_kv\n",
        "                # A workaround for quantization to work. Otherwise JIT compilation\n",
        "                # treats bias in linear module as method.\n",
        "                and not torch.jit.is_scripting()\n",
        "                and self.q_head_dim == self.head_dim\n",
        "        ):\n",
        "            assert key is not None and value is not None\n",
        "            assert attn_mask is None\n",
        "\n",
        "            attn_mask_rel_pos = None\n",
        "            if position_bias is not None:\n",
        "                attn_mask_rel_pos = position_bias\n",
        "                if self.gru_rel_pos:\n",
        "                    query_layer = query.transpose(0, 1)\n",
        "                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n",
        "                    query_layer = query_layer.view(*new_x_shape)\n",
        "                    query_layer = query_layer.permute(0, 2, 1, 3)\n",
        "                    _B, _H, _L, __ = query_layer.size()\n",
        "\n",
        "                    gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                        _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n",
        "            k_proj_bias = self.k_proj.bias\n",
        "            if k_proj_bias is None:\n",
        "                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n",
        "\n",
        "            x, attn = F.multi_head_attention_forward( #멀티헤드 어텐션 계산\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                torch.empty([0]),\n",
        "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout_module.p,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                self.training,\n",
        "                # self.training or self.dropout_module.apply_during_inference,\n",
        "                key_padding_mask,\n",
        "                need_weights,\n",
        "                attn_mask_rel_pos,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj.weight,\n",
        "                k_proj_weight=self.k_proj.weight,\n",
        "                v_proj_weight=self.v_proj.weight,\n",
        "            )\n",
        "            return x, attn, position_bias\n",
        "\n",
        "        if incremental_state is not None: #증분상태 결합\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention: #프로젝션 계산, q,k,v 생성\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None: #저장 상태\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0: #패딩 추가의 경우\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "            position_bias = position_bias.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + position_bias\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights"
      ],
      "metadata": {
        "id": "HKnfEv1p9YFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import logging\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm\n",
        "# from modules import (\n",
        "#     Fp32GroupNorm,\n",
        "#     Fp32LayerNorm,\n",
        "#     GradMultiply,\n",
        "#     MultiheadAttention,\n",
        "#     SamePad,\n",
        "#     init_bert_params,\n",
        "#     get_activation_fn,\n",
        "#     TransposeLast,\n",
        "#     GLU_Linear,\n",
        "# )\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def compute_mask_indices(\n",
        "    shape: Tuple[int, int],\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    mask_prob: float,\n",
        "    mask_length: int,\n",
        "    mask_type: str = \"static\",\n",
        "    mask_other: float = 0.0,\n",
        "    min_masks: int = 0,\n",
        "    no_overlap: bool = False,\n",
        "    min_space: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes random mask spans for a given shape\n",
        "\n",
        "    Args:\n",
        "        shape: the the shape for which to compute masks.\n",
        "            should be of size 2 where first element is batch size and 2nd is timesteps\n",
        "        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements\n",
        "        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n",
        "            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n",
        "            however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n",
        "        mask_type: how to compute mask lengths\n",
        "            static = fixed size\n",
        "            uniform = sample from uniform distribution [mask_other, mask_length*2]\n",
        "            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element\n",
        "            poisson = sample from possion distribution with lambda = mask length\n",
        "        min_masks: minimum number of masked spans\n",
        "        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping\n",
        "        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans\n",
        "    \"\"\"\n",
        "\n",
        "    bsz, all_sz = shape\n",
        "    mask = np.full((bsz, all_sz), False)\n",
        "\n",
        "    all_num_mask = int(\n",
        "        # add a random number for probabilistic rounding\n",
        "        mask_prob * all_sz / float(mask_length)\n",
        "        + np.random.rand()\n",
        "    )\n",
        "\n",
        "    all_num_mask = max(min_masks, all_num_mask)\n",
        "\n",
        "    mask_idcs = []\n",
        "    for i in range(bsz):\n",
        "        if padding_mask is not None:\n",
        "            sz = all_sz - padding_mask[i].long().sum().item()\n",
        "            num_mask = int(\n",
        "                # add a random number for probabilistic rounding\n",
        "                mask_prob * sz / float(mask_length)\n",
        "                + np.random.rand()\n",
        "            )\n",
        "            num_mask = max(min_masks, num_mask)\n",
        "        else:\n",
        "            sz = all_sz\n",
        "            num_mask = all_num_mask\n",
        "\n",
        "        if mask_type == \"static\":\n",
        "            lengths = np.full(num_mask, mask_length)\n",
        "        elif mask_type == \"uniform\":\n",
        "            lengths = np.random.randint(mask_other, mask_length * 2 + 1, size=num_mask)\n",
        "        elif mask_type == \"normal\":\n",
        "            lengths = np.random.normal(mask_length, mask_other, size=num_mask)\n",
        "            lengths = [max(1, int(round(x))) for x in lengths]\n",
        "        elif mask_type == \"poisson\":\n",
        "            lengths = np.random.poisson(mask_length, size=num_mask)\n",
        "            lengths = [int(round(x)) for x in lengths]\n",
        "        else:\n",
        "            raise Exception(\"unknown mask selection \" + mask_type)\n",
        "\n",
        "        if sum(lengths) == 0:\n",
        "            lengths[0] = min(mask_length, sz - 1)\n",
        "\n",
        "        if no_overlap:\n",
        "            mask_idc = []\n",
        "\n",
        "            def arrange(s, e, length, keep_length):\n",
        "                span_start = np.random.randint(s, e - length)\n",
        "                mask_idc.extend(span_start + i for i in range(length))\n",
        "\n",
        "                new_parts = []\n",
        "                if span_start - s - min_space >= keep_length:\n",
        "                    new_parts.append((s, span_start - min_space + 1))\n",
        "                if e - span_start - keep_length - min_space > keep_length:\n",
        "                    new_parts.append((span_start + length + min_space, e))\n",
        "                return new_parts\n",
        "\n",
        "            parts = [(0, sz)]\n",
        "            min_length = min(lengths)\n",
        "            for length in sorted(lengths, reverse=True):\n",
        "                lens = np.fromiter(\n",
        "                    (e - s if e - s >= length + min_space else 0 for s, e in parts),\n",
        "                    np.int,\n",
        "                )\n",
        "                l_sum = np.sum(lens)\n",
        "                if l_sum == 0:\n",
        "                    break\n",
        "                probs = lens / np.sum(lens)\n",
        "                c = np.random.choice(len(parts), p=probs)\n",
        "                s, e = parts.pop(c)\n",
        "                parts.extend(arrange(s, e, length, min_length))\n",
        "            mask_idc = np.asarray(mask_idc)\n",
        "        else:\n",
        "            min_len = min(lengths)\n",
        "            if sz - min_len <= num_mask:\n",
        "                min_len = sz - num_mask - 1\n",
        "\n",
        "            mask_idc = np.random.choice(sz - min_len, num_mask, replace=False)\n",
        "\n",
        "            mask_idc = np.asarray(\n",
        "                [\n",
        "                    mask_idc[j] + offset\n",
        "                    for j in range(len(mask_idc))\n",
        "                    for offset in range(lengths[j])\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        mask_idcs.append(np.unique(mask_idc[mask_idc < sz]))\n",
        "\n",
        "    min_len = min([len(m) for m in mask_idcs])\n",
        "    for i, mask_idc in enumerate(mask_idcs):\n",
        "        if len(mask_idc) > min_len:\n",
        "            mask_idc = np.random.choice(mask_idc, min_len, replace=False)\n",
        "        mask[i, mask_idc] = True\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "class WavLMConfig:\n",
        "    def __init__(self, cfg=None):\n",
        "        self.extractor_mode: str = \"default\"     # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n",
        "        self.encoder_layers: int = 12     # num encoder layers in the transformer\n",
        "\n",
        "        self.encoder_embed_dim: int = 768     # encoder embedding dimension\n",
        "        self.encoder_ffn_embed_dim: int = 3072     # encoder embedding dimension for FFN\n",
        "        self.encoder_attention_heads: int = 12     # num encoder attention heads\n",
        "        self.activation_fn: str = \"gelu\"     # activation function to use\n",
        "\n",
        "        self.layer_norm_first: bool = False     # apply layernorm first in the transformer\n",
        "        self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"     # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]\n",
        "        self.conv_bias: bool = False     # include bias in conv encoder\n",
        "        self.feature_grad_mult: float = 1.0     # multiply feature extractor var grads by this\n",
        "\n",
        "        self.normalize: bool = False  # normalize input to have 0 mean and unit variance during training\n",
        "\n",
        "        # dropouts\n",
        "        self.dropout: float = 0.1     # dropout probability for the transformer\n",
        "        self.attention_dropout: float = 0.1     # dropout probability for attention weights\n",
        "        self.activation_dropout: float = 0.0     # dropout probability after activation in FFN\n",
        "        self.encoder_layerdrop: float = 0.0     # probability of dropping a tarnsformer layer\n",
        "        self.dropout_input: float = 0.0     # dropout to apply to the input (after feat extr)\n",
        "        self.dropout_features: float = 0.0     # dropout to apply to the features (after feat extr)\n",
        "\n",
        "        # masking\n",
        "        self.mask_length: int = 10     # mask length\n",
        "        self.mask_prob: float = 0.65     # probability of replacing a token with mask\n",
        "        self.mask_selection: str = \"static\"     # how to choose mask length\n",
        "        self.mask_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indicesh\n",
        "        self.no_mask_overlap: bool = False     # whether to allow masks to overlap\n",
        "        self.mask_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # channel masking\n",
        "        self.mask_channel_length: int = 10     # length of the mask for features (channels)\n",
        "        self.mask_channel_prob: float = 0.0     # probability of replacing a feature with 0\n",
        "        self.mask_channel_selection: str = \"static\"     # how to choose mask length for channel masking\n",
        "        self.mask_channel_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indices\n",
        "        self.no_mask_channel_overlap: bool = False     # whether to allow channel masks to overlap\n",
        "        self.mask_channel_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # positional embeddings\n",
        "        self.conv_pos: int = 128     # number of filters for convolutional positional embeddings\n",
        "        self.conv_pos_groups: int = 16     # number of groups for convolutional positional embedding\n",
        "\n",
        "        # relative position embedding\n",
        "        self.relative_position_embedding: bool = False     # apply relative position embedding\n",
        "        self.num_buckets: int = 320     # number of buckets for relative position embedding\n",
        "        self.max_distance: int = 1280     # maximum distance for relative position embedding\n",
        "        self.gru_rel_pos: bool = False     # apply gated relative position embedding\n",
        "\n",
        "        if cfg is not None:\n",
        "            self.update(cfg)\n",
        "\n",
        "    def update(self, cfg: dict):\n",
        "        self.__dict__.update(cfg)\n",
        "\n",
        "\n",
        "class WavLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg: WavLMConfig,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        logger.info(f\"WavLM Config: {cfg.__dict__}\")\n",
        "\n",
        "        self.cfg = cfg\n",
        "        feature_enc_layers = eval(cfg.conv_feature_layers)\n",
        "        self.embed = feature_enc_layers[-1][0]\n",
        "\n",
        "        self.feature_extractor = ConvFeatureExtractionModel(\n",
        "            conv_layers=feature_enc_layers,\n",
        "            dropout=0.0,\n",
        "            mode=cfg.extractor_mode,\n",
        "            conv_bias=cfg.conv_bias,\n",
        "        )\n",
        "\n",
        "        self.post_extract_proj = (\n",
        "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
        "            if self.embed != cfg.encoder_embed_dim\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.mask_prob = cfg.mask_prob\n",
        "        self.mask_selection = cfg.mask_selection\n",
        "        self.mask_other = cfg.mask_other\n",
        "        self.mask_length = cfg.mask_length\n",
        "        self.no_mask_overlap = cfg.no_mask_overlap\n",
        "        self.mask_min_space = cfg.mask_min_space\n",
        "\n",
        "        self.mask_channel_prob = cfg.mask_channel_prob\n",
        "        self.mask_channel_selection = cfg.mask_channel_selection\n",
        "        self.mask_channel_other = cfg.mask_channel_other\n",
        "        self.mask_channel_length = cfg.mask_channel_length\n",
        "        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
        "        self.mask_channel_min_space = cfg.mask_channel_min_space\n",
        "\n",
        "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
        "        self.dropout_features = nn.Dropout(cfg.dropout_features)\n",
        "\n",
        "        self.feature_grad_mult = cfg.feature_grad_mult\n",
        "\n",
        "        self.mask_emb = nn.Parameter(\n",
        "            torch.FloatTensor(cfg.encoder_embed_dim).uniform_()\n",
        "        )\n",
        "\n",
        "        self.encoder = TransformerEncoder(cfg)\n",
        "        self.layer_norm = LayerNorm(self.embed)\n",
        "\n",
        "    def apply_mask(self, x, padding_mask):\n",
        "        B, T, C = x.shape\n",
        "        if self.mask_prob > 0:\n",
        "            mask_indices = compute_mask_indices(\n",
        "                (B, T),\n",
        "                padding_mask,\n",
        "                self.mask_prob,\n",
        "                self.mask_length,\n",
        "                self.mask_selection,\n",
        "                self.mask_other,\n",
        "                min_masks=2,\n",
        "                no_overlap=self.no_mask_overlap,\n",
        "                min_space=self.mask_min_space,\n",
        "            )\n",
        "            mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
        "            x[mask_indices] = self.mask_emb\n",
        "        else:\n",
        "            mask_indices = None\n",
        "\n",
        "        if self.mask_channel_prob > 0:\n",
        "            mask_channel_indices = compute_mask_indices(\n",
        "                (B, C),\n",
        "                None,\n",
        "                self.mask_channel_prob,\n",
        "                self.mask_channel_length,\n",
        "                self.mask_channel_selection,\n",
        "                self.mask_channel_other,\n",
        "                no_overlap=self.no_mask_channel_overlap,\n",
        "                min_space=self.mask_channel_min_space,\n",
        "            )\n",
        "            mask_channel_indices = (\n",
        "                torch.from_numpy(mask_channel_indices)\n",
        "                .to(x.device)\n",
        "                .unsqueeze(1)\n",
        "                .expand(-1, T, -1)\n",
        "            )\n",
        "            x[mask_channel_indices] = 0\n",
        "\n",
        "        return x, mask_indices\n",
        "\n",
        "    def forward_padding_mask(\n",
        "            self, features: torch.Tensor, padding_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        extra = padding_mask.size(1) % features.size(1)\n",
        "        if extra > 0:\n",
        "            padding_mask = padding_mask[:, :-extra]\n",
        "        padding_mask = padding_mask.view(\n",
        "            padding_mask.size(0), features.size(1), -1\n",
        "        )\n",
        "        padding_mask = padding_mask.all(-1)\n",
        "        return padding_mask\n",
        "\n",
        "    def extract_features(\n",
        "        self,\n",
        "        source: torch.Tensor,\n",
        "        padding_mask: Optional[torch.Tensor] = None,\n",
        "        mask: bool = False,\n",
        "        ret_conv: bool = False,\n",
        "        output_layer: Optional[int] = None,\n",
        "        ret_layer_results: bool = False,\n",
        "    ):\n",
        "\n",
        "        if self.feature_grad_mult > 0:\n",
        "            features = self.feature_extractor(source)\n",
        "            if self.feature_grad_mult != 1.0:\n",
        "                features = GradMultiply.apply(features, self.feature_grad_mult)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                features = self.feature_extractor(source)\n",
        "\n",
        "        features = features.transpose(1, 2)\n",
        "        features = self.layer_norm(features)\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = self.forward_padding_mask(features, padding_mask)\n",
        "\n",
        "        if self.post_extract_proj is not None:\n",
        "            features = self.post_extract_proj(features)\n",
        "\n",
        "        features = self.dropout_input(features)\n",
        "\n",
        "        if mask:\n",
        "            x, mask_indices = self.apply_mask(\n",
        "                features, padding_mask\n",
        "            )\n",
        "        else:\n",
        "            x = features\n",
        "\n",
        "        # feature: (B, T, D), float\n",
        "        # target: (B, T), long\n",
        "        # x: (B, T, D), float\n",
        "        # padding_mask: (B, T), bool\n",
        "        # mask_indices: (B, T), bool\n",
        "        x, layer_results = self.encoder(\n",
        "            x,\n",
        "            padding_mask=padding_mask,\n",
        "            layer=None if output_layer is None else output_layer - 1\n",
        "        )\n",
        "\n",
        "        res = {\"x\": x, \"padding_mask\": padding_mask, \"features\": features, \"layer_results\": layer_results}\n",
        "\n",
        "        feature = res[\"features\"] if ret_conv else res[\"x\"]\n",
        "        if ret_layer_results:\n",
        "            feature = (feature, res[\"layer_results\"])\n",
        "        return feature, res[\"padding_mask\"]\n",
        "\n",
        "\n",
        "class ConvFeatureExtractionModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            conv_layers: List[Tuple[int, int, int]],\n",
        "            dropout: float = 0.0,\n",
        "            mode: str = \"default\",\n",
        "            conv_bias: bool = False,\n",
        "            conv_type: str = \"default\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert mode in {\"default\", \"layer_norm\"}\n",
        "\n",
        "        def block(\n",
        "                n_in,\n",
        "                n_out,\n",
        "                k,\n",
        "                stride,\n",
        "                is_layer_norm=False,\n",
        "                is_group_norm=False,\n",
        "                conv_bias=False,\n",
        "        ):\n",
        "            def make_conv():\n",
        "                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)\n",
        "                nn.init.kaiming_normal_(conv.weight)\n",
        "                return conv\n",
        "\n",
        "            assert (\n",
        "                           is_layer_norm and is_group_norm\n",
        "                   ) == False, \"layer norm and group norm are exclusive\"\n",
        "\n",
        "            if is_layer_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Sequential(\n",
        "                        TransposeLast(),\n",
        "                        Fp32LayerNorm(dim, elementwise_affine=True),\n",
        "                        TransposeLast(),\n",
        "                    ),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            elif is_group_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    Fp32GroupNorm(dim, dim, affine=True),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            else:\n",
        "                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n",
        "\n",
        "        self.conv_type = conv_type\n",
        "        if self.conv_type == \"default\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    block(\n",
        "                        in_d,\n",
        "                        dim,\n",
        "                        k,\n",
        "                        stride,\n",
        "                        is_layer_norm=mode == \"layer_norm\",\n",
        "                        is_group_norm=mode == \"default\" and i == 0,\n",
        "                        conv_bias=conv_bias,\n",
        "                    )\n",
        "                )\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"conv2d\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride)\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"custom\":\n",
        "            in_d = 1\n",
        "            idim = 80\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride, padding=1)\n",
        "                )\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.LayerNorm([dim, idim])\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "                if (i + 1) % 2 == 0:\n",
        "                    self.conv_layers.append(\n",
        "                        torch.nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "                    )\n",
        "                    idim = int(math.ceil(idim / 2))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # BxT -> BxCxT\n",
        "        x = x.unsqueeze(1)\n",
        "        if self.conv_type == \"custom\":\n",
        "            for conv in self.conv_layers:\n",
        "                if isinstance(conv, nn.LayerNorm):\n",
        "                    x = x.transpose(1, 2)\n",
        "                    x = conv(x).transpose(1, 2)\n",
        "                else:\n",
        "                    x = conv(x)\n",
        "            x = x.transpose(2, 3).contiguous()\n",
        "            x = x.view(x.size(0), -1, x.size(-1))\n",
        "        else:\n",
        "            for conv in self.conv_layers:\n",
        "                x = conv(x)\n",
        "            if self.conv_type == \"conv2d\":\n",
        "                b, c, t, f = x.size()\n",
        "                x = x.transpose(2, 3).contiguous().view(b, c * f, t)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.embedding_dim = args.encoder_embed_dim\n",
        "\n",
        "        self.pos_conv = nn.Conv1d(\n",
        "            self.embedding_dim,\n",
        "            self.embedding_dim,\n",
        "            kernel_size=args.conv_pos,\n",
        "            padding=args.conv_pos // 2,\n",
        "            groups=args.conv_pos_groups,\n",
        "        )\n",
        "        dropout = 0\n",
        "        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n",
        "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
        "        nn.init.constant_(self.pos_conv.bias, 0)\n",
        "\n",
        "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
        "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n",
        "\n",
        "        if hasattr(args, \"relative_position_embedding\"):\n",
        "            self.relative_position_embedding = args.relative_position_embedding\n",
        "            self.num_buckets = args.num_buckets\n",
        "            self.max_distance = args.max_distance\n",
        "        else:\n",
        "            self.relative_position_embedding = False\n",
        "            self.num_buckets = 0\n",
        "            self.max_distance = 0\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerSentenceEncoderLayer(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                    has_relative_attention_bias=(self.relative_position_embedding and i == 0),\n",
        "                    num_buckets=self.num_buckets,\n",
        "                    max_distance=self.max_distance,\n",
        "                    gru_rel_pos=args.gru_rel_pos,\n",
        "                )\n",
        "                for i in range(args.encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layer_norm_first = args.layer_norm_first\n",
        "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
        "        self.layerdrop = args.encoder_layerdrop\n",
        "\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "    def forward(self, x, padding_mask=None, streaming_mask=None, layer=None):\n",
        "        x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)\n",
        "\n",
        "        if self.layer_norm_first and layer is None:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "    def extract_features(self, x, padding_mask=None, streaming_mask=None, tgt_layer=None):\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            x[padding_mask] = 0\n",
        "\n",
        "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "        x = x + x_conv\n",
        "\n",
        "        if not self.layer_norm_first:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        layer_results = []\n",
        "        z = None\n",
        "        if tgt_layer is not None:\n",
        "            layer_results.append((x, z))\n",
        "        r = None\n",
        "        pos_bias = None\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            dropout_probability = np.random.random()\n",
        "            if not self.training or (dropout_probability > self.layerdrop):\n",
        "                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,\n",
        "                                       self_attn_mask=streaming_mask, pos_bias=pos_bias)\n",
        "            if tgt_layer is not None:\n",
        "                layer_results.append((x, z))\n",
        "            if i == tgt_layer:\n",
        "                r = x\n",
        "                break\n",
        "\n",
        "        if r is not None:\n",
        "            x = r\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "\n",
        "class TransformerSentenceEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = \"relu\",\n",
        "            layer_norm_first: bool = False,\n",
        "            has_relative_attention_bias: bool = False,\n",
        "            num_buckets: int = 0,\n",
        "            max_distance: int = 0,\n",
        "            rescale_init: bool = False,\n",
        "            gru_rel_pos: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        # Initialize parameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        # Initialize blocks\n",
        "        self.activation_name = activation_fn\n",
        "        self.activation_fn = get_activation_fn(activation_fn)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=True,\n",
        "            has_relative_attention_bias=has_relative_attention_bias,\n",
        "            num_buckets=num_buckets,\n",
        "            max_distance=max_distance,\n",
        "            rescale_init=rescale_init,\n",
        "            gru_rel_pos=gru_rel_pos,\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm_first = layer_norm_first\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "        if self.activation_name == \"glu\":\n",
        "            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            self_attn_mask: torch.Tensor = None,\n",
        "            self_attn_padding_mask: torch.Tensor = None,\n",
        "            need_weights: bool = False,\n",
        "            pos_bias=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        LayerNorm is applied either before or after the self-attention/ffn\n",
        "        modules similar to the original Transformer imlementation.\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        if self.layer_norm_first:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=False,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            residual = x\n",
        "            x = self.final_layer_norm(x)\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "        else:\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "            residual = x\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        return x, attn, pos_bias\n"
      ],
      "metadata": {
        "id": "CCVI1lw99aD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcMzi-hiajZ"
      },
      "source": [
        "#pqmf.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAzKgcDTpCEh"
      },
      "outputs": [],
      "source": [
        "def design_prototype_filter(taps=62, cutoff_ratio=0.142, beta=9.0): #프로토타입 필터 예시\n",
        "    \"\"\"Design prototype filter for PQMF.\n",
        "    This method is based on `A Kaiser window approach for the design of prototype\n",
        "    filters of cosine modulated filterbanks`_.\n",
        "    Args:\n",
        "        taps (int): The number of filter taps.\n",
        "        cutoff_ratio (float): Cut-off frequency ratio.\n",
        "        beta (float): Beta coefficient for kaiser window.\n",
        "    Returns:\n",
        "        ndarray: Impluse response of prototype filter (taps + 1,).\n",
        "    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n",
        "        https://ieeexplore.ieee.org/abstract/document/681427\n",
        "    \"\"\"\n",
        "    # check the arguments are valid, 제약조건\n",
        "    assert taps % 2 == 0, \"The number of taps mush be even number.\" #소수 지정\n",
        "    assert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\" #컷오프 비율\n",
        "\n",
        "    # make initial filter\n",
        "    omega_c = np.pi * cutoff_ratio #차단 주파수 계산\n",
        "    with np.errstate(invalid=\"ignore\"): #차단 주파수 기준으로 sinc함수로 필터 설계\n",
        "        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n",
        "            np.pi * (np.arange(taps + 1) - 0.5 * taps)\n",
        "        )\n",
        "    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # 완전 중심값의 경우, 분모가 0이므로 에러 발생하므로 예외처리\n",
        "\n",
        "    # apply kaiser window\n",
        "    w = kaiser(taps + 1, beta)# 양 옆으로 있고, 중간에 1개만 있으므로 +1, 베타를 가지는 카이저 윈도우\n",
        "    h = h_i * w #\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gob2R_Voicxx"
      },
      "outputs": [],
      "source": [
        "class PQMF(torch.nn.Module):\n",
        "    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "\n",
        "        super(PQMF, self).__init__()\n",
        "\n",
        "        # build analysis & synthesis filter coefficients\n",
        "        h_proto = design_prototype_filter(taps, cutoff_ratio, beta) #필터 생성\n",
        "        h_analysis = np.zeros((subbands, len(h_proto))) #서브 밴드 대역만큼 0으로 된 배열 생성\n",
        "        h_synthesis = np.zeros((subbands, len(h_proto))) #합성도 동일한 크기의 배열 생성\n",
        "        for k in range(subbands): #각 대역별로 분석, 합성을 진행\n",
        "            h_analysis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    + (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "            h_synthesis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    - (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # convert to tensor\n",
        "        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1)\n",
        "        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n",
        "\n",
        "        # register coefficients as beffer\n",
        "        self.register_buffer(\"analysis_filter\", analysis_filter)\n",
        "        self.register_buffer(\"synthesis_filter\", synthesis_filter)\n",
        "\n",
        "        # filter for downsampling & upsampling\n",
        "        updown_filter = torch.zeros((subbands, subbands, subbands)).float()\n",
        "        for k in range(subbands):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.subbands = subbands\n",
        "\n",
        "        # keep padding info\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n",
        "\n",
        "    def analysis(self, x): #다운 샘플링\n",
        "        x = F.conv1d(self.pad_fn(x), self.analysis_filter) #패딩 추가, [batch_size, subbands, t]\n",
        "        return F.conv1d(x, self.updown_filter, stride=self.subbands)#(batch_size, sub_bands,t//sub_bands)\n",
        "\n",
        "    def synthesis(self, x): #업 샘플링\n",
        "        x = F.conv_transpose1d(\n",
        "            x, self.updown_filter * self.subbands, stride=self.subbands\n",
        "        )#[batch_size, subbands, t//subbands]\n",
        "        return F.conv1d(self.pad_fn(x), self.synthesis_filter)#[batch_size, 1, t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQ3Q0mqht0K"
      },
      "source": [
        "#CoMBD.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgOcC75wouAx"
      },
      "outputs": [],
      "source": [
        "class CoMBDBlock(torch.nn.Module):\n",
        "    def __init__( #CoMBD 블록 선언\n",
        "        self,\n",
        "        h_u: List[int], #각 계층별 출력 갯수(히든 유닛) 리스트\n",
        "        d_k: List[int], #각 계층의 커널 크기 리스트\n",
        "        d_s: List[int], #각 계층의 스트라이드 리스트\n",
        "        d_d: List[int], #각 계층의 dilation(확장 계수) 리스트\n",
        "        d_g: List[int], #각 계층의 그룹 수 리스트\n",
        "        d_p: List[int], #각 계층의 패딩 크기 리스트\n",
        "        op_f: int, #프로젝션 계층의 출력 채널 크기\n",
        "        op_k: int, #프로젝션 계층의 커널 크기\n",
        "        op_g: int, #프로젝션 계층의 그룹 수\n",
        "        use_spectral_norm=False #spectral normalization 적용여부\n",
        "    ):\n",
        "        super(CoMBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters = [[1, h_u[0]]] #출력 필터 지정\n",
        "        for i in range(len(h_u) - 1): #각 계층마다 필터 지정\n",
        "            filters.append([h_u[i], h_u[i + 1]]) #각 필터 시작지점, 끝지점으로 묶기\n",
        "        for _f, _k, _s, _d, _g, _p in zip(filters, d_k, d_s, d_d, d_g, d_p):#각 대역폭별로 사용될 데이터 묶기\n",
        "            self.convs.append(norm_f(\n",
        "                Conv1d(\n",
        "                    in_channels=_f[0],\n",
        "                    out_channels=_f[1],\n",
        "                    kernel_size=_k,\n",
        "                    stride=_s,\n",
        "                    dilation=_d,\n",
        "                    groups=_g,\n",
        "                    padding=_p\n",
        "                )\n",
        "            ))\n",
        "        self.projection_conv = norm_f( #제일 마지막 프로젝션 레이어\n",
        "            Conv1d(\n",
        "                in_channels=filters[-1][1],\n",
        "                out_channels=op_f,\n",
        "                kernel_size=op_k,\n",
        "                groups=op_g\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for block in self.convs:\n",
        "            x = block(x) #블록\n",
        "            x = F.leaky_relu(x, 0.2) #활성화 함수\n",
        "            fmap.append(x) #묶어서 추가\n",
        "        x = self.projection_conv(x)\n",
        "        return x, fmap    #CoMBD 블록 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-ga7kQJhs-0"
      },
      "outputs": [],
      "source": [
        "class CoMBD(torch.nn.Module):\n",
        "    def __init__(self, h, pqmf_list=None, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        self.h = h #하이퍼 파라미터 설정 객체\n",
        "        if pqmf_list is not None: #대역 분할 있으면, 지정\n",
        "            self.pqmf = pqmf_list\n",
        "        else: #대역 분할이 없으면 LV1, LV2로 분할\n",
        "            self.pqmf = [\n",
        "                PQMF(*h.pqmf_config[\"lv2\"]),\n",
        "                PQMF(*h.pqmf_config[\"lv1\"])\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.ModuleList() #CoMBD 블럭의 데이터 가져옴\n",
        "        for _h_u, _d_k, _d_s, _d_d, _d_g, _d_p, _op_f, _op_k, _op_g in zip(\n",
        "            h.combd_h_u,\n",
        "            h.combd_d_k,\n",
        "            h.combd_d_s,\n",
        "            h.combd_d_d,\n",
        "            h.combd_d_g,\n",
        "            h.combd_d_p,\n",
        "            h.combd_op_f,\n",
        "            h.combd_op_k,\n",
        "            h.combd_op_g,\n",
        "        ):\n",
        "            self.blocks.append(CoMBDBlock( #COMBD 블럭들 쌓기\n",
        "                _h_u,\n",
        "                _d_k,\n",
        "                _d_s,\n",
        "                _d_d,\n",
        "                _d_g,\n",
        "                _d_p,\n",
        "                _op_f,\n",
        "                _op_k,\n",
        "                _op_g,\n",
        "            ))\n",
        "\n",
        "    def _block_forward(self, input, blocks, outs, f_maps): #순차적 통과\n",
        "        for x, block in zip(input, blocks):\n",
        "            out, f_map = block(x) #각 계층마다의 중간맵 출력\n",
        "            outs.append(out)\n",
        "            f_maps.append(f_map)\n",
        "        return outs, f_maps\n",
        "\n",
        "    def _pqmf_forward(self, ys, ys_hat): #PQMF를 통한 신호의 대역폭 분할\n",
        "        # ys는 실제 신호 리스트 - training_step 파트에서 만들어짐\n",
        "        # ys_hat은 생성된 신호 리스트\n",
        "        #  ys = [\n",
        "        #self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],  # PQMF Level 2\n",
        "        #self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],  # PQMF Level 1\n",
        "        #y  ] # 원래 입력 신호\n",
        "        multi_scale_inputs = []\n",
        "        multi_scale_inputs_hat = []\n",
        "        for pqmf in self.pqmf: #lv1, lv2가 들어가있는 pqmf 객체 리스트에 다운샘플링\n",
        "            multi_scale_inputs.append(  #analysis 함수는 일종의 다운샘플링\n",
        "                pqmf.to(ys[-1]).analysis(ys[-1])[:, :1, :] #batch_size, subband,t\n",
        "            )\n",
        "            multi_scale_inputs_hat.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys_hat[-1])[:, :1, :]\n",
        "            )\n",
        "\n",
        "        outs_real = []\n",
        "        f_maps_real = []\n",
        "        # real\n",
        "        # for hierarchical forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            ys, self.blocks, outs_real, f_maps_real)\n",
        "        # for multi_scale forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            multi_scale_inputs, self.blocks[:-1], outs_real, f_maps_real)\n",
        "\n",
        "        outs_fake = []\n",
        "        f_maps_fake = []\n",
        "        # predicted\n",
        "        # for hierarchical forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            ys_hat, self.blocks, outs_fake, f_maps_fake)\n",
        "        # for multi_scale forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            multi_scale_inputs_hat, self.blocks[:-1], outs_fake, f_maps_fake)\n",
        "\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake\n",
        "\n",
        "    def forward(self, ys, ys_hat):\n",
        "        outs_real, outs_fake, f_maps_real, f_maps_fake = self._pqmf_forward(\n",
        "            ys, ys_hat)\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeFVq4_hzq6"
      },
      "source": [
        "#SBD.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBMUu1GRozy6"
      },
      "outputs": [],
      "source": [
        "class MDC(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.d_convs = nn.ModuleList()\n",
        "        for _k, _d in zip(kernel_size, dilations):\n",
        "            self.d_convs.append(\n",
        "                norm_f(Conv1d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=_k,\n",
        "                    dilation=_d,\n",
        "                    padding=get_padding(_k, _d)\n",
        "                ))\n",
        "            )\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=strides,\n",
        "            padding=get_padding(_k, _d)\n",
        "        ))\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _out = None\n",
        "        for _l in self.d_convs:\n",
        "            _x = torch.unsqueeze(_l(x), -1)\n",
        "            _x = F.leaky_relu(_x, 0.2)\n",
        "            if _out is None:\n",
        "                _out = _x\n",
        "            else:\n",
        "                _out = torch.cat([_out, _x], axis=-1)\n",
        "        x = torch.sum(_out, dim=-1)\n",
        "        x = self.post_conv(x)\n",
        "        x = F.leaky_relu(x, 0.2)  # @@\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2OaM69uh0_5"
      },
      "outputs": [],
      "source": [
        "class SBDBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        segment_dim,\n",
        "        strides,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(SBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters_in_out = [(segment_dim, filters[0])]\n",
        "        for i in range(len(filters) - 1):\n",
        "            filters_in_out.append([filters[i], filters[i + 1]])\n",
        "\n",
        "        for _s, _f, _k, _d in zip(\n",
        "            strides,\n",
        "            filters_in_out,\n",
        "            kernel_size,\n",
        "            dilations\n",
        "        ):\n",
        "            self.convs.append(MDC(\n",
        "                in_channels=_f[0],\n",
        "                out_channels=_f[1],\n",
        "                strides=_s,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=_f[1],\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=3 // 2\n",
        "        ))  # @@\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for _l in self.convs:\n",
        "            x = _l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.post_conv(x)  # @@\n",
        "\n",
        "        return x, fmap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuGqG0y4o4oB"
      },
      "outputs": [],
      "source": [
        "class MDCDConfig:\n",
        "    def __init__(self, h):\n",
        "        self.pqmf_params = h.pqmf_config[\"sbd\"]\n",
        "        self.f_pqmf_params = h.pqmf_config[\"fsbd\"]\n",
        "        self.filters = h.sbd_filters\n",
        "        self.kernel_sizes = h.sbd_kernel_sizes\n",
        "        self.dilations = h.sbd_dilations\n",
        "        self.strides = h.sbd_strides\n",
        "        self.band_ranges = h.sbd_band_ranges\n",
        "        self.transpose = h.sbd_transpose\n",
        "        self.segment_size = h.segment_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H-kRPzzo3dp"
      },
      "outputs": [],
      "source": [
        "class SBD(torch.nn.Module):\n",
        "    def __init__(self, h, use_spectral_norm=False):\n",
        "        super(SBD, self).__init__()\n",
        "        self.config = MDCDConfig(h)\n",
        "        self.pqmf = PQMF(\n",
        "            *self.config.pqmf_params\n",
        "        )\n",
        "        if True in h.sbd_transpose:\n",
        "            self.f_pqmf = PQMF(\n",
        "                *self.config.f_pqmf_params\n",
        "            )\n",
        "        else:\n",
        "            self.f_pqmf = None\n",
        "\n",
        "        self.discriminators = torch.nn.ModuleList()\n",
        "\n",
        "        for _f, _k, _d, _s, _br, _tr in zip(\n",
        "            self.config.filters,\n",
        "            self.config.kernel_sizes,\n",
        "            self.config.dilations,\n",
        "            self.config.strides,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if _tr:\n",
        "                segment_dim = self.config.segment_size // _br[1] - _br[0]\n",
        "            else:\n",
        "                segment_dim = _br[1] - _br[0]\n",
        "\n",
        "            self.discriminators.append(SBDBlock(\n",
        "                segment_dim=segment_dim,\n",
        "                filters=_f,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                strides=_s,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        y_in = self.pqmf.analysis(y)\n",
        "        y_hat_in = self.pqmf.analysis(y_hat)\n",
        "        if self.f_pqmf is not None:\n",
        "            y_in_f = self.f_pqmf.analysis(y)\n",
        "            y_hat_in_f = self.f_pqmf.analysis(y_hat)\n",
        "\n",
        "        for d, br, tr in zip(\n",
        "            self.discriminators,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if tr:\n",
        "                _y_in = y_in_f[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in_f[:, br[0]:br[1], :]\n",
        "                _y_in = torch.transpose(_y_in, 1, 2)\n",
        "                _y_hat_in = torch.transpose(_y_hat_in, 1, 2)\n",
        "            else:\n",
        "                _y_in = y_in[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in[:, br[0]:br[1], :]\n",
        "            y_d_r, fmap_r = d(_y_in)\n",
        "            y_d_g, fmap_g = d(_y_hat_in)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttjqPrPh4sC"
      },
      "source": [
        "#generator.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHcgmHJEo90g"
      },
      "outputs": [],
      "source": [
        "class ResBlock(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.2)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, 0.2)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for _l in self.convs1:\n",
        "            remove_weight_norm(_l)\n",
        "        for _l in self.convs2:\n",
        "            remove_weight_norm(_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUBqwfZgh3dx"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.resblock = h.resblock\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            _ups = nn.ModuleList()\n",
        "            for _i, (_u, _k) in enumerate(zip(u, k)):\n",
        "                in_channel = h.upsample_initial_channel // (2**i)\n",
        "                out_channel = h.upsample_initial_channel // (2**(i + 1))\n",
        "                _ups.append(weight_norm(\n",
        "                    ConvTranspose1d(in_channel, out_channel, _k, _u, padding=(_k - _u) // 2)))\n",
        "            self.ups.append(_ups)\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        self.conv_post = nn.ModuleList()\n",
        "        for i in range(self.num_upsamples):\n",
        "            ch = h.upsample_initial_channel // (2**(i + 1))\n",
        "            temp = nn.ModuleList()\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                temp.append(resblock(h, ch, k, d))\n",
        "            self.resblocks.append(temp)\n",
        "\n",
        "            if self.h.projection_filters[i] != 0:\n",
        "                self.conv_post.append(\n",
        "                    weight_norm(\n",
        "                        Conv1d(\n",
        "                            ch, self.h.projection_filters[i],\n",
        "                            self.h.projection_kernels[i], 1, padding=self.h.projection_kernels[i] // 2\n",
        "                        )))\n",
        "            else:\n",
        "                self.conv_post.append(torch.nn.Identity())\n",
        "\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.conv_pre(x)\n",
        "        for i, (ups, resblocks, conv_post) in enumerate(zip(self.ups, self.resblocks, self.conv_post)):\n",
        "            x = F.leaky_relu(x, 0.2)\n",
        "            for _ups in ups:\n",
        "                x = _ups(x)\n",
        "            xs = None\n",
        "            for j, resblock in enumerate(resblocks):\n",
        "                if xs is None:\n",
        "                    xs = resblock(x)\n",
        "                else:\n",
        "                    xs += resblock(x)\n",
        "            x = xs / self.num_kernels\n",
        "            if i >= (self.num_upsamples-3):\n",
        "                _x = F.leaky_relu(x)\n",
        "                _x = conv_post(_x)\n",
        "                _x = torch.tanh(_x)\n",
        "                outs.append(_x)\n",
        "            else:\n",
        "                x = conv_post(x)\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for ups in self.ups:\n",
        "            for _l in ups:\n",
        "                remove_weight_norm(_l)\n",
        "        for resblock in self.resblocks:\n",
        "            for _l in resblock:\n",
        "                _l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        for _l in self.conv_post:\n",
        "            if not isinstance(_l, torch.nn.Identity):\n",
        "                remove_weight_norm(_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyxi8fxiHbD"
      },
      "source": [
        "#data_module.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RVIsixRpGYI"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AvocodoDataConfig:\n",
        "    segment_size: int\n",
        "    num_mels: int\n",
        "    num_freq: int\n",
        "    sampling_rate: int\n",
        "    n_fft: int\n",
        "    hop_size: int\n",
        "    win_size: int\n",
        "    fmin: int\n",
        "    fmax: int\n",
        "    batch_size: int\n",
        "    num_workers: int\n",
        "\n",
        "    fine_tuning: bool\n",
        "    base_mels_path: str\n",
        "\n",
        "    input_wavs_dir: str\n",
        "    input_mels_dir: str\n",
        "    input_training_file: str\n",
        "    input_validation_file: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3ikwJZniJKo"
      },
      "outputs": [],
      "source": [
        "class AvocodoData(LightningDataModule):\n",
        "    def __init__(self, h: AvocodoDataConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "    def prepare_data(self):\n",
        "\n",
        "        self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "            self.hparams.input_wavs_dir,\n",
        "            self.hparams.input_training_file,\n",
        "            self.hparams.input_validation_file\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.trainset = MelDataset(\n",
        "            self.training_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(\n",
        "            self.trainset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "    @rank_zero_only\n",
        "    def val_dataloader(self):\n",
        "        validset = MelDataset(\n",
        "            self.validation_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            False,\n",
        "            False,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(validset, num_workers=max_workers, shuffle=False,\n",
        "                          sampler=None,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTk_rGDiPHB"
      },
      "source": [
        "#lightning_module.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scoreq"
      ],
      "metadata": {
        "id": "tmF042BgmR4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lsd(est ,target):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim = 1) ** 0.5\n",
        "    lsd = torch.mean(squared_diff, dim = 0)\n",
        "    return lsd\n",
        "\n",
        "def lsd_hf(est, target, hf_ratio=0.25):\n",
        "    assert est.shape == target.shape, \"Spectrograms must have the same shape.\"\n",
        "    est = est.squeeze(0).squeeze(0) ** 2\n",
        "    target = target.squeeze(0).squeeze(0) ** 2\n",
        "\n",
        "    # Define high-frequency range\n",
        "    num_freq_bins = est.shape[0]\n",
        "    hf_start = int(num_freq_bins * (1 - hf_ratio))  # Starting index for high frequencies\n",
        "\n",
        "    # Focus on high-frequency bands\n",
        "    est_hf = est[hf_start:, :]\n",
        "    target_hf = target[hf_start:, :]\n",
        "\n",
        "    # Compute the log of the magnitude spectrograms (adding a small epsilon to avoid log(0))\n",
        "    epsilon = 1e-10\n",
        "    log_spectrogram1 = torch.log10(target_hf + epsilon)\n",
        "    log_spectrogram2 = torch.log10(est_hf + epsilon)\n",
        "    squared_diff = (log_spectrogram1 - log_spectrogram2) ** 2\n",
        "    squared_diff = torch.mean(squared_diff, dim=1) ** 0.5\n",
        "    lsd_hf = torch.mean(squared_diff, dim=0)\n",
        "\n",
        "    return lsd_hf\n",
        "\n",
        "def extract_f0_from_audio(audio, sr, fmin=50, fmax=500):\n",
        "    audio_np = audio.cpu().numpy()  # Convert to numpy for librosa\n",
        "    f0, voiced_flag, _ = librosa.pyin(audio_np, fmin=fmin, fmax=fmax, sr=sr)\n",
        "    f0 = torch.tensor(f0, dtype=torch.float32)  # Convert back to tensor\n",
        "    f0[~torch.tensor(voiced_flag, dtype=torch.bool)] = 0  # Set unvoiced regions to 0\n",
        "    return f0\n",
        "\n",
        "def f0_rmse(f0_pred, f0_target):\n",
        "    assert f0_pred.shape == f0_target.shape, \"F0 shapes must match.\"\n",
        "    squared_error = (f0_pred - f0_target) ** 2\n",
        "    mse = torch.mean(squared_error)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "-1Hk6oeBoo-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsaB8xZGiOeh"
      },
      "outputs": [],
      "source": [
        "from torchaudio.transforms import Resample\n",
        "\n",
        "class Avocodo(LightningModule):\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "        #WavLM 설정\n",
        "        wavlm_cfg_dict = OmegaConf.load(h.wavlm_config)\n",
        "        wavlm_cfg = WavLMConfig(wavlm_cfg_dict)\n",
        "        self.wavlm = WavLM(wavlm_cfg)\n",
        "\n",
        "        # WavLM 가중치 로드 (요 부분 좀 헷갈림)\n",
        "        checkpoint = torch.load(h.wavlm_checkpoint)\n",
        "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "        self.wavlm.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        # WavLM 가중치 동결\n",
        "        self.freeze_module(self.wavlm)\n",
        "\n",
        "        # Resample (22050Hz -> 16000Hz)\n",
        "        self.resample = Resample(orig_freq=22050, new_freq=16000)\n",
        "\n",
        "\n",
        "        if \"loss_scale_formant\" not in self.hparams:\n",
        "          self.hparams.loss_scale_formant = 1.0 # formant loss scale 도 나중에 따로 조정해줘야 할듯\n",
        "\n",
        "        # Model components\n",
        "        self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])\n",
        "        self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "        self.generator = Generator(self.hparams.generator)\n",
        "        self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])\n",
        "        self.sbd = SBD(self.hparams.sbd)\n",
        "\n",
        "        # Validation outputs storage\n",
        "        self.validation_l1_outputs = []\n",
        "        self.validation_rmse_outputs = []\n",
        "        self.validation_rmse_f0_outputs = []\n",
        "        self.validation_lsd_outputs = []\n",
        "        self.validation_lsd_hf_outputs = []\n",
        "        self.validation_scoreq_outputs = []\n",
        "\n",
        "        # SCOREQ 초기화\n",
        "        self.scoreq_nr = scoreq.Scoreq(data_domain='natural', mode='nr')  # No-reference\n",
        "\n",
        "        # Manual optimization\n",
        "        self.automatic_optimization = False\n",
        "        self.lambda_gp = 10\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        h = self.hparams.optimizer\n",
        "        opt_g = torch.optim.AdamW(\n",
        "            self.generator.parameters(),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        opt_d = torch.optim.AdamW(\n",
        "            itertools.chain(self.combd.parameters(), self.sbd.parameters()),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        return [opt_g, opt_d]\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)[-1]\n",
        "\n",
        "    def freeze_module(self, module):\n",
        "        \"\"\"\n",
        "        특정 모듈의 가중치를 동결합니다.\n",
        "        \"\"\"\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def extract_layer_features(self, wav_input, layer=6):\n",
        "        \"\"\"\n",
        "        WavLM을 사용하여 특정 레이어의 특징값 추출.\n",
        "        \"\"\"\n",
        "        # WavLM에서 특징값 추출\n",
        "        rep, layer_results = self.wavlm.extract_features(\n",
        "            source=wav_input,\n",
        "            output_layer=6,\n",
        "            ret_layer_results=True\n",
        "        )\n",
        "\n",
        "        layer_tensor = rep[0]  # 튜플의 첫 번째 요소 추출\n",
        "        if isinstance(layer_tensor, torch.Tensor):\n",
        "            layer_features = layer_tensor.transpose(0, 1)  # (batch, time, feature_dim)\n",
        "            #print(f\"Extracted layer {layer} features shape:\", layer_features.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Expected a Tensor but got: {type(layer_tensor)}\")\n",
        "        # Shape 디버깅용 출력\n",
        "        #print(f\"Extracted layer {layer} features shape: {layer_features.shape}\")\n",
        "\n",
        "        return layer_features\n",
        "\n",
        "    def compute_gradient_penalty(self, discriminator, real_samples, fake_samples):\n",
        "        \"\"\"\n",
        "        Compute gradient penalty for WGAN-GP.\n",
        "        \"\"\"\n",
        "        batch_size = real_samples.size(0)\n",
        "\n",
        "        # Random weight term for interpolation\n",
        "        alpha = torch.rand(batch_size, 1, 1, device=real_samples.device)\n",
        "\n",
        "        # Get random interpolation between real and fake samples\n",
        "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "\n",
        "        # Forward pass through discriminator\n",
        "        if discriminator == self.combd:\n",
        "            # For CoMBD, create a list of interpolated samples\n",
        "            interpolated_list = [interpolates] * 3  # Assuming 3 levels like in ys\n",
        "            d_interpolates_r, d_interpolates_g, _, _ = discriminator(interpolated_list, interpolated_list)\n",
        "            d_interpolates = d_interpolates_r  # Use real outputs for gradient computation\n",
        "        else:\n",
        "            # For SBD\n",
        "            d_interpolates_r, d_interpolates_g, _, _ = discriminator(interpolates, interpolates)\n",
        "            d_interpolates = d_interpolates_r\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients_list = []\n",
        "        for d_out in d_interpolates:\n",
        "            # Ensure the output has the correct batch dimension\n",
        "            if d_out.dim() == 2:\n",
        "                d_out = d_out.sum(dim=1)\n",
        "            elif d_out.dim() == 3:\n",
        "                d_out = d_out.sum(dim=(1,2))\n",
        "\n",
        "            # Create grad_outputs with matching batch size\n",
        "            grad_outputs = torch.ones(batch_size, device=interpolates.device)\n",
        "\n",
        "            # Calculate gradients for this output\n",
        "            grads = torch.autograd.grad(\n",
        "                outputs=d_out,\n",
        "                inputs=interpolates,\n",
        "                grad_outputs=grad_outputs,\n",
        "                create_graph=True,\n",
        "                retain_graph=True,\n",
        "                only_inputs=True\n",
        "            )[0]\n",
        "            gradients_list.append(grads)\n",
        "\n",
        "        # Combine gradients from all outputs\n",
        "        gradients = sum(gradients_list) / len(gradients_list)\n",
        "\n",
        "        # Calculate gradient penalty\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "        return gradient_penalty\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #학습 루프의 단일 스텝을 정의\n",
        "        x, y, _, y_mel = batch #x는 입력 mel, y는 ground truth wav\n",
        "        y2 = y #wavlm용 데이터 복사\n",
        "        y = y.unsqueeze(1) # Con1D 연산을 하기 위해 차원 맞추기\n",
        "        ys = [\n",
        "            self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "            self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "            y\n",
        "        ] # Discriminator의 입력으로 사용되는 multi-band signals\n",
        "\n",
        "        y_g_hats = self.generator(x) #입력 mel을 입력받아 생성된 오디오 신호\n",
        "\n",
        "        #perception loss 계산 파트\n",
        "        # wav 생성 후 조정\n",
        "        generated_wav = y_g_hats[-1] if isinstance(y_g_hats, list) else y_g_hats\n",
        "        generated_wav = generated_wav.squeeze(1)  # [batch, time]\n",
        "\n",
        "        # Normalize and resample inputs for WavLM\n",
        "        wav_input1_16khz = self.resample(y2).squeeze(1)  # 원본 wav 리샘플링\n",
        "        wav_input2_16khz = self.resample(generated_wav)  # 생성된 wav 리샘플링\n",
        "\n",
        "        # Extract features from 6th layer\n",
        "\n",
        "        original_features = self.extract_layer_features(wav_input1_16khz, layer=6)\n",
        "        generated_features = self.extract_layer_features(wav_input2_16khz, layer=6)\n",
        "\n",
        "        # Calculate L1 Loss for WavLM features\n",
        "        wavlm_feature_loss = F.l1_loss(original_features, generated_features)\n",
        "        wavlm_feature_loss = self.hparams.loss_scale_wavlm * wavlm_feature_loss\n",
        "\n",
        "        self.log(\"train/wavlm_feature_loss_step\", wavlm_feature_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "        self.log(\"train/wavlm_feature_loss_epoch\", wavlm_feature_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        #여기까지가 perception loss 계산파트\n",
        "\n",
        "\n",
        "        # Get optimizers\n",
        "        opt_g, opt_d = self.optimizers() #생성기와 판별기의 옵티마이저 할당\n",
        "\n",
        "        # Train Generator\n",
        "        opt_g.zero_grad() #Generator Gradient 초기화\n",
        "        y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "        loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)\n",
        "        loss_gen_u, _ = wgan_generator_loss(y_du_hat_g) #wgan 적용\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "        loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "        loss_gen_s, _ = wgan_generator_loss(y_ds_hat_g) #wgan 적용\n",
        "\n",
        "        # L1 Mel-Spectrogram Loss\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hats[-1].squeeze(1), #y_g_hats[-1]은 최종 전체 대역 신호를 의미\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        ) #Generator가 생성한 wav를 mel spectrogram으로 변환\n",
        "\n",
        "        loss_mel = F.l1_loss(y_mel, y_g_hat_mel) #ground truth와 generator 출력의 l1 loss\n",
        "        self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        loss_mel = loss_mel * self.hparams.loss_scale_mel #해당 loss 가중치 여기서 조절\n",
        "\n",
        "        # Formant loss 추가\n",
        "        loss_formant=formant_loss(y.squeeze(1),y_g_hats[-1].squeeze(1), self.hparams.audio.sampling_rate)\n",
        "        self.log(\"train/loss_formant\", loss_formant, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        loss_formant = loss_formant * self.hparams.loss_scale_formant  # lambda 값 적용\n",
        "\n",
        "        #Generator의 총 손실 (Wavlm + Formant loss 추가)\n",
        "        g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel + loss_formant + wavlm_feature_loss #여길 건드려서 loss를 조절하면 됨\n",
        "\n",
        "        self.manual_backward(g_loss) #바로 위의 g_loss를 기반으로 역전파를 수행하여 그래디언트 계산\n",
        "        opt_g.step() #Generator 옵티마이저가 파라미터 갱신\n",
        "        self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_d.zero_grad() #Discriminator 옵티마이저의 그래디언트 초기화\n",
        "        detached_y_g_hats = [x.detach() for x in y_g_hats] #Discriminator 학습 시 Generator의 그래디언트를 업데이트하지 않도록 분리\n",
        "\n",
        "        y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "        loss_disc_u, _, _ = discriminator_loss(y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "        loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "        gradient_penalty_combd = self.compute_gradient_penalty(self.combd,ys[-1], detached_y_g_hats[-1])\n",
        "        gradient_penalty_sbd = self.compute_gradient_penalty(self.sbd,y, detached_y_g_hats[-1])\n",
        "\n",
        "        d_loss = loss_disc_s + loss_disc_u + self.lambda_gp*gradient_penalty_combd  + self.lambda_gp*gradient_penalty_sbd #single band + multi band\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "        self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        # Save first batch data for visualization at epoch end\n",
        "        if batch_idx == 0:\n",
        "            self.validation_data = (x, y, y_mel)\n",
        "\n",
        "        y_g_hat = self(x)\n",
        "\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hat.squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "         # L1 loss 계산\n",
        "        l1_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "\n",
        "        # RMSE 계산\n",
        "        rmse = torch.sqrt(F.mse_loss(y_g_hat_mel, y_mel, reduction='mean'))\n",
        "\n",
        "        # F0_RMSE 계산\n",
        "        f0_y = extract_f0_from_audio(y.squeeze(1), self.hparams.audio.sampling_rate)\n",
        "        f0_y_g_hat = extract_f0_from_audio(y_g_hat.squeeze(1), self.hparams.audio.sampling_rate)\n",
        "        rmse_f0 = f0_rmse(f0_y_g_hat, f0_y)\n",
        "\n",
        "        # LSD\n",
        "        LSD = lsd(y_g_hat_mel,  y_mel)\n",
        "\n",
        "        # LSD_HF\n",
        "        LSD_HF = lsd_hf(y_g_hat_mel,  y_mel)\n",
        "\n",
        "        # SCOREQ 계산\n",
        "        # 참조 및 생성된 오디오를 파일로 저장 (SCOREQ는 파일 경로로 입력 받음)\n",
        "        test_audio_path = f\"temp_test_audio_{batch_idx}.wav\"\n",
        "        torchaudio.save(test_audio_path, y_g_hat.squeeze(1).cpu(), self.hparams.audio.sampling_rate)\n",
        "\n",
        "        # No-reference 모드에서 품질 평가\n",
        "        mos_score = self.scoreq_nr.predict(test_path=test_audio_path)\n",
        "\n",
        "        self.validation_l1_outputs.append(l1_loss)\n",
        "        self.validation_rmse_outputs.append(rmse)\n",
        "        self.validation_rmse_f0_outputs.append(rmse_f0)\n",
        "        self.validation_lsd_outputs.append(LSD)\n",
        "        self.validation_lsd_hf_outputs.append(LSD_HF)\n",
        "        self.validation_scoreq_outputs.append(mos_score)\n",
        "\n",
        "        # Log validation loss\n",
        "        self.log(\"validation/l1_loss\", l1_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"validation/rmse_loss\", rmse, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"validation/rmse_f0_loss\", rmse_f0, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"validation/lsd_loss\", LSD, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"validation/lsd_hf_loss\", LSD_HF, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"validation/mos_score\", mos_score, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # 필요하면 파일 삭제\n",
        "        os.remove(test_audio_path)\n",
        "\n",
        "\n",
        "        return l1_loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Ensure validation data exists\n",
        "        if not hasattr(self, \"validation_data\"):\n",
        "            print(\"No validation data available.\")\n",
        "            return\n",
        "        x, y, y_mel = self.validation_data\n",
        "        y_g_hat = self(x)\n",
        "\n",
        "        self.logger.experiment.log({\n",
        "            \"predicted_audio\": wandb.Audio(\n",
        "                y_g_hat.squeeze().cpu().numpy(),\n",
        "                sample_rate=self.hparams.audio.sampling_rate,\n",
        "                caption=\"Predicted Audio (Sample)\"\n",
        "            ),\n",
        "            \"gt_audio\": wandb.Audio(\n",
        "                y[0].squeeze().cpu().numpy(),\n",
        "                sample_rate=self.hparams.audio.sampling_rate,\n",
        "                caption=\"Ground Truth Audio (Sample)\"\n",
        "            )\n",
        "        })\n",
        "\n",
        "        if self.validation_l1_outputs:\n",
        "            avg_val_l1loss = torch.stack(self.validation_l1_outputs).mean()\n",
        "            self.log(\"validation/avg_l1loss\", avg_val_l1loss, prog_bar=True, logger=True)\n",
        "\n",
        "            avg_val_rmseloss = torch.stack(self.validation_rmse_outputs).mean()\n",
        "            self.log(\"validation/avg_rmseloss\", avg_val_rmseloss, prog_bar=True, logger=True)\n",
        "\n",
        "            avg_val_rmsef0loss = torch.stack(self.validation_rmse_f0_outputs).mean()\n",
        "            self.log(\"validation/avg_rmsef0loss\", avg_val_rmsef0loss, prog_bar=True, logger=True)\n",
        "\n",
        "            avg_val_lsdloss = torch.stack(self.validation_lsd_outputs).mean()\n",
        "            self.log(\"validation/avg_lsdloss\", avg_val_lsdloss, prog_bar=True, logger=True)\n",
        "\n",
        "            avg_val_lsdhfloss = torch.stack(self.validation_lsd_hf_outputs).mean()\n",
        "            self.log(\"validation/avg_lsdhfloss\", avg_val_lsdhfloss, prog_bar=True, logger=True)\n",
        "\n",
        "            avg_val_mossloss = torch.stack([torch.tensor(x, dtype=torch.float32) for x in self.validation_scoreq_outputs]).mean()\n",
        "            self.log(\"validation/avg_mossloss\", avg_val_mossloss, prog_bar=True, logger=True)\n",
        "\n",
        "        self.validation_l1_outputs.clear()\n",
        "        self.validation_rmse_outputs.clear()\n",
        "        self.validation_rmse_f0_outputs.clear()\n",
        "        self.validation_lsd_outputs.clear()\n",
        "        self.validation_lsd_hf_outputs.clear()\n",
        "        self.validation_scoreq_outputs.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VohGwH8g_Dov"
      },
      "source": [
        "#train.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_name = \"AVOCODO_WGAN_formant_WAVLM\""
      ],
      "metadata": {
        "id": "mDK6dXRwtt16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK6AJpMZxaOC"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# WandB 프로젝트 초기화\n",
        "wandb_logger = WandbLogger(project=\"AVOCODO\", name= wandb_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHLRL85QEBkW"
      },
      "outputs": [],
      "source": [
        "class TBLogger(TensorBoardLogger):\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        metrics.pop('epoch', None)\n",
        "        return super().log_metrics(metrics, step)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/AVOCODO/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo' # 변경해주세용"
      ],
      "metadata": {
        "id": "aQS4nS6RlC9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY4uFuC9EEZY"
      },
      "outputs": [],
      "source": [
        "# parser = argparse.ArgumentParser() #세팅\n",
        "\n",
        "# parser.add_argument('--group_name', default=None)\n",
        "# parser.add_argument('--input_wavs_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs')\n",
        "# parser.add_argument('--input_mels_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram')\n",
        "# parser.add_argument('--input_training_file',\n",
        "#                     default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt')\n",
        "# parser.add_argument('--input_validation_file',\n",
        "#                     default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt')\n",
        "# parser.add_argument('--config', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json')\n",
        "# parser.add_argument('--training_epochs', default=100, type=int)\n",
        "# parser.add_argument('--fine_tuning', default=False, type=bool)\n",
        "# parser.add_argument('--wavlm_config_pretrained',default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt')\n",
        "\n",
        "\n",
        "\n",
        "# if \"ipykernel_launcher\" in sys.argv[0]:\n",
        "#     sys.argv = [\n",
        "#         'script_name',\n",
        "#         '--group_name', 'default_group',\n",
        "#         '--input_wavs_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs',\n",
        "#         '--input_mels_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram',\n",
        "#         '--input_training_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt',\n",
        "#         '--input_validation_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt',\n",
        "#         '--config', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json',\n",
        "#         '--training_epochs', '100',\n",
        "#         '--fine_tuning', 'False',\n",
        "#         '--wavlm_config_pretrained','/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt'\n",
        "#     ]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from easydict import EasyDict as edict\n",
        "\n",
        "path = '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo'\n",
        "\n",
        "args = edict({\n",
        "    'group_name': 'default_group',\n",
        "    'input_wavs_dir': f'{path}/dataset/LJSpeech-1.1/wavs',\n",
        "    'input_mels_dir': f'{path}/mel_spectrogram',\n",
        "    'input_training_file': f'{path}/dataset_setting/training.txt',\n",
        "    'input_validation_file': f'{path}/dataset_setting/validation.txt',\n",
        "    'config': f'{path}/config/avocodo_v1_wavlm.json',\n",
        "    'training_epochs': 50,\n",
        "    'fine_tuning': False,\n",
        "    'wavlm_config_pretrained': f'{path}/dataset/WavLM-Base.pt'\n",
        "})"
      ],
      "metadata": {
        "id": "4B7slUtm78Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJuJSVJR_DRA"
      },
      "outputs": [],
      "source": [
        "# OmegaConf 설정\n",
        "OmegaConf.register_resolver(\"from_args\", lambda x: getattr(args, x))\n",
        "OmegaConf.register_resolver(\"dir\", lambda base_dir, string: os.path.join(base_dir, string))\n",
        "conf = OmegaConf.load(args.config)\n",
        "conf.data.input_wavs_dir = args.input_wavs_dir\n",
        "conf.data.input_mels_dir = args.input_mels_dir\n",
        "conf.data.input_training_file = args.input_training_file\n",
        "conf.data.input_validation_file = args.input_validation_file\n",
        "conf.data.fine_tuning = args.fine_tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQYocu51VK1C"
      },
      "outputs": [],
      "source": [
        "# 데이터 및 모델 초기화\n",
        "dm = AvocodoData(conf.data)\n",
        "\n",
        "model = Avocodo(conf.model)\n",
        "\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/AVOCODO/checkpoint/Avocodo_WGAN_formant_WAVLM/best-checkpoint-epoch=09-avg_lsdloss=0.00.ckpt')\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "limit_train_batches = 1.0\n",
        "limit_val_batches = 1.0\n",
        "log_every_n_steps = 50\n",
        "max_epochs = conf.model.train.training_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hmp4fZrEasS"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='validation/avg_lsdloss',               # Validation 손실 기준\n",
        "    dirpath= path+'/checkpoint/Avocodo_WGAN_formant_WAVLM',  # 체크포인트 저장 디렉토리\n",
        "    filename='best-checkpoint-{epoch:02d}-{avg_lsdloss:.2f}',  # 파일 이름 패턴\n",
        "    save_top_k=1,                     # 가장 좋은 k개의 모델만 저장\n",
        "    mode='min',                       # 손실 기준으로 최소값 저장\n",
        ")\n",
        "\n",
        "# 조기 종료 콜백: Validation 손실 개선이 없으면 중단\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='validation/avg_lsdloss',              # Validation 손실 기준\n",
        "    patience=10,                     # 몇 에포크 동안 개선 없으면 중단\n",
        "    verbose=True,                    # 중단 시 메시지 출력\n",
        "    mode='min'                       # 손실 기준으로 최소값 기준\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUWFRnb9EJ33"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = path+'/checkpoint/Avocodo_WGAN_formant_WAVLM'\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=\"auto\",\n",
        "    max_epochs=100,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        RichProgressBar(\n",
        "            refresh_rate=1,\n",
        "            theme=RichProgressBarTheme(\n",
        "                description=\"#AF81EB\",\n",
        "                progress_bar=\"#8BE9FE\",\n",
        "                progress_bar_finished=\"#8BE9FE\",\n",
        "                progress_bar_pulse=\"#1363DF\",\n",
        "                batch_progress=\"#AF81EB\",\n",
        "                time=\"#1363DF\",\n",
        "                processing_speed=\"#1363DF\",\n",
        "                metrics=\"#9BF9FE\",\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    # logger=TensorBoardLogger(\"logs\", name=\"Avocodo\"),\n",
        "    logger=wandb_logger,\n",
        "    limit_train_batches=limit_train_batches,\n",
        "    limit_val_batches=limit_val_batches,\n",
        "    log_every_n_steps=log_every_n_steps,\n",
        "    num_sanity_val_steps=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq4FBO9OArgF"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "y0gJcaH0l03B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LuKV0hhkiCFp",
        "9owc_vcWhiAC",
        "98DxXGMdhmT5",
        "d-6kqMPP1G7a",
        "ZBWGDS_TwA1k",
        "MfcMzi-hiajZ",
        "zSQ3Q0mqht0K",
        "NPeFVq4_hzq6",
        "4ttjqPrPh4sC",
        "BUyxi8fxiHbD"
      ],
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}