{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Week1_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 들어가며"
      ],
      "metadata": {
        "id": "ED-Vl68A8q8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "딥러닝을 공부할 때에는 논문을 읽고 이해하는 것도 중요하나 논문에 나온 아이디어를 실제로 구현할 수 있는 것도 중요합니다. 아이디어들은 많은 경우 수학, 통계학적 직관에서 출발하나 딥러닝 연구는 지극히 경험적 (Empirical)입니다. 앞으로 진행할 과제들은 매주 코딩을 간단히 수행하게 되는데, 데이터 수집부터 모델 제작, 학습 및 추론에 이르기까지 전 과정을 매주 구현하는 것은 무리가 있기에 전체 과정을 몇 단계로 나누고 이들 중 일부를 직접 구현해야 합니다. 딥러닝 모델링과 관련한 코딩 단계를 다음과 같이 나누어 보았습니다.\n",
        "\n",
        "---\n",
        "1.   **데이터 수집 및 전처리**\n",
        "\n",
        "먼저 모델 학습에 사용할 데이터를 수집합니다. 데이터는 주로 선행 연구에서 사용한 것을 이용하는 경우가 많으며 상황에 따라 새로운 데이터셋을 탐색할 수도 있습니다. 이후 모델링에 알맞은 방식으로 데이터를 전처리합니다. 정규화나 표준화를 이용할 수도 있으며, 패딩 (padding), 결측치 처리 (imputation), MFCC, Mel 등과 같은 신호처리, 트리밍 (trimming)(자르기) 등 기법을 사용할 수 있습니다.\n",
        "\n",
        "---\n",
        "2.   **모델 디자인**\n",
        "\n",
        "가장 재미있는 부분입니다. (지극히 주관적) CNN, RNN 등부터 시작해서 FCN, Attention, Masking 등 여러 모델링 아이디어를 코드로 구현하는 과정입니다. 딥러닝 프레임워크에서 제공하는 다양한 tool을 활용할 수도, 고전적인 방식을 활용해 코딩할 수도 있습니다.\n",
        "\n",
        "---\n",
        "3. **모델 학습**\n",
        "\n",
        "역시 매우 중요한 부분입니다. 학습 에폭 (epoch), 배치 크기 (batch size) 등 각종 설정을 조율하는 것부터 목적 함수 (objective function)를 설정하고 최적화 방식을 설정할 수 있습니다. 더불어 Gradient Vanishing 을 해결하거나 과적합을 방지하는 아이디어를 제안할 수 있고 다중 GPU인 경우에 병렬학습을 고안할 수 있습니다. 모델 학습 과정에서 teacher-forcing, distillation 등 학습 방법을 제안할 수도 있습니다.\n",
        "\n",
        "\n",
        "---\n",
        "4. **추론 및 평가**\n",
        "\n",
        "추론은 모델에서 가장 중요한 부분 중 하나입니다. 결국 추론을 할 수 있어야 모델을 상용화하고 배포할 수 있습니다. 디코더 기반이라면 loop을 이용해 autoregressive 구조를 제안해야 할 것이며, 분류 모델이었다면 softmax를 hard label로 바꾸어 실제 라벨을 맵핑하는 과정 등이 그 예시입니다.\n",
        "\n",
        "평가를 위한 지표는 목적 함수와는 다를 수도 있습니다. 모델 성능을 평가하기 위해 시각화, 음성화 등을 이용할 수 있고, 중간 feature extraction을 평가하기 위해 t-SNE 등을 활용하기도 합니다. 사람들에게 설문조사를 통해 모델의 성능을 비교할 수도 있습니다. 또한 모델의 효율성을 평가하기 위해 MACs, FLOPs 등을 이용해 연산복잡도를 측정할 수도 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "5. **기타**\n",
        "\n",
        "위의 과정 이외에도 여타 라이브러리를 이용해 모델 학습을 효율화하거나 모델링을 더 쉽게 만드는 여러 방법을 제안할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "*과제의 많은 문제들은 정답이 없는 경우가 많습니다.*"
      ],
      "metadata": {
        "id": "pxX1mnbP8s3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 1. Design a Model that does not need positional embedding in AST, compare performance with AST without pretraining."
      ],
      "metadata": {
        "id": "FwPUCWg9DUsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "과제 목표: 모델 디자인, 모델 학습, 모델 추론 코드를 작성할 수 있다."
      ],
      "metadata": {
        "id": "XbtBGQItDdXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 2. Test if different ‘patchify’ system does help training process converge better in AST"
      ],
      "metadata": {
        "id": "CIlGP7v03cyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 제반 작업"
      ],
      "metadata": {
        "id": "ZGteOo1OEBwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhmaa2mq8b6W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount(\"/content/gdrive\")\n",
        "import sys\n",
        "!pip install fvcore #컴퓨터 비전용\n",
        "!pip install timm #이미지 모델이 포함된 라이브러리\n",
        "!pip install torchinfo #pytorch의 모델 요약정보를 출력할때 사용\n",
        "!pip install python_speech_features # 음성신호 처리에 필요한 기능들\n",
        "!pip install wget #웹에서 파일 가져오기\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "import timm\n",
        "from timm.models.layers import to_2tuple,trunc_normal_\n",
        "import wget\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "from python_speech_features import mfcc, logfbank\n",
        "import librosa\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from fvcore.nn import parameter_count_table\n",
        "from torchinfo import summary\n",
        "import pickle\n",
        "import einops\n",
        "import torch.autograd as autograd"
      ],
      "metadata": {
        "id": "uvbjHRkmEXO5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing(전처리)"
      ],
      "metadata": {
        "id": "diClIBdVE3Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Description\n",
        "\n",
        "https://www.kaggle.com/c/freesound-audio-tagging/data\n",
        "\n",
        "다양한 악기, 동물소리, 사람 소리 등이 라벨과 함께 주어진 데이터\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "QkSDpbjm9dVu",
        "outputId": "e47274be-7a71-4586-bada-3d549a2cff6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nData Description\\n\\nhttps://www.kaggle.com/c/freesound-audio-tagging/data\\n\\n다양한 악기, 동물소리, 사람 소리 등이 라벨과 함께 주어진 데이터\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "엔벨롭 함수(게이트) - 임계치 이하 에너지는 무시하고 나머지만 남김\n"
      ],
      "metadata": {
        "id": "Hvr6psTvYgZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def envelope(y, rate, threshold): #각 마스크별 유효값 확인(게이트 함수)\n",
        "  mask = []\n",
        "  y = pd.Series(y).apply(np.abs)  # make y absolute value\n",
        "  y_mean = y.rolling(window = int((rate/10)), min_periods = 1, center = True).mean()#??\n",
        "  for mean in y_mean: #각 윈도우의 평균이 임계값이 넘는지 확인하고 추가\n",
        "    if mean > threshold:\n",
        "      mask.append(True)\n",
        "    else:\n",
        "      mask.append(False)\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def normalize_mel(S, min_level_db = -100): #정규화\n",
        "    return np.clip((S-min_level_db)/-min_level_db, 0,1)"
      ],
      "metadata": {
        "id": "tP--ct7e9Z1I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_path = \"/content/gdrive/MyDrive/Audio Classification/audio_train\" # 수정\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/Audio Classification/real_df.csv\") # 수정\n",
        "df.set_index(\"fname\", inplace = True) #fname을 기준으로 인덱스 지정\n",
        "\n",
        "if len(os.listdir(\"/content/gdrive/MyDrive/Audio Classification/clean\")) == 0:\n",
        "  for f in tqdm(df.fname):#데이터프레임의 파일 이름별로 읽어옴\n",
        "    signal, rate = librosa.load(os.path.join(m_path, f), sr = 16000)\n",
        "    mask = envelope(signal, rate, 0.0005)\n",
        "    wavfile.write(filename = \"/content/gdrive/MyDrive/Audio Classification/clean/\"+f, rate = rate, data = signal[mask])\n",
        "#파일이름, 16000 샘플링레이트, 시그널 중 true인 값만 남긴 데이터로만 구성됨\n",
        "\n"
      ],
      "metadata": {
        "id": "waL_JqgEEoGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤으로 정해진 샘플의\n",
        "\n",
        "랜덤한 위치의 멜스펙트로그램과\n",
        "\n",
        "라벨을 npy파일로 저장함"
      ],
      "metadata": {
        "id": "ClD8WRB6a5Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = df[\"label\"].unique() #안겹치게 라벨들 가져옴, numpy배열\n",
        "class_dist = df.groupby([\"label\"])[\"length\"].mean() #각 라벨별 길이평균, Mean duration for every class ex: saxophone = 3.2 ...\n",
        "n_samples = 3 * int(df[\"length\"].sum()/10)  # 샘플링 할 갯수 계산(?) how many times in sampling.\n",
        "prob_dist = class_dist/class_dist.sum() #각 라벨별 샘플링될 확률, sampling probability proportional to n_samples\n",
        "step = int(128*99) #약 0.7초용 스텝갯수 sampling for about 0.7 seconds.\n",
        "\n",
        "def build_rand_feat():\n",
        "  random.seed(1229)\n",
        "  X = []\n",
        "  labels = []\n",
        "  _min, _max = float(\"inf\"), -float(\"inf\")# 최대 최소\n",
        "  for _ in tqdm(range(n_samples)):#샘플 갯수 만큼 반복\n",
        "    rand_class = np.random.choice(class_dist.index, p = prob_dist) #랜덤하게 하나 가져옴\n",
        "    file = np.random.choice(df[df.label == rand_class].index)  # RandomSample a file in the class category\n",
        "    try: #랜덤한 샘플 가져오기\n",
        "\n",
        "      wav, sr = librosa.load(\"/content/gdrive/MyDrive/Audio Classification/clean/\"+file, sr = 16000)\n",
        "      #가져와지는 wav는 numpy2차원 배열\n",
        "      label = str(df.at[file, \"label\"]) # sampled instrument\n",
        "    except:\n",
        "      print(\"can't read file; moving onto next file\")\n",
        "      continue\n",
        "    try:#랜덤한 샘플의 랜덤한 위치 가져오기\n",
        "\n",
        "      rand_index = np.random.randint(0, wav.shape[0]-step)# 스텝이 int라 위치도 int로 가져옴\n",
        "      sample = wav[rand_index:rand_index+step] # sample for about 0.7 seconds\n",
        "\n",
        "      X_sample = librosa.feature.melspectrogram(y = sample, sr = sr, n_fft = 1024, hop_length = 128, n_mels = 128)#데이터 추출\n",
        "      X_sample = normalize_mel(librosa.power_to_db(X_sample, ref = np.max)) # 파워 멜스펙트로그램 Normalized Log Mel Spectrogram\n",
        "      X.append(X_sample)\n",
        "      label = np.where(classes == label)[0][0]\n",
        "      labels.append(label)\n",
        "\n",
        "    except:\n",
        "\n",
        "      continue\n",
        "\n",
        "  return X, labels"
      ],
      "metadata": {
        "id": "IT1VFXL59q5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = build_rand_feat() #랜덤한 요소 멜스펙트로그램, 라벨 저장\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "np.save(\"/content/gdrive/MyDrive/코딩공부/deep_daiv/dataset/Xmel_torch.npy\", X)\n",
        "np.save(\"/content/gdrive/MyDrive/코딩공부/deep_daiv/dataset/ymel_torch.npy\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deSJ4ogI-4IY",
        "outputId": "5063e9b3-583e-47c3-c554-02dc3f63ac37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19290/19290 [09:04<00:00, 35.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Oob9zxtiOeU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "m16S-d31aAkK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 설정 필요\n",
        "\n",
        "X = np.load(\"/content/gdrive/MyDrive/코딩공부/deep_daiv/dataset/Xmel_torch.npy\", allow_pickle = True) # 데이터 수, 멜주파수 대역 갯수, 시간축 길이(#data, #mel_bins, #time bins)\n",
        "y = np.load(\"/content/gdrive/MyDrive/코딩공부/deep_daiv/dataset/ymel_torch.npy\") # 각 파일의 라벨(#data, )"
      ],
      "metadata": {
        "id": "NZQnRxqoOgKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 라벨에 대해서 원핫 인코딩\n",
        "\n",
        "해당 인코딩을 통해"
      ],
      "metadata": {
        "id": "Yx9uk74GctD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 41\n",
        "\n",
        "y = np.eye(num_classes)[y] # 각 데이터에 대해 대각행렬 적용, 한마디로 각각 1을 가지는 위치가 생김One-hot-encoder\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "#데이터 갯수, 멜스펙트로그램 대역 갯수, 시간축 길이(100으로 나눔)\n",
        "#데이터 갯수, 각 라벨에 대한 원핫 인코딩 값"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEF3HE5afvNt",
        "outputId": "e08b912e-6a2e-45a7-b6dc-9a06aa826cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16745, 128, 100) (16745, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "오디오 데이터셋 선언\n"
      ],
      "metadata": {
        "id": "jkI4RdyWdFpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class audio_dataset(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "\n",
        "        #데이터셋으로 묶음\n",
        "        self.X_data = X_data #(#data, #coefs, #timetsteps)\n",
        "        self.y_data = y_data #(#data, #labels)\n",
        "        #플로트 텐서화\n",
        "        self.X_torch = torch.FloatTensor(self.X_data).to(device) #(#data, #coefs, #timetsteps)\n",
        "        self.y_torch = torch.FloatTensor(self.y_data).to(device) #(#data, #labels)\n",
        "        #길이, 샘플 갯수 추출\n",
        "        self.x_len = self.X_torch.shape[0]\n",
        "        self.y_len = self.y_torch.shape[0]\n",
        "        #조건여부 확인\n",
        "        assert self.x_len == self.y_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_torch[index], self.y_torch[index] #x=인덱스값의 데이터, 주파수 대역, 시간 반환, y=인덱스 값의 데이터, 라벨 원핫 반환\n",
        "    def __len__(self): #데이터 갯수 반환\n",
        "        return self.x_len"
      ],
      "metadata": {
        "id": "7WbWw1VWGPTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "CIjeSkUEpjOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_model(model, input_shape, is_cuda = False):\n",
        "  global device\n",
        "\n",
        "  if is_cuda: #gpu여부 확인\n",
        "    x = torch.rand(input_shape).to(device)\n",
        "  else:\n",
        "    x = torch.rand(input_shape)\n",
        "  print(parameter_count_table(model))\n",
        "\n",
        "  print(summary(model, input_size = input_shape))"
      ],
      "metadata": {
        "id": "300deR4JeH0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  # [Batch_size, sequence_length, emb_size]\n",
        "    def __init__(self, emb_size = 384, num_heads = 6, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      #x.shape = [batch_size, num_patches, embed_dim]\n",
        "\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = einops.rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        scaling = self.emb_size ** (1/2)\n",
        "\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = einops.rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "indgeAa73zki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_size=384, num_heads=8, dropout=0.1, forward_expansion=2):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.attn = MultiHeadAttention(emb_size, num_heads, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_size, forward_expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(forward_expansion * emb_size, emb_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.feed_forward(self.norm2(x))\n",
        "        return x # Input shape and Output Shape are the same."
      ],
      "metadata": {
        "id": "907nt48_cDeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AST(nn.Module):\n",
        "    \"\"\"\n",
        "    label_dim: the number of total classes, 41\n",
        "    fstride: the stride of patch spliting on the frequency dimension\n",
        "    tstride: the stride of patch spliting on the time dimension\n",
        "    input_fdim: # frequency bins of the input spectrogram\n",
        "    input_tdim: # time frames of the input spectrogram\n",
        "    embed_dim : # embed_dimensions of ViT\n",
        "    \"\"\"\n",
        "    def __init__(self, label_dim=41, fstride=10, tstride=10, input_fdim=128, input_tdim=100, embed_dim = 384, num_heads = 6, n_blocks = 12, verbose=True):\n",
        "\n",
        "        super(AST, self).__init__()\n",
        "\n",
        "        self.label_dim = label_dim\n",
        "        self.fstride = fstride\n",
        "        self.tstride = tstride\n",
        "        self.input_fdim = input_fdim\n",
        "        self.input_tdim = input_tdim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.n_blocks = n_blocks\n",
        "\n",
        "\n",
        "        if verbose == True:\n",
        "            print('---------------AST Model Initializing---------------')\n",
        "\n",
        "\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(in_channels = 1, out_channels = self.embed_dim, kernel_size = 16, stride = (self.fstride, self.tstride))\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
        "        self.patch_embedding.apply(self.init_weights)\n",
        "        f_dim, t_dim = self.get_shape(fstride = self.fstride, tstride = self.tstride,\n",
        "                       input_fdim = self.input_fdim, input_tdim = self.input_tdim)\n",
        "        num_patches = f_dim * t_dim\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(num_patches + 1, self.embed_dim))\n",
        "\n",
        "        self.blocks = nn.ModuleList() # Transformer block 만들기.\n",
        "        for _ in range(self.n_blocks):\n",
        "          self.blocks.append(TransformerBlock(self.embed_dim, self.num_heads))\n",
        "\n",
        "\n",
        "        self.norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.mlp_head = nn.Sequential(nn.LayerNorm(self.embed_dim), nn.Linear(self.embed_dim, self.label_dim))\n",
        "\n",
        "\n",
        "        if verbose == True:\n",
        "\n",
        "            print('frequency stride={:d}, time stride={:d}'.format(self.fstride, self.tstride))\n",
        "            print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "\n",
        "\n",
        "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=100):\n",
        "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
        "        test_proj = nn.Conv2d(1, self.embed_dim, kernel_size=16, stride=(fstride, tstride))\n",
        "        test_out = test_proj(test_input)\n",
        "        f_dim = test_out.shape[2]\n",
        "        t_dim = test_out.shape[3]\n",
        "        return f_dim, t_dim\n",
        "\n",
        "    def init_weights(self, m, mean = 0.0, std = 0.01):\n",
        "      classname = m.__class__.__name__\n",
        "      if classname.find(\"Conv\") != -1: # if module == Conv\n",
        "        m.weight.data.normal_(mean,std) #Initialize weights\n",
        "\n",
        "    @autocast() #Speed up Training\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        return: prediction\n",
        "        \"\"\"\n",
        "        # expect input x = (batch_size, frequency bins, time bins), e.g., (16, 128, 100)\n",
        "        x = x.unsqueeze(1) # (batch_size, 1, frequency bins, time bins), e.g., (16, 1, 128, 100)\n",
        "\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embedding(x) # (batch_size, #embed_dim, f_dim, t_dim)\n",
        "        x = x.flatten(2) # (batch_size, #embed_dim, num_patches)\n",
        "        x = x.transpose(1, 2) #(batch_size, num_patches, #embed_dim)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, #embed_dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, #embed_dim)\n",
        "        x = x + self.position_embeddings #(B, num_patches + 1, #embed_dim)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = (x[:, 0] + x[:, 1]) / 2 # (B, 1, #embed dim).\n",
        "\n",
        "        x = self.mlp_head(x) # Why no Softmax in the end? https://slamwithme.oopy.io/305bb7e0-1062-4785-a82e-9e2a5debd0f4\n",
        "        return x"
      ],
      "metadata": {
        "id": "eZVBjmwiby9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ef4f61-2645-48f6-8eca-97f113223d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 M parameters\n",
        "\n",
        "ast = AST(num_heads = 3, n_blocks = 6).to(device)\n",
        "summarize_model(ast, [16, 100, 128], is_cuda = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKjMfZ0GeCkk",
        "outputId": "4e1de8f5-e228-4f94-f3f1-304ed2d7d32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Initializing---------------\n",
            "frequency stride=10, time stride=10\n",
            "number of patches=108\n",
            "| name                     | #elements or shape   |\n",
            "|:-------------------------|:---------------------|\n",
            "| model                    | 7.3M                 |\n",
            "|  cls_token               |  (1, 1, 384)         |\n",
            "|  position_embeddings     |  (109, 384)          |\n",
            "|  patch_embedding         |  98.7K               |\n",
            "|   patch_embedding.weight |   (384, 1, 16, 16)   |\n",
            "|   patch_embedding.bias   |   (384,)             |\n",
            "|  blocks                  |  7.1M                |\n",
            "|   blocks.0               |   1.2M               |\n",
            "|    blocks.0.norm1        |    0.8K              |\n",
            "|    blocks.0.norm2        |    0.8K              |\n",
            "|    blocks.0.attn         |    0.6M              |\n",
            "|    blocks.0.feed_forward |    0.6M              |\n",
            "|   blocks.1               |   1.2M               |\n",
            "|    blocks.1.norm1        |    0.8K              |\n",
            "|    blocks.1.norm2        |    0.8K              |\n",
            "|    blocks.1.attn         |    0.6M              |\n",
            "|    blocks.1.feed_forward |    0.6M              |\n",
            "|   blocks.2               |   1.2M               |\n",
            "|    blocks.2.norm1        |    0.8K              |\n",
            "|    blocks.2.norm2        |    0.8K              |\n",
            "|    blocks.2.attn         |    0.6M              |\n",
            "|    blocks.2.feed_forward |    0.6M              |\n",
            "|   blocks.3               |   1.2M               |\n",
            "|    blocks.3.norm1        |    0.8K              |\n",
            "|    blocks.3.norm2        |    0.8K              |\n",
            "|    blocks.3.attn         |    0.6M              |\n",
            "|    blocks.3.feed_forward |    0.6M              |\n",
            "|   blocks.4               |   1.2M               |\n",
            "|    blocks.4.norm1        |    0.8K              |\n",
            "|    blocks.4.norm2        |    0.8K              |\n",
            "|    blocks.4.attn         |    0.6M              |\n",
            "|    blocks.4.feed_forward |    0.6M              |\n",
            "|   blocks.5               |   1.2M               |\n",
            "|    blocks.5.norm1        |    0.8K              |\n",
            "|    blocks.5.norm2        |    0.8K              |\n",
            "|    blocks.5.attn         |    0.6M              |\n",
            "|    blocks.5.feed_forward |    0.6M              |\n",
            "|  norm                    |  0.8K                |\n",
            "|   norm.weight            |   (384,)             |\n",
            "|   norm.bias              |   (384,)             |\n",
            "|  mlp_head                |  16.6K               |\n",
            "|   mlp_head.0             |   0.8K               |\n",
            "|    mlp_head.0.weight     |    (384,)            |\n",
            "|    mlp_head.0.bias       |    (384,)            |\n",
            "|   mlp_head.1             |   15.8K              |\n",
            "|    mlp_head.1.weight     |    (41, 384)         |\n",
            "|    mlp_head.1.bias       |    (41,)             |\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "AST                                      [16, 41]                  42,240\n",
            "├─Conv2d: 1-1                            [16, 384, 9, 12]          98,688\n",
            "├─ModuleList: 1-2                        --                        --\n",
            "│    └─TransformerBlock: 2-1             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-1               [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-2      [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-3               [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-4              [16, 109, 384]            590,976\n",
            "│    └─TransformerBlock: 2-2             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-5               [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-6      [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-7               [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-8              [16, 109, 384]            590,976\n",
            "│    └─TransformerBlock: 2-3             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-9               [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-10     [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-11              [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-12             [16, 109, 384]            590,976\n",
            "│    └─TransformerBlock: 2-4             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-13              [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-14     [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-15              [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-16             [16, 109, 384]            590,976\n",
            "│    └─TransformerBlock: 2-5             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-17              [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-18     [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-19              [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-20             [16, 109, 384]            590,976\n",
            "│    └─TransformerBlock: 2-6             [16, 109, 384]            --\n",
            "│    │    └─LayerNorm: 3-21              [16, 109, 384]            768\n",
            "│    │    └─MultiHeadAttention: 3-22     [16, 109, 384]            591,360\n",
            "│    │    └─LayerNorm: 3-23              [16, 109, 384]            768\n",
            "│    │    └─Sequential: 3-24             [16, 109, 384]            590,976\n",
            "├─LayerNorm: 1-3                         [16, 109, 384]            768\n",
            "├─Sequential: 1-4                        [16, 41]                  --\n",
            "│    └─LayerNorm: 2-7                    [16, 384]                 768\n",
            "│    └─Linear: 2-8                       [16, 41]                  15,785\n",
            "==========================================================================================\n",
            "Total params: 7,261,481\n",
            "Trainable params: 7,261,481\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 284.46\n",
            "==========================================================================================\n",
            "Input size (MB): 0.82\n",
            "Forward/backward pass size (MB): 300.03\n",
            "Params size (MB): 28.88\n",
            "Estimated Total Size (MB): 329.73\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "w7wNdbszOdQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    'lr': 0.0002,\n",
        "    'epochs': 30,\n",
        "    'min_batch': 16,\n",
        "    'weight_decay': 1e-4, # optional, TBD\n",
        "    \"save_path\": \"/content/gdrive/MyDrive/Audio Classification\"\n",
        "}"
      ],
      "metadata": {
        "id": "A8N183NGWx4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_len = X.shape[0]\n",
        "train_size = 0.8\n",
        "idx = np.random.permutation(X_len)\n",
        "train_idx = idx[:round(train_size*X_len)]; test_idx = idx[round(train_size*X_len):]\n",
        "X_train = X[train_idx].astype(np.float32); X_test = X[test_idx].astype(np.float32);\n",
        "y_train = y[train_idx].astype(np.float32); y_test = y[test_idx].astype(np.float32) ;\n",
        "audio_train = audio_dataset(X_train, y_train)\n",
        "audio_test = audio_dataset(X_test, y_test)\n",
        "train_loader = DataLoader(audio_train, batch_size = CONFIG[\"min_batch\"], shuffle = True)\n",
        "test_loader = DataLoader(audio_test, batch_size = CONFIG[\"min_batch\"], shuffle = True)"
      ],
      "metadata": {
        "id": "x8gZtb1rrysh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AST(num_heads = 3, n_blocks = 6).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = CONFIG[\"lr\"])\n",
        "\n",
        "loss_history = []\n",
        "val_loss_history = []\n",
        "init_loss = 991229 # Random high number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-_VmwhK35Po",
        "outputId": "ca2e6214-902d-4392-a725-0824b486ae10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Initializing---------------\n",
            "frequency stride=10, time stride=10\n",
            "number of patches=108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing training...\")\n",
        "torch.manual_seed(1229)\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(CONFIG[\"epochs\"])):\n",
        "  print(f\"\\n Epoch {epoch}...\")\n",
        "  run_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    run_loss += loss.item()\n",
        "\n",
        "\n",
        "    if i % 500 == 499:\n",
        "      loss_history.append(run_loss/500)\n",
        "      print(f'[{epoch +1}, {i + 1:5d}] loss : {run_loss/500:.3f}')\n",
        "      run_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          val_loss = 0.0\n",
        "          for k, (val_inputs, val_labels) in enumerate(test_loader):\n",
        "              val_output = model(val_inputs)\n",
        "              v_loss = criterion(val_output, val_labels)\n",
        "              val_loss += v_loss\n",
        "          print(f'[{epoch + 1}, {i + 1:5d}] val loss: {val_loss / k:.3f}')\n",
        "          val_loss_history.append(val_loss.item()/500)\n",
        "\n",
        "      if val_loss < init_loss:\n",
        "          torch.save(model, os.path.join(CONFIG[\"save_path\"], 'ast.pt'))\n",
        "\n",
        "          init_loss = val_loss\n",
        "\n",
        "print(\"finished training ...\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SR7yxUY41wN",
        "outputId": "23fffa59-22ed-446c-9bfa-e6ac1b2d2ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 0...\n",
            "[1,   500] loss : 3.311\n",
            "[1,   500] val loss: 3.070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/30 [28:47<13:54:56, 1727.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1...\n",
            "[2,   500] loss : 2.734\n",
            "[2,   500] val loss: 2.752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [57:11<13:19:37, 1713.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 2...\n",
            "[3,   500] loss : 2.590\n",
            "[3,   500] val loss: 2.552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [1:25:34<12:49:01, 1708.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 3...\n",
            "[4,   500] loss : 2.413\n",
            "[4,   500] val loss: 2.400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [1:54:03<12:20:33, 1709.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 4...\n",
            "[5,   500] loss : 2.243\n",
            "[5,   500] val loss: 2.357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [2:22:36<11:52:36, 1710.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 5...\n",
            "[6,   500] loss : 2.082\n",
            "[6,   500] val loss: 2.155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [2:51:03<11:23:41, 1709.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 6...\n",
            "[7,   500] loss : 1.938\n",
            "[7,   500] val loss: 2.082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [3:19:29<10:54:44, 1708.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 7...\n",
            "[8,   500] loss : 1.791\n",
            "[8,   500] val loss: 1.955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [3:47:53<10:25:48, 1706.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 8...\n",
            "[9,   500] loss : 1.684\n",
            "[9,   500] val loss: 1.937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [4:16:17<9:57:04, 1705.95s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 9...\n",
            "[10,   500] loss : 1.578\n",
            "[10,   500] val loss: 1.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [4:44:44<9:28:45, 1706.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 10...\n",
            "[11,   500] loss : 1.457\n",
            "[11,   500] val loss: 1.776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [5:13:07<8:59:58, 1705.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 11...\n",
            "[12,   500] loss : 1.363\n",
            "[12,   500] val loss: 1.766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [5:41:30<8:31:25, 1704.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 12...\n",
            "[13,   500] loss : 1.253\n",
            "[13,   500] val loss: 1.668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [6:09:56<8:03:06, 1705.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 13...\n",
            "[14,   500] loss : 1.130\n",
            "[14,   500] val loss: 1.629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Nc27d83X7mzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, X_test, y_test, device):\n",
        "  model.eval()\n",
        "  inputs = X_test\n",
        "  labels = y_test\n",
        "\n",
        "  with torch.no_grad():\n",
        "      inputs = torch.FloatTensor(inputs).to(device)  # (#data, #dim, #timesteps)\n",
        "      labels = torch.FloatTensor(labels).to(device)  # (#data, #labels)\n",
        "      utputs = model(inputs)\n",
        "      preds = torch.argmax(outputs, dim=1)\n",
        "      labels = torch.argmax(labels)\n",
        "  return np.array(preds.cpu()), np.array(labels.cpu())\n"
      ],
      "metadata": {
        "id": "CQ8Swi_86ITS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}