{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1C4Q6ceYWlu8kYcQJIFceFKI7aMntgr45",
      "authorship_tag": "ABX9TyP6n33XoPixIvJtZrtoYVcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/fast_speech2_%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8_origin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fast speech2 파이프라인"
      ],
      "metadata": {
        "id": "pBVf5fzTMwJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/HGU-DLLAB/Korean-FastSpeech2-Pytorch"
      ],
      "metadata": {
        "id": "UTBT-hr8xMcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #데이터셋이 없는 경우에만 다운로드\n",
        "# import os\n",
        "\n",
        "# # Kaggle 데이터셋 다운로드\n",
        "# !kaggle datasets download -d bryanpark/korean-single-speaker-speech-dataset -p /content/drive/MyDrive/fast_speech2\n",
        "\n",
        "# # 압축 해제\n",
        "# !unzip -q /content/drive/MyDrive/fast_speech2/korean-single-speaker-speech-dataset.zip -d /content/drive/MyDrive/fast_speech2/dataset\n",
        "# print(\"Dataset saved to /content/drive/MyDrive/fast_speech2\")"
      ],
      "metadata": {
        "id": "qq2RlS_FYse7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gdown #다운 받으면 일부 파일 누락됨\n",
        "# !gdown https://drive.google.com/uc?id=1LgZPfWAvPcdOpGBSncvMgv54rGIf1y-H"
      ],
      "metadata": {
        "id": "u2EeuZCEKDZ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnTU5aw3M3ND",
        "outputId": "a2979ac7-3207-4ebf-dbc9-80ab9d3768ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q /content/drive/MyDrive/TextGrid.zip -d /content/drive/MyDrive/fast_speech2/dataset/preprocessed/kss\n",
        "# print(\"Dataset saved to /content/drive/MyDrive/fast_speech2/dataset/preprocessed/kss\")"
      ],
      "metadata": {
        "id": "7DJePL3ALE8P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCwB_-wCLzxC",
        "outputId": "917ef5bb-cc97-4a00-d84d-94dba64a6b45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "# import os\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"4c7d91ca2cd073dc0f1c148b6e4bacff713df5c6\"\n",
        "# wandb.login()"
      ],
      "metadata": {
        "id": "NHAdSllZL2gb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/drive/MyDrive/fast_speech2/requirements.txt\"\n",
        "output.clear() #버전 에러가 나는 경우, 여기 확인"
      ],
      "metadata": {
        "id": "4Itmk-38Q-bT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install g2pk\n",
        "!pip install jamo\n",
        "!pip install unidecode\n",
        "!pip install pyworld\n",
        "!pip install tgt\n",
        "!pip install cleaners\n",
        "!pip install utils\n",
        "!pip install audio\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "p7yAMUPBzC7x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import codecs\n",
        "import argparse\n",
        "from string import punctuation\n",
        "from collections import OrderedDict\n",
        "import copy\n",
        "import ast\n",
        "import inflect\n",
        "import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "\n",
        "# Numerical Computations\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# Korean Language Libraries\n",
        "from g2pk import G2p\n",
        "from jamo import hangul_to_jamo, h2j, j2h, hcj_to_jamo, is_hcj\n",
        "from jamo.jamo import _jamo_char_to_hcj\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Audio and Signal Libraries\n",
        "from scipy.io import wavfile\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy.signal import get_window\n",
        "from librosa.util import pad_center, tiny\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "import librosa.util as librosa_util\n",
        "import pyworld as pw\n",
        "import tgt\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "EP4LDynQ1FjW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 하이퍼 파라미터, 필요 내용"
      ],
      "metadata": {
        "id": "49qmgLfLOdNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams:\n",
        "    # Dataset paths\n",
        "    dataset = \"my_voice_dataset\"\n",
        "    data_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/dataset\",dataset)#내가 만든 데이터셋 위치\n",
        "    meta_name = \"metadata.txt\"  # 음성 데이터 발화내용\n",
        "    textgrid_name = \"my_voice_dataset_aligned.zip\" #경로 문제뜨면 이거 대소문자 문제임\n",
        "\n",
        "    # GPU settings\n",
        "    train_visible_devices = \"0,1\"\n",
        "    synth_visible_devices = \"0\"\n",
        "\n",
        "    # Text\n",
        "    text_cleaners = ['korean_cleaners']\n",
        "\n",
        "    # Audio and mel\n",
        "    sampling_rate = 22050\n",
        "    filter_length = 1024\n",
        "    hop_length = 256\n",
        "    win_length = 1024\n",
        "    max_wav_value = 32768.0\n",
        "    n_mel_channels = 80\n",
        "    mel_fmin = 0\n",
        "    mel_fmax = 8000\n",
        "\n",
        "    f0_min = 71.0\n",
        "    f0_max = 792.8\n",
        "    energy_min = 0.0\n",
        "    energy_max = 283.72\n",
        "\n",
        "    # FastSpeech 2\n",
        "    encoder_layer = 4\n",
        "    encoder_head = 2\n",
        "    encoder_hidden = 256\n",
        "    decoder_layer = 4\n",
        "    decoder_head = 2\n",
        "    decoder_hidden = 256\n",
        "    fft_conv1d_filter_size = 1024\n",
        "    fft_conv1d_kernel_size = (9, 1)\n",
        "    encoder_dropout = 0.2\n",
        "    decoder_dropout = 0.2\n",
        "\n",
        "    variance_predictor_filter_size = 256\n",
        "    variance_predictor_kernel_size = 3\n",
        "    variance_predictor_dropout = 0.5\n",
        "\n",
        "    max_seq_len = 1000\n",
        "\n",
        "    # Checkpoints and synthesis path, 원 계정에서 사용할 것\n",
        "    # preprocessed_path = os.path.join(\"./preprocessed/\", dataset)\n",
        "    # checkpoint_path = os.path.join(\"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/fast_speech_2/fast_speech2/checkpoint\")\n",
        "    # eval_path = os.path.join(\"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/fast_speech_2/fast_speech2_korean_dataset/eval\")\n",
        "    # log_path = os.path.join(\"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/fast_speech_2/logs\")\n",
        "    # test_path = \"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/fast_speech_2/result\"\n",
        "\n",
        "    preprocessed_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/dataset/preprocessed\",dataset)\n",
        "    checkpoint_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/checkpoint/kss/checkpoint_350000.pth.tar\")# fast speech2 사전학습모델\n",
        "    eval_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/dataset/eval\", dataset)\n",
        "    log_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/log\", dataset)\n",
        "    test_path = \"/content/drive/MyDrive/fast_speech2/results/test\"\n",
        "\n",
        "    # Optimizer\n",
        "    batch_size = 4\n",
        "    epochs = 1000\n",
        "    n_warm_up_step = 4000\n",
        "    grad_clip_thresh = 1.0\n",
        "    acc_steps = 1\n",
        "    betas = (0.9, 0.98)\n",
        "    eps = 1e-9\n",
        "    weight_decay = 0.0\n",
        "\n",
        "\n",
        "    # Vocoder, 나중에 원하는 모델로 변경 및 몇몇 함수 수정필요\n",
        "    # vocoder = 'avocodo'\n",
        "    vocoder = 'vocgan'\n",
        "    vocoder_pretrained_model_name = \"vocgan_kss_pretrained_model_epoch_4500.pt\" #사전 학습 모델 생기면 지정\n",
        "    vocoder_pretrained_model_path = os.path.join(\"/content/drive/MyDrive/fast_speech2/checkpoint/\", vocoder_pretrained_model_name)# 아예 위치 지정\n",
        "\n",
        "\n",
        "    # Log-scaled duration\n",
        "    log_offset = 1.\n",
        "\n",
        "    # Save, log, and synthesis\n",
        "    save_step = 10000\n",
        "    eval_step = 1000\n",
        "    eval_size = 256\n",
        "    log_step = 1000\n",
        "    clear_Time = 20\n",
        "\n",
        "\n",
        "# Instantiate hparams for usage\n",
        "hp = HParams()\n"
      ],
      "metadata": {
        "id": "OubgEp2TOgO7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#text파일"
      ],
      "metadata": {
        "id": "8VPdgWpAN2af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PAD = '_'\n",
        "EOS = '~'\n",
        "PUNC = '!\\'(),-.:;?'\n",
        "SPACE = ' '\n",
        "_SILENCES = ['sp', 'spn', 'sil']\n",
        "\n",
        "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
        "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
        "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
        "\n",
        "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + PUNC + SPACE\n",
        "ALL_SYMBOLS = list(PAD + EOS + VALID_CHARS) + _SILENCES\n",
        "s_to_i={c: i for i, c in enumerate(ALL_SYMBOLS)}\n",
        "#print('s_to_i: ',s_to_i)\n",
        "KOR_SYMBOLS=ALL_SYMBOLS\n",
        "\n",
        "Kchar_to_id={c: i for i, c in enumerate(KOR_SYMBOLS)}\n",
        "id_to_Kchar={i: c for i, c in enumerate(KOR_SYMBOLS)}\n",
        "\n",
        "'''\n",
        "Cleaners are transformations that run over the input text at both training and eval time.\n",
        "\n",
        "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
        "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
        "  1. \"english_cleaners\" for English text\n",
        "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
        "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
        "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
        "     the symbols in symbols.py to match your data).\n",
        "'''\n"
      ],
      "metadata": {
        "id": "ckEO7mET3gML",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "81bc7885-89f1-498a-dde2-6000f600ec09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCleaners are transformations that run over the input text at both training and eval time.\\n\\nCleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\\nhyperparameter. Some cleaners are English-specific. You\\'ll typically want to use:\\n  1. \"english_cleaners\" for English text\\n  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\\n  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\\n     the symbols in symbols.py to match your data).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kor_symbols=KOR_SYMBOLS\n",
        "symbols=kor_symbols"
      ],
      "metadata": {
        "id": "pMrWjvRGD5e9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mappings from symbol to numeric ID and vice versa:\n",
        "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
        "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
        "\n",
        "# Regular expression matching text enclosed in curly braces:\n",
        "_curly_re = re.compile(r'(.*?)\\{(.+?)\\}(.*)')\n",
        "\n",
        "\n",
        "def text_to_sequence(text, cleaner_names):\n",
        "    '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
        "\n",
        "      The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
        "      in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
        "\n",
        "      Args:\n",
        "        text: string to convert to a sequence\n",
        "        cleaner_names: names of the cleaner functions to run the text through\n",
        "\n",
        "      Returns:\n",
        "        List of integers corresponding to the symbols in the text\n",
        "    '''\n",
        "\n",
        "    sequence = []\n",
        "    while len(text):\n",
        "        m = _curly_re.match(text)\n",
        "        if not m:\n",
        "            print('not m!')\n",
        "            sequence = _symbols_to_sequence(text)\n",
        "            break\n",
        "        sequence = _symbols_to_sequence(m.group(2))\n",
        "        text = m.group(3)\n",
        "    return sequence\n",
        "\n",
        "def sequence_to_text(sequence):\n",
        "    '''Converts a sequence of IDs back to a string'''\n",
        "    result = ''\n",
        "    for symbol_id in sequence:\n",
        "        if symbol_id in _id_to_symbol:\n",
        "            s = _id_to_symbol[symbol_id]\n",
        "            # Enclose ARPAbet back in curly braces:\n",
        "            if len(s) > 1 and s[0] == '@':\n",
        "                s = '{%s}' % s[1:]\n",
        "            result += s\n",
        "    return result.replace('}{', ' ')\n",
        "\n",
        "\n",
        "def _clean_text(text, cleaner_names):\n",
        "    for name in cleaner_names:\n",
        "        cleaner = getattr(cleaners, name)\n",
        "        if not cleaner:\n",
        "            raise Exception('Unknown cleaner: %s' % name)\n",
        "        text = cleaner(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def _symbols_to_sequence(symbols):\n",
        "    return [_symbol_to_id[s] for s in symbols.split() if _should_keep_symbol(s)]\n",
        "\n",
        "\n",
        "def _arpabet_to_sequence(text):\n",
        "    return _symbols_to_sequence(['@' + s for s in text.split()])\n",
        "\n",
        "\n",
        "def _should_keep_symbol(s):\n",
        "    #return s in _symbol_to_id and s is not '_' and s is not '~'\n",
        "    return s in _symbol_to_id and s != '~' and s != '_'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print()"
      ],
      "metadata": {
        "id": "YNx0-hSEDrFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb33889c-2515-42a8-a72c-b71ae52ebb90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_inflect = inflect.engine()\n",
        "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
        "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
        "_pounds_re = re.compile(r'£([0-9\\,]*[0-9]+)')\n",
        "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
        "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
        "_number_re = re.compile(r'[0-9]+')\n",
        "\n",
        "\n",
        "def _remove_commas(m):\n",
        "  return m.group(1).replace(',', '')\n",
        "\n",
        "\n",
        "def _expand_decimal_point(m):\n",
        "  return m.group(1).replace('.', ' point ')\n",
        "\n",
        "\n",
        "def _expand_dollars(m):\n",
        "  match = m.group(1)\n",
        "  parts = match.split('.')\n",
        "  if len(parts) > 2:\n",
        "    return match + ' dollars'  # Unexpected format\n",
        "  dollars = int(parts[0]) if parts[0] else 0\n",
        "  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
        "  if dollars and cents:\n",
        "    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
        "    cent_unit = 'cent' if cents == 1 else 'cents'\n",
        "    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
        "  elif dollars:\n",
        "    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
        "    return '%s %s' % (dollars, dollar_unit)\n",
        "  elif cents:\n",
        "    cent_unit = 'cent' if cents == 1 else 'cents'\n",
        "    return '%s %s' % (cents, cent_unit)\n",
        "  else:\n",
        "    return 'zero dollars'\n",
        "\n",
        "\n",
        "def _expand_ordinal(m):\n",
        "  return _inflect.number_to_words(m.group(0))\n",
        "\n",
        "\n",
        "def _expand_number(m):\n",
        "  num = int(m.group(0))\n",
        "  if num > 1000 and num < 3000:\n",
        "    if num == 2000:\n",
        "      return 'two thousand'\n",
        "    elif num > 2000 and num < 2010:\n",
        "      return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
        "    elif num % 100 == 0:\n",
        "      return _inflect.number_to_words(num // 100) + ' hundred'\n",
        "    else:\n",
        "      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
        "  else:\n",
        "    return _inflect.number_to_words(num, andword='')\n",
        "\n",
        "\n",
        "def normalize_numbers(text):\n",
        "  text = re.sub(_comma_number_re, _remove_commas, text)\n",
        "  text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
        "  text = re.sub(_dollars_re, _expand_dollars, text)\n",
        "  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
        "  text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
        "  text = re.sub(_number_re, _expand_number, text)\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "AyPZXbTwNlin"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "_whitespace_re = re.compile(r'\\s+')\n",
        "\n",
        "# List of (regular expression, replacement) pairs for abbreviations:\n",
        "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
        "    ('mrs', 'misess'),\n",
        "    ('mr', 'mister'),\n",
        "    ('dr', 'doctor'),\n",
        "    ('st', 'saint'),\n",
        "    ('co', 'company'),\n",
        "    ('jr', 'junior'),\n",
        "    ('maj', 'major'),\n",
        "    ('gen', 'general'),\n",
        "    ('drs', 'doctors'),\n",
        "    ('rev', 'reverend'),\n",
        "    ('lt', 'lieutenant'),\n",
        "    ('hon', 'honorable'),\n",
        "    ('sgt', 'sergeant'),\n",
        "    ('capt', 'captain'),\n",
        "    ('esq', 'esquire'),\n",
        "    ('ltd', 'limited'),\n",
        "    ('col', 'colonel'),\n",
        "    ('ft', 'fort'),\n",
        "]]\n",
        "\n",
        "\n",
        "def expand_abbreviations(text):\n",
        "    for regex, replacement in _abbreviations:\n",
        "        text = re.sub(regex, replacement, text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def expand_numbers(text):\n",
        "    return normalize_numbers(text)\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def collapse_whitespace(text):\n",
        "    return re.sub(_whitespace_re, ' ', text)\n",
        "\n",
        "\n",
        "def convert_to_ascii(text):\n",
        "    return unidecode(text)\n",
        "\n",
        "\n",
        "def basic_cleaners(text):\n",
        "    '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
        "    text = lowercase(text)\n",
        "    text = collapse_whitespace(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def transliteration_cleaners(text):\n",
        "    '''Pipeline for non-English text that transliterates to ASCII.'''\n",
        "    text = convert_to_ascii(text)\n",
        "    text = lowercase(text)\n",
        "    text = collapse_whitespace(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def english_cleaners(text):\n",
        "    '''Pipeline for English text, including number and abbreviation expansion.'''\n",
        "    text = convert_to_ascii(text)\n",
        "    text = lowercase(text)\n",
        "    text = expand_numbers(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = collapse_whitespace(text)\n",
        "    return text\n",
        "\n",
        "def korean_cleaners(text):\n",
        "    text=ko_tokenize(text, as_id=False)\n",
        "    return text\n",
        "\n",
        "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
        "\n",
        "'''\n",
        "Defines the set of symbols used in text input to the model.\n",
        "\n",
        "The default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n",
        "'''\n",
        "kor_symbols=KOR_SYMBOLS\n",
        "symbols=kor_symbols"
      ],
      "metadata": {
        "id": "MfRm8xss3Z_v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#트랜스포머 파일(트랜스포머 구현인듯?)"
      ],
      "metadata": {
        "id": "VLthuuzuN8y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "UNK = 1\n",
        "BOS = 2\n",
        "EOS = 3\n",
        "\n",
        "PAD_WORD = '<blank>'\n",
        "UNK_WORD = '<unk>'\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'"
      ],
      "metadata": {
        "id": "H1ABSOOGN6vX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFTBlock(torch.nn.Module):\n",
        "    \"\"\"FFT Block\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 d_inner,\n",
        "                 n_head,\n",
        "                 d_k,\n",
        "                 d_v,\n",
        "                 dropout=0.1):\n",
        "        super(FFTBlock, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(\n",
        "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(\n",
        "            d_model, d_inner, dropout=dropout)\n",
        "    def forward(self, enc_input, mask=None, slf_attn_mask=None):\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "        enc_output = enc_output.masked_fill(mask.unsqueeze(-1), 0)\n",
        "\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        enc_output = enc_output.masked_fill(mask.unsqueeze(-1), 0)\n",
        "\n",
        "        return enc_output, enc_slf_attn\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=1,\n",
        "                 stride=1,\n",
        "                 padding=None,\n",
        "                 dilation=1,\n",
        "                 bias=True,\n",
        "                 w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels,\n",
        "                                    out_channels,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=stride,\n",
        "                                    padding=padding,\n",
        "                                    dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    PostNet: Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_mel_channels=80,\n",
        "                 postnet_embedding_dim=512,\n",
        "                 postnet_kernel_size=5,\n",
        "                 postnet_n_convolutions=5):\n",
        "\n",
        "        super(PostNet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(n_mel_channels,\n",
        "                         postnet_embedding_dim,\n",
        "                         kernel_size=postnet_kernel_size,\n",
        "                         stride=1,\n",
        "                         padding=int((postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1,\n",
        "                         w_init_gain='tanh'),\n",
        "\n",
        "                nn.BatchNorm1d(postnet_embedding_dim))\n",
        "        )\n",
        "\n",
        "        for i in range(1, postnet_n_convolutions - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(postnet_embedding_dim,\n",
        "                             postnet_embedding_dim,\n",
        "                             kernel_size=postnet_kernel_size,\n",
        "                             stride=1,\n",
        "                             padding=int((postnet_kernel_size - 1) / 2),\n",
        "                             dilation=1,\n",
        "                             w_init_gain='tanh'),\n",
        "\n",
        "                    nn.BatchNorm1d(postnet_embedding_dim))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(postnet_embedding_dim,\n",
        "                         n_mel_channels,\n",
        "                         kernel_size=postnet_kernel_size,\n",
        "                         stride=1,\n",
        "                         padding=int((postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1,\n",
        "                         w_init_gain='linear'),\n",
        "\n",
        "                nn.BatchNorm1d(n_mel_channels))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().transpose(1, 2)\n",
        "\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = F.dropout(torch.tanh(\n",
        "                self.convolutions[i](x)), 0.5, self.training)\n",
        "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "        x = x.contiguous().transpose(1, 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "H0RQ3xYIN8gf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n",
        "    ''' Sinusoid position encoding table '''\n",
        "\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i)\n",
        "                               for pos_i in range(n_position)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "    if padding_idx is not None:\n",
        "        # zero vector for padding dimension\n",
        "        sinusoid_table[padding_idx] = 0.\n",
        "\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    ''' Encoder '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_src_vocab=len(symbols)+1,\n",
        "                 len_max_seq=hp.max_seq_len,\n",
        "                 d_word_vec=hp.encoder_hidden,\n",
        "                 n_layers=hp.encoder_layer,\n",
        "                 n_head=hp.encoder_head,\n",
        "                 d_k=hp.encoder_hidden // hp.encoder_head,\n",
        "                 d_v=hp.encoder_hidden // hp.encoder_head,\n",
        "                 d_model=hp.encoder_hidden,\n",
        "                 d_inner=hp.fft_conv1d_filter_size,\n",
        "                 dropout=hp.encoder_dropout):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        n_position = len_max_seq + 1\n",
        "\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=PAD)\n",
        "        self.position_enc = nn.Parameter(\n",
        "            get_sinusoid_encoding_table(n_position, d_word_vec).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
        "            d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, mask, return_attns=False):\n",
        "\n",
        "        enc_slf_attn_list = []\n",
        "        batch_size, max_len = src_seq.shape[0], src_seq.shape[1]\n",
        "\n",
        "        # -- Prepare masks\n",
        "        slf_attn_mask = mask.unsqueeze(1).expand(-1, max_len, -1)\n",
        "\n",
        "        # -- Forward\n",
        "        if not self.training and src_seq.shape[1] > hp.max_seq_len:\n",
        "            enc_output = self.src_word_emb(src_seq) + get_sinusoid_encoding_table(src_seq.shape[1], hp.encoder_hidden)[:src_seq.shape[1], :].unsqueeze(0).expand(batch_size, -1, -1).to(src_seq.device)\n",
        "        else:\n",
        "            enc_output = self.src_word_emb(src_seq) + self.position_enc[:, :max_len, :].expand(batch_size, -1, -1)\n",
        "\n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(\n",
        "                enc_output,\n",
        "                mask=mask,\n",
        "                slf_attn_mask=slf_attn_mask)\n",
        "            if return_attns:\n",
        "                enc_slf_attn_list += [enc_slf_attn]\n",
        "\n",
        "        return enc_output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Decoder \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 len_max_seq=hp.max_seq_len,\n",
        "                 d_word_vec=hp.encoder_hidden,\n",
        "                 n_layers=hp.decoder_layer,\n",
        "                 n_head=hp.decoder_head,\n",
        "                 d_k=hp.decoder_hidden // hp.decoder_head,\n",
        "                 d_v=hp.decoder_hidden // hp.decoder_head,\n",
        "                 d_model=hp.decoder_hidden,\n",
        "                 d_inner=hp.fft_conv1d_filter_size,\n",
        "                 dropout=hp.decoder_dropout):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        n_position = len_max_seq + 1\n",
        "\n",
        "        self.position_enc = nn.Parameter(\n",
        "            get_sinusoid_encoding_table(n_position, d_word_vec).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
        "            d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_seq, mask, return_attns=False):\n",
        "\n",
        "        dec_slf_attn_list = []\n",
        "        batch_size, max_len = enc_seq.shape[0], enc_seq.shape[1]\n",
        "\n",
        "        # -- Prepare masks\n",
        "        slf_attn_mask = mask.unsqueeze(1).expand(-1, max_len, -1)\n",
        "\n",
        "        # -- Forward\n",
        "        if not self.training and enc_seq.shape[1] > hp.max_seq_len:\n",
        "            dec_output = enc_seq + get_sinusoid_encoding_table(enc_seq.shape[1], hp.decoder_hidden)[:enc_seq.shape[1], :].unsqueeze(0).expand(batch_size, -1, -1).to(enc_seq.device)\n",
        "        else:\n",
        "            dec_output = enc_seq + self.position_enc[:, :max_len, :].expand(batch_size, -1, -1)\n",
        "\n",
        "        for dec_layer in self.layer_stack:\n",
        "            dec_output, dec_slf_attn = dec_layer(\n",
        "                dec_output,\n",
        "                mask=mask,\n",
        "                slf_attn_mask=slf_attn_mask)\n",
        "            if return_attns:\n",
        "                dec_slf_attn_list += [dec_slf_attn]\n",
        "\n",
        "        return dec_output"
      ],
      "metadata": {
        "id": "dfzuNE-9OCHC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        attn = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn = attn / self.temperature\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask, -np.inf)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "        output = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "zmEXhCr_OEJd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(\n",
        "            temperature=np.power(d_k, 0.5))\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "\n",
        "        sz_b, len_q, _ = q.size()\n",
        "        sz_b, len_k, _ = k.size()\n",
        "        sz_b, len_v, _ = v.size()\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "        q = q.permute(2, 0, 1, 3).contiguous().view(-1,\n",
        "                                                    len_q, d_k)  # (n*b) x lq x dk\n",
        "        k = k.permute(2, 0, 1, 3).contiguous().view(-1,\n",
        "                                                    len_k, d_k)  # (n*b) x lk x dk\n",
        "        v = v.permute(2, 0, 1, 3).contiguous().view(-1,\n",
        "                                                    len_v, d_v)  # (n*b) x lv x dv\n",
        "\n",
        "        mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
        "        output, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.view(n_head, sz_b, len_q, d_v)\n",
        "        output = output.permute(1, 2, 0, 3).contiguous().view(\n",
        "            sz_b, len_q, -1)  # b x lq x (n*dv)\n",
        "\n",
        "        output = self.dropout(self.fc(output))\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use Conv1D\n",
        "        # position-wise\n",
        "        self.w_1 = nn.Conv1d(\n",
        "            d_in, d_hid, kernel_size=hp.fft_conv1d_kernel_size[0], padding=(hp.fft_conv1d_kernel_size[0]-1)//2)\n",
        "        # position-wise\n",
        "        self.w_2 = nn.Conv1d(\n",
        "            d_hid, d_in, kernel_size=hp.fft_conv1d_kernel_size[1], padding=(hp.fft_conv1d_kernel_size[1]-1)//2)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_in)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = x.transpose(1, 2)\n",
        "        output = self.w_2(F.relu(self.w_1(output)))\n",
        "        output = output.transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "6dEc0xcWOGm_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py\n"
      ],
      "metadata": {
        "id": "mItFqeC1OV3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_alignment(tier): #파일 분석\n",
        "    sil_phones = ['sil', 'sp', 'spn']\n",
        "\n",
        "    phones = []\n",
        "    durations = []\n",
        "    start_time = 0\n",
        "    end_time = 0\n",
        "    end_idx = 0\n",
        "    for t in tier._objects:\n",
        "        s, e, p = t.start_time, t.end_time, t.text\n",
        "\n",
        "        # Trimming leading silences\n",
        "        if phones == []:\n",
        "            if p in sil_phones:\n",
        "                continue\n",
        "            else:\n",
        "                start_time = s\n",
        "        if p not in sil_phones:\n",
        "            phones.append(p)\n",
        "            end_time = e\n",
        "            end_idx = len(phones)\n",
        "        else:\n",
        "            phones.append(p)\n",
        "        durations.append(int(e*hp.sampling_rate/hp.hop_length)-int(s*hp.sampling_rate/hp.hop_length))\n",
        "\n",
        "    # Trimming tailing silences\n",
        "    phones = phones[:end_idx]\n",
        "    durations = durations[:end_idx]\n",
        "\n",
        "    return phones, np.array(durations), start_time, end_time\n",
        "\n",
        "def process_meta(meta_path): #메타데이터(대본) 추출 및 분할\n",
        "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = []\n",
        "        name = []\n",
        "        for line in f.readlines():\n",
        "            n, t = line.strip('\\n').split('|')\n",
        "            name.append(n)\n",
        "            text.append(t)\n",
        "        return name, text\n",
        "\n",
        "def get_param_num(model):\n",
        "    num_param = sum(param.numel() for param in model.parameters())\n",
        "    return num_param\n",
        "\n",
        "def plot_data(data, titles=None, filename=None):\n",
        "    fig, axes = plt.subplots(len(data), 1, squeeze=False)\n",
        "    if titles is None:\n",
        "        titles = [None for i in range(len(data))]\n",
        "\n",
        "    def add_axis(fig, old_ax, offset=0):\n",
        "        ax = fig.add_axes(old_ax.get_position(), anchor='W')\n",
        "        ax.set_facecolor(\"None\")\n",
        "        return ax\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        spectrogram, pitch, energy = data[i]\n",
        "        axes[i][0].imshow(spectrogram, origin='lower')\n",
        "        axes[i][0].set_aspect(2.5, adjustable='box')\n",
        "        axes[i][0].set_ylim(0, hp.n_mel_channels)\n",
        "        axes[i][0].set_title(titles[i], fontsize='medium')\n",
        "        axes[i][0].tick_params(labelsize='x-small', left=False, labelleft=False)\n",
        "        axes[i][0].set_anchor('W')\n",
        "\n",
        "        ax1 = add_axis(fig, axes[i][0])\n",
        "        ax1.plot(pitch, color='tomato')\n",
        "        ax1.set_xlim(0, spectrogram.shape[1])\n",
        "        ax1.set_ylim(0, hp.f0_max)\n",
        "        ax1.set_ylabel('F0', color='tomato')\n",
        "        ax1.tick_params(labelsize='x-small', colors='tomato', bottom=False, labelbottom=False)\n",
        "\n",
        "        ax2 = add_axis(fig, axes[i][0], 1.2)\n",
        "        ax2.plot(energy, color='darkviolet')\n",
        "        ax2.set_xlim(0, spectrogram.shape[1])\n",
        "        ax2.set_ylim(hp.energy_min, hp.energy_max)\n",
        "        ax2.set_ylabel('Energy', color='darkviolet')\n",
        "        ax2.yaxis.set_label_position('right')\n",
        "        ax2.tick_params(labelsize='x-small', colors='darkviolet', bottom=False, labelbottom=False, left=False, labelleft=False, right=True, labelright=True)\n",
        "\n",
        "    plt.savefig(filename, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def get_mask_from_lengths(lengths, max_len=None): #마스크 길이 가져옴\n",
        "    batch_size = lengths.shape[0]\n",
        "    if max_len is None:\n",
        "        max_len = torch.max(lengths).item()\n",
        "\n",
        "    ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1).to(device)\n",
        "    mask = (ids >= lengths.unsqueeze(1).expand(-1, max_len))\n",
        "\n",
        "    return mask\n",
        "\n",
        "\"\"\"\n",
        "원본은 vocgan 사용함, 이 부분을 다른 모델로 바꿔서 들고오면 될듯?\n",
        "\"\"\"\n",
        "def get_vocgan(ckpt_path, n_mel_channels=hp.n_mel_channels, generator_ratio = [4, 4, 2, 2, 2, 2], n_residual_layers=4, mult=256, out_channels=1): #vocgan 사용하는 경우\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    model = Generator(n_mel_channels, n_residual_layers,\n",
        "                        ratios=generator_ratio, mult=mult,\n",
        "                        out_band=out_channels)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_g'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def vocgan_infer(mel, vocoder, path):\n",
        "    model = vocoder\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if len(mel.shape) == 2:\n",
        "            mel = mel.unsqueeze(0)\n",
        "\n",
        "        audio = model.infer(mel).squeeze()\n",
        "        audio = hp.max_wav_value * audio[:-(hp.hop_length*10)]\n",
        "        audio = audio.clamp(min=-hp.max_wav_value, max=hp.max_wav_value-1)\n",
        "        audio = audio.short().cpu().detach().numpy()\n",
        "\n",
        "        wavfile.write(path, hp.sampling_rate, audio)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# 1차원 패딩 함수\n",
        "def pad_1D(inputs, PAD=0):\n",
        "\n",
        "    def pad_data(x, length, PAD):\n",
        "        x_padded = np.pad(x, (0, length - x.shape[0]),\n",
        "                          mode='constant',\n",
        "                          constant_values=PAD)\n",
        "        return x_padded\n",
        "\n",
        "    max_len = max((len(x) for x in inputs))\n",
        "    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
        "\n",
        "    return padded\n",
        "\n",
        "#2차원 패딩\n",
        "def pad_2D(inputs, maxlen=None):\n",
        "\n",
        "    def pad(x, max_len):\n",
        "        PAD = 0\n",
        "        if np.shape(x)[0] > max_len:\n",
        "            raise ValueError(\"not max_len\")\n",
        "\n",
        "        s = np.shape(x)[1]\n",
        "        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n",
        "                          mode='constant',\n",
        "                          constant_values=PAD)\n",
        "        return x_padded[:, :s]\n",
        "\n",
        "    if maxlen:\n",
        "        output = np.stack([pad(x, maxlen) for x in inputs])\n",
        "    else:\n",
        "        max_len = max(np.shape(x)[0] for x in inputs)\n",
        "        output = np.stack([pad(x, max_len) for x in inputs])\n",
        "\n",
        "    return output\n",
        "\n",
        "#입력값에 따라서 패딩을 다르게 걸기?\n",
        "def pad(input_ele, mel_max_length=None):\n",
        "    if mel_max_length:\n",
        "        max_len = mel_max_length\n",
        "    else:\n",
        "        max_len = max([input_ele[i].size(0)for i in range(len(input_ele))])\n",
        "\n",
        "    out_list = list()\n",
        "    for i, batch in enumerate(input_ele):\n",
        "        if len(batch.shape) == 1:\n",
        "            one_batch_padded = F.pad(\n",
        "                batch, (0, max_len-batch.size(0)), \"constant\", 0.0)\n",
        "        elif len(batch.shape) == 2:\n",
        "            one_batch_padded = F.pad(\n",
        "                batch, (0, 0, 0, max_len-batch.size(0)), \"constant\", 0.0)\n",
        "        out_list.append(one_batch_padded)\n",
        "    out_padded = torch.stack(out_list)\n",
        "    return out_padded\n",
        "\n",
        "# from dathudeptrai's FastSpeech2 implementation\n",
        "def standard_norm(x, mean, std, is_mel=False):\n",
        "\n",
        "    if not is_mel:\n",
        "        x = remove_outlier(x)\n",
        "\n",
        "    zero_idxs = np.where(x == 0.0)[0]\n",
        "    x = (x - mean) / std\n",
        "    x[zero_idxs] = 0.0\n",
        "    return x\n",
        "\n",
        "def standard_norm_torch(x, mean, std):\n",
        "\n",
        "    zero_idxs = torch.where(x == 0.0)[0]\n",
        "    x = (x - mean) / std\n",
        "    x[zero_idxs] = 0.0\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def de_norm(x, mean, std):\n",
        "    zero_idxs = torch.where(x == 0.0)[0]\n",
        "    x = mean + std * x\n",
        "    x[zero_idxs] = 0.0\n",
        "    return x\n",
        "\n",
        "\n",
        "def _is_outlier(x, p25, p75):\n",
        "    \"\"\"Check if value is an outlier.\"\"\"\n",
        "    lower = p25 - 1.5 * (p75 - p25)\n",
        "    upper = p75 + 1.5 * (p75 - p25)\n",
        "\n",
        "    return np.logical_or(x <= lower, x >= upper)\n",
        "\n",
        "\n",
        "def remove_outlier(x):\n",
        "    \"\"\"Remove outlier from x.\"\"\"\n",
        "    p25 = np.percentile(x, 25)\n",
        "    p75 = np.percentile(x, 75)\n",
        "\n",
        "    indices_of_outliers = []\n",
        "    for ind, value in enumerate(x):\n",
        "        if _is_outlier(value, p25, p75):\n",
        "            indices_of_outliers.append(ind)\n",
        "\n",
        "    x[indices_of_outliers] = 0.0\n",
        "\n",
        "    # replace by mean f0.\n",
        "    x[indices_of_outliers] = np.max(x)\n",
        "    return x\n",
        "\n",
        "def average_by_duration(x, durs):\n",
        "    mel_len = durs.sum()\n",
        "    durs_cum = np.cumsum(np.pad(durs, (1, 0)))\n",
        "\n",
        "    # calculate charactor f0/energy\n",
        "    x_char = np.zeros((durs.shape[0],), dtype=np.float32)\n",
        "    for idx, start, end in zip(range(mel_len), durs_cum[:-1], durs_cum[1:]):\n",
        "        values = x[start:end][np.where(x[start:end] != 0.0)[0]]\n",
        "        x_char[idx] = np.mean(values) if len(values) > 0 else 0.0  # np.mean([]) = nan.\n",
        "\n",
        "    return x_char.astype(np.float32)"
      ],
      "metadata": {
        "id": "9cENbHatzVBi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#audio 파일"
      ],
      "metadata": {
        "id": "hDF00gGQNbxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class STFT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, filter_length, hop_length, win_length,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = librosa_util.pad_center(fft_window, size=filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float()\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float())\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    def transform(self, input_data): #데이터 변환\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data.cuda(),\n",
        "            Variable(self.forward_basis, requires_grad=False).cuda(),\n",
        "            stride=self.hop_length,\n",
        "            padding=0).cpu()\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase): #데이터 역변환\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :,\n",
        "                              approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:,\n",
        "                                              :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data): #재구성인듯?\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction\n"
      ],
      "metadata": {
        "id": "e44NPVHe3w0z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TacotronSTFT(torch.nn.Module): #stft\n",
        "    def __init__(self, filter_length, hop_length, win_length,\n",
        "                 n_mel_channels, sampling_rate, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sr=sampling_rate, n_fft=filter_length, n_mels=n_mel_channels, fmin=mel_fmin, fmax=mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        energy = torch.norm(magnitudes, dim=1)\n",
        "\n",
        "        return mel_output, energy\n",
        "\n"
      ],
      "metadata": {
        "id": "7C9s0R8O307c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_sumsquare(window, n_frames, hop_length=hp.hop_length, win_length=hp.win_length,\n",
        "                     n_fft=hp.filter_length, dtype=np.float32, norm=None):\n",
        "    \"\"\"\n",
        "    # from librosa 0.6\n",
        "    Compute the sum-square envelope of a window function at a given hop length.\n",
        "\n",
        "    This is used to estimate modulation effects induced by windowing\n",
        "    observations in short-time fourier transforms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : string, tuple, number, callable, or list-like\n",
        "        Window specification, as in `get_window`\n",
        "\n",
        "    n_frames : int > 0\n",
        "        The number of analysis frames\n",
        "\n",
        "    hop_length : int > 0\n",
        "        The number of samples to advance between frames\n",
        "\n",
        "    win_length : [optional]\n",
        "        The length of the window function.  By default, this matches `n_fft`.\n",
        "\n",
        "    n_fft : int > 0\n",
        "        The length of each analysis frame.\n",
        "\n",
        "    dtype : np.dtype\n",
        "        The data type of the output\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
        "        The sum-squared envelope of the window function\n",
        "    \"\"\"\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)\n",
        "          ] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    magnitudes: spectrogram magnitudes\n",
        "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
        "    \"\"\"\n",
        "\n",
        "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
        "    angles = angles.astype(np.float32)\n",
        "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
        "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        _, angles = stft_fn.transform(signal)\n",
        "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "    return signal\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6jGlaLhcNCtn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_stft = TacotronSTFT( #주어진 데이터에 대해 stft 실행\n",
        "    hp.filter_length, hp.hop_length, hp.win_length,\n",
        "    hp.n_mel_channels, hp.sampling_rate, hp.mel_fmin,\n",
        "    hp.mel_fmax)"
      ],
      "metadata": {
        "id": "OpIi8e1Tz6Er"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "\n",
        "def get_mel(filename):\n",
        "    audio, sampling_rate = load_wav_to_torch(filename)\n",
        "    if sampling_rate != _stft.sampling_rate:\n",
        "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, _stft.sampling_rate))\n",
        "    audio_norm = audio / hp.max_wav_value\n",
        "    audio_norm = audio_norm.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec, energy = _stft.mel_spectrogram(audio_norm)\n",
        "    melspec = torch.squeeze(melspec, 0)\n",
        "    energy = torch.squeeze(energy, 0)\n",
        "    # melspec = torch.from_numpy(_normalize(melspec.numpy()))\n",
        "\n",
        "    return melspec, energy\n",
        "\n",
        "\n",
        "def get_mel_from_wav(audio):\n",
        "    sampling_rate = hp.sampling_rate\n",
        "    if sampling_rate != _stft.sampling_rate:\n",
        "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, _stft.sampling_rate))\n",
        "    audio_norm = audio / hp.max_wav_value\n",
        "    audio_norm = audio_norm.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec, energy = _stft.mel_spectrogram(audio_norm)\n",
        "    melspec = torch.squeeze(melspec, 0)\n",
        "    energy = torch.squeeze(energy, 0)\n",
        "\n",
        "    return melspec, energy\n",
        "\n",
        "\n",
        "def inv_mel_spec(mel, out_filename, griffin_iters=60):\n",
        "    mel = torch.stack([mel])\n",
        "    # mel = torch.stack([torch.from_numpy(_denormalize(mel.numpy()))])\n",
        "    mel_decompress = _stft.spectral_de_normalize(mel)\n",
        "    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n",
        "    spec_from_mel_scaling = 1000\n",
        "    spec_from_mel = torch.mm(mel_decompress[0], _stft.mel_basis)\n",
        "    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n",
        "    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n",
        "\n",
        "    audio = griffin_lim(torch.autograd.Variable(\n",
        "        spec_from_mel[:, :, :-1]), _stft.stft_fn, griffin_iters)\n",
        "\n",
        "    audio = audio.squeeze()\n",
        "    audio = audio.cpu().numpy()\n",
        "    audio_path = out_filename\n",
        "    write(audio_path, hp.sampling_rate, audio)"
      ],
      "metadata": {
        "id": "oKol-VMS37HM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data 파일"
      ],
      "metadata": {
        "id": "onxf_FJPNYyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_align(in_dir, meta): #데이터 처리용 라이브러리\n",
        "    with open(os.path.join(in_dir, meta), encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('|')\n",
        "            basename, text = parts[0], parts[1]\n",
        "            basename=basename.replace('.wav','.txt')\n",
        "\n",
        "            with open(os.path.join(in_dir,'wavs',basename),'w') as f1:\n",
        "                f1.write(text)\n",
        "\n",
        "def build_from_path(in_dir, out_dir, meta): #데이터 정규화 및 f0, 에너지, 멜 생성\n",
        "    train, val = list(), list()\n",
        "\n",
        "    scalers = [StandardScaler(copy=False) for _ in range(3)]\t# scalers for mel, f0, energy\n",
        "\n",
        "    n_frames = 0\n",
        "\n",
        "    with open(os.path.join(in_dir, meta)) as f:\n",
        "        for index, line in enumerate(f):\n",
        "\n",
        "            parts = line.strip().split('|')\n",
        "            basename, text = parts[0], parts[1]\n",
        "\n",
        "            ret = process_utterance(in_dir, out_dir, basename, scalers)\n",
        "\n",
        "            if ret is None:\n",
        "                continue\n",
        "            else:\n",
        "                info, n = ret\n",
        "\n",
        "            if basename[0] == '1': #1번 파일만 검증, 나머지는 학습으로 사용, 여기서 타겟 지정해서 하면 될듯?\n",
        "                val.append(info)\n",
        "            else:\n",
        "                train.append(info)\n",
        "\n",
        "            if index % 100 == 0:\n",
        "                print(\"Done %d\" % index)\n",
        "\n",
        "            n_frames += n\n",
        "\n",
        "    param_list = [np.array([scaler.mean_, scaler.scale_]) for scaler in scalers]\n",
        "    param_name_list = ['mel_stat.npy', 'f0_stat.npy', 'energy_stat.npy']\n",
        "    [np.save(os.path.join(out_dir, param_name), param_list[idx]) for idx, param_name in enumerate(param_name_list)]\n",
        "\n",
        "    return [r for r in train if r is not None], [r for r in val if r is not None]\n",
        "\n",
        "\n",
        "def process_utterance(in_dir, out_dir, basename, scalers): #wav별 실행\n",
        "    wav_bak_basename=basename.replace('.wav','')\n",
        "    basename = wav_bak_basename\n",
        "    wav_bak_path = os.path.join(in_dir, \"wavs_bak\", \"{}.wav\".format(wav_bak_basename))\n",
        "    wav_path = os.path.join(in_dir, 'wavs', '{}.wav'.format(basename))\n",
        "    textgrid_name=hp.textgrid_name.replace(\".zip\",\"\")\n",
        "\n",
        "    # Convert kss data into PCM encoded wavs\n",
        "    if not os.path.isfile(wav_path):\n",
        "        os.system(\"ffmpeg -i {} -ac 1 -ar 22050 {}\".format(wav_bak_path, wav_path))\n",
        "    tg_path = os.path.join(out_dir, textgrid_name, '{}.TextGrid'.format(basename))\n",
        "\n",
        "    # Get alignments\n",
        "    textgrid = tgt.io.read_textgrid(tg_path)\n",
        "    phone, duration, start, end = get_alignment(textgrid.get_tier_by_name('phones'))\n",
        "    print(\"phone : {}\\nduration: {}\\nstart : {}\\n,end : {}\\n\".format(phone,duration,start,end))\n",
        "\n",
        "    text = '{'+ '}{'.join(phone) + '}' # '{A}{B}{$}{C}', $ represents silent phones\n",
        "    text = text.replace('{$}', ' ')    # '{A}{B} {C}'\n",
        "    text = text.replace('}{', ' ')     # '{A B} {C}'\n",
        "\n",
        "\n",
        "    if start >= end:\n",
        "        return None\n",
        "\n",
        "    # Read and trim wav files\n",
        "    _, wav = read(wav_path)\n",
        "    wav = wav[int(hp.sampling_rate*start):int(hp.sampling_rate*end)].astype(np.float32) #샘플레이트\n",
        "\n",
        "    if wav.ndim == 2:\n",
        "        wav = wav.mean(axis=1)\n",
        "\n",
        "    # Compute fundamental frequency\n",
        "    f0, _ = pw.dio(wav.astype(np.float64), hp.sampling_rate, frame_period=hp.hop_length/hp.sampling_rate*1000) #f0 추측\n",
        "    f0 = f0[:sum(duration)]\n",
        "\n",
        "    # Compute mel-scale spectrogram and energy\n",
        "    mel_spectrogram, energy = get_mel_from_wav(torch.FloatTensor(wav)) #wav에서 멜 추출, 에너지 추출\n",
        "    mel_spectrogram = mel_spectrogram.numpy().astype(np.float32)[:, :sum(duration)]\n",
        "    energy = energy.numpy().astype(np.float32)[:sum(duration)]\n",
        "\n",
        "    f0, energy = remove_outlier(f0), remove_outlier(energy)\n",
        "    f0, energy = average_by_duration(f0, duration), average_by_duration(energy, duration)\n",
        "\n",
        "    if mel_spectrogram.shape[1] >= hp.max_seq_len:\n",
        "        return None\n",
        "\n",
        "    # Save alignment\n",
        "    ali_filename = '{}-ali-{}.npy'.format(hp.dataset, basename)\n",
        "    np.save(os.path.join(out_dir, 'my_voice_dataset_aligned', ali_filename), duration, allow_pickle=False)\n",
        "\n",
        "    # Save fundamental prequency\n",
        "    f0_filename = '{}-f0-{}.npy'.format(hp.dataset, basename)\n",
        "    np.save(os.path.join(out_dir, 'f0', f0_filename), f0, allow_pickle=False)\n",
        "\n",
        "    # Save energy\n",
        "    energy_filename = '{}-energy-{}.npy'.format(hp.dataset, basename)\n",
        "    np.save(os.path.join(out_dir, 'energy', energy_filename), energy, allow_pickle=False)\n",
        "\n",
        "    # Save spectrogram\n",
        "    mel_filename = '{}-mel-{}.npy'.format(hp.dataset, basename)\n",
        "    np.save(os.path.join(out_dir, 'mel', mel_filename), mel_spectrogram.T, allow_pickle=False)\n",
        "\n",
        "    mel_scaler, f0_scaler, energy_scaler = scalers\n",
        "\n",
        "    mel_scaler.partial_fit(mel_spectrogram.T)\n",
        "    f0_scaler.partial_fit(f0[f0!=0].reshape(-1, 1))\n",
        "    energy_scaler.partial_fit(energy[energy != 0].reshape(-1, 1))\n",
        "\n",
        "    return '|'.join([basename, text]), mel_spectrogram.shape[1]"
      ],
      "metadata": {
        "id": "VpKNpP-YNPbY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prepare_align.py"
      ],
      "metadata": {
        "id": "r1kZRQerPrlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def main(): #데이터 전처리, 내 목소리 생기면 그때 돌리기\n",
        "#     in_dir = hp.data_path\n",
        "#     if hp.dataset == \"my_voice_dataset\":\n",
        "#         prepare_align(in_dir, hp.meta_name)\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "12XJQeDnPvt9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preprocess.py"
      ],
      "metadata": {
        "id": "NDMcMnM2PE0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 전처리, 메타데이터 작성\n",
        "def write_metadata(train, val, out_dir): #학습 데이터셋, 검증 데이터셋용 메타데이터 생성\n",
        "    with open(os.path.join(out_dir, 'my_voice_text_train_filelist.txt'), 'w', encoding='utf-8') as f:\n",
        "        for m in train:\n",
        "            f.write(m + '\\n')\n",
        "    with open(os.path.join(out_dir, 'my_voice_text_val_filelist.txt'), 'w', encoding='utf-8') as f:\n",
        "        for m in val:\n",
        "            f.write(m + '\\n')\n"
      ],
      "metadata": {
        "id": "tajhE3sw1792"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def main():\n",
        "#     in_dir = hp.data_path\n",
        "#     out_dir = hp.preprocessed_path\n",
        "#     meta = hp.meta_name\n",
        "#     textgrid_name = hp.textgrid_name\n",
        "\n",
        "#     mel_out_dir = os.path.join(out_dir, \"mel\")\n",
        "#     if not os.path.exists(mel_out_dir):\n",
        "#         os.makedirs(mel_out_dir, exist_ok=True)\n",
        "\n",
        "#     ali_out_dir = os.path.join(out_dir, \"my_voice_dataset_aligned\")\n",
        "#     if not os.path.exists(ali_out_dir):\n",
        "#         os.makedirs(ali_out_dir, exist_ok=True)\n",
        "\n",
        "#     f0_out_dir = os.path.join(out_dir, \"f0\")\n",
        "#     if not os.path.exists(f0_out_dir):\n",
        "#         os.makedirs(f0_out_dir, exist_ok=True)\n",
        "\n",
        "#     energy_out_dir = os.path.join(out_dir, \"energy\")\n",
        "#     if not os.path.exists(energy_out_dir):\n",
        "#         os.makedirs(energy_out_dir, exist_ok=True)\n",
        "\n",
        "#     if os.path.isfile(textgrid_name):\n",
        "#         os.system('mv ./{} {}'.format(textgrid_name, out_dir))\n",
        "\n",
        "#     if not os.path.exists(os.path.join(out_dir, textgrid_name.replace(\".zip\", \"\"))):\n",
        "#         os.system('unzip {} -d {}'.format(os.path.join(out_dir, textgrid_name), out_dir))\n",
        "\n",
        "\n",
        "#     if \"kss\" in hp.dataset:\n",
        "#         # kss version 1.3\n",
        "#         if \"v.1.3\" in meta:\n",
        "#             if not os.path.exists(os.path.join(in_dir, \"wavs_bak\")):\n",
        "#                 os.system(\"mv {} {}\".format(os.path.join(in_dir, \"wavs\"), os.path.join(in_dir, \"wavs_bak\")))\n",
        "#                 os.makedirs(os.path.join(in_dir, \"wavs\"))\n",
        "\n",
        "#         # kss version 1.4\n",
        "#         if \"v.1.4\" in meta:\n",
        "#             if not os.path.exists(os.path.join(in_dir, \"wavs_bak\")):\n",
        "#                 os.makedirs(os.path.join(in_dir, \"wavs\"))\n",
        "#                 os.system(\"mv {} {}\".format(os.path.join(in_dir, \"../\", meta), os.path.join(in_dir)))\n",
        "#                 for i in range(1, 5) : os.system(\"mv {} {}\".format(os.path.join(in_dir, str(i)), os.path.join(in_dir, \"wavs\")))\n",
        "#                 os.system(\"mv {} {}\".format(os.path.join(in_dir, \"wavs\"), os.path.join(in_dir, \"wavs_bak\")))\n",
        "#                 os.makedirs(os.path.join(in_dir, \"wavs\"))\n",
        "\n",
        "#     if \"my_voice\" in hp.dataset:\n",
        "#         if not os.path.exists(os.path.join(in_dir, \"wavs_bak\")):\n",
        "#             #os.makedirs(os.path.join(in_dir, \"wavs\"))\n",
        "#             os.system(\"mv {} {}\".format(os.path.join(in_dir, \"../\", meta), os.path.join(in_dir)))\n",
        "#             for i in range(1, 5) : os.system(\"mv {} {}\".format(os.path.join(in_dir, str(i)), os.path.join(in_dir, \"wavs\")))\n",
        "#             os.system(\"mv {} {}\".format(os.path.join(in_dir, \"wavs\"), os.path.join(in_dir, \"wavs_bak\")))\n",
        "#             #os.makedirs(os.path.join(in_dir, \"wavs\"))\n",
        "\n",
        "\n",
        "#     train, val = build_from_path(in_dir, out_dir, meta)\n",
        "\n",
        "#     write_metadata(train, val, out_dir) #여기서 train.txt, val.txt 생성됨\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def create_directories(base_dir, subdirs):\n",
        "    \"\"\"디렉토리를 생성하는 함수\"\"\"\n",
        "    for subdir in subdirs:\n",
        "        os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n",
        "\n",
        "def move_file(src, dest):\n",
        "    \"\"\"파일을 이동하는 함수\"\"\"\n",
        "    if os.path.exists(src):\n",
        "        shutil.move(src, dest)\n",
        "\n",
        "def unzip_file(zip_path, extract_to):\n",
        "    \"\"\"파일 압축 해제 함수\"\"\"\n",
        "    if os.path.exists(zip_path):\n",
        "        shutil.unpack_archive(zip_path, extract_to)\n",
        "\n",
        "def main():\n",
        "    in_dir = hp.data_path\n",
        "    out_dir = hp.preprocessed_path\n",
        "    meta = hp.meta_name\n",
        "    textgrid_name = hp.textgrid_name\n",
        "\n",
        "    # 출력 디렉토리 생성\n",
        "    create_directories(out_dir, [\"mel\", \"my_voice_dataset_aligned\", \"f0\", \"energy\"])\n",
        "\n",
        "    # TextGrid 파일 처리\n",
        "    move_file(textgrid_name, out_dir)\n",
        "    unzip_file(os.path.join(out_dir, textgrid_name), out_dir)\n",
        "\n",
        "    # wavs 디렉토리 확인 및 처리\n",
        "    wavs_dir = os.path.join(in_dir, \"wavs\")\n",
        "    if not os.path.exists(wavs_dir):\n",
        "        print(f\"Error: {wavs_dir} 디렉토리가 존재하지 않습니다.\")\n",
        "        return\n",
        "\n",
        "    # wavs 하위의 파일을 그대로 사용 (화자별 분리 불필요)\n",
        "    wav_files = [f for f in os.listdir(wavs_dir) if f.endswith(\".wav\")]\n",
        "    lab_files = [f for f in os.listdir(wavs_dir) if f.endswith(\".lab\")]\n",
        "\n",
        "    if not wav_files:\n",
        "        print(f\"Error: {wavs_dir}에 .wav 파일이 없습니다.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(wav_files)} wav files and {len(lab_files)} lab files in {wavs_dir}.\")\n",
        "\n",
        "    # 학습 데이터 생성\n",
        "    train, val = build_from_path(in_dir, out_dir, meta)\n",
        "\n",
        "    # 메타데이터 저장\n",
        "    write_metadata(train, val, out_dir)\n"
      ],
      "metadata": {
        "id": "vfmQX8LkPIDO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": #학습 셋, 검증셋 생성, 내 목소리 생기면 그때 돌리기\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "EtjjbknDRtNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "9ce4f39d-6c77-4f25-d2e5-65733885bf7d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 960 wav files and 960 lab files in /content/drive/MyDrive/fast_speech2/dataset/my_voice_dataset/wavs.\n",
            "phone : ['ᄒ', 'ᅩ', 'ᆨ', 'ᄉ', 'ᅵ', 'ᄅ', 'ᅡ', 'ᄃ', 'ᅩ', 'ᄉ', 'ᅥ', 'ᆼ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄋ', 'ᅳ', 'ᆯ', 'ᄋ', 'ᅵ', 'ᆰ', 'ᄂ', 'ᅳ', 'ᆫ', 'ᄃ', 'ᅩ', 'ᆨ', 'ᄌ', 'ᅡ', 'ᄃ', 'ᅳ', 'ᆯ', 'ᄋ', 'ᅵ', 'ᄇ', 'ᅦ', 'ᄃ', 'ᅳ', 'ᄅ', 'ᅩ', 'ᄋ', 'ᅴ', 'ᄆ', 'ᅡ', 'ᆯ', 'ᄋ', 'ᅳ', 'ᆯ', 'ᄀ', 'ᅩ', 'ᆮ', 'ᄋ', 'ᅵ', 'ᄀ', 'ᅩ', 'ᆮ', 'ᄃ', 'ᅢ', 'ᄅ', 'ᅩ', 'ᄇ', 'ᅡ', 'ᆮ', 'ᄋ', 'ᅡ', 'ᄃ', 'ᅳ', 'ᆯ', 'ᄋ', 'ᅵ', 'ᆯ', 'ᄁ', 'ᅡ', 'ᄇ', 'ᅪ', 'ᄋ', 'ᅧ', 'ᆷ', 'ᄅ', 'ᅧ', 'ᄒ', 'ᅢ', 'ᄉ', 'ᅥ', 'ᄎ', 'ᅵ', 'ᆫ', 'ᄌ', 'ᅥ', 'ᆯ', 'ᄒ', 'ᅡ', 'ᄀ', 'ᅦ', 'ᄉ', 'ᅥ', 'ᆯ', 'ᄆ', 'ᅧ', 'ᆼ', 'ᄋ', 'ᅳ', 'ᆯ', 'ᄒ', 'ᅢ', 'ᄌ', 'ᅮ', 'ᄀ', 'ᅩ', 'ᄋ', 'ᅵ', 'ᆻ', 'ᄂ', 'ᅳ', 'ᆫ', 'ᄀ', 'ᅥ', 'ᆺ', 'ᄋ', 'ᅵ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ']\n",
            "duration: [ 0  6  0 10  6  1 14  0  6  6  4  7  1  9  4  0  4  4  1  7  1  9  7  6\n",
            "  1 10  1  5 11  1  3  6  1 17  5 10  1  7  8  3  1  7  6  7  4  0  3  9\n",
            "  1  7  5  1  5  1 12  1  1  8  5  6  3  8  1  1 11  1  4  4  1  4  5  6\n",
            "  9  4 13  1  6  8  3 11  1  3  8 11 10  2  4  4  4  6  1 10  0  8 10  4\n",
            "  5  3  6  2  1  4  7  0  7  5  6  0  6  1  4  4  4  4  6  1  6  1  0 12\n",
            " 16  1  1  1  9]\n",
            "start : 0.0\n",
            ",end : 7.6\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-08bd72f97e28>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#학습 셋, 검증셋 생성, 내 목소리 생기면 그때 돌리기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-fe03526843c3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# 학습 데이터 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# 메타데이터 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-65467e72d4ff>\u001b[0m in \u001b[0;36mbuild_from_path\u001b[0;34m(in_dir, out_dir, meta)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-65467e72d4ff>\u001b[0m in \u001b[0;36mprocess_utterance\u001b[0;34m(in_dir, out_dir, basename, scalers)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Compute mel-scale spectrogram and energy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mel_from_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#wav에서 멜 추출, 에너지 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-657e3fb7b236>\u001b[0m in \u001b[0;36mget_mel_from_wav\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0maudio_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0maudio_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmelspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mmelspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-babbb1863bd2>\u001b[0m in \u001b[0;36mmel_spectrogram\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmagnitudes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mmagnitudes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagnitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitudes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7c748e960a7c>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         forward_transform = F.conv1d(\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loss.py\n"
      ],
      "metadata": {
        "id": "BnjQYPP9Onm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FastSpeech2Loss(nn.Module): #로스값 계산\n",
        "    \"\"\" FastSpeech2 Loss \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FastSpeech2Loss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.mae_loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, log_d_predicted, log_d_target, p_predicted, p_target, e_predicted, e_target, mel, mel_postnet, mel_target, src_mask, mel_mask):\n",
        "        log_d_target.requires_grad = False\n",
        "        p_target.requires_grad = False\n",
        "        e_target.requires_grad = False\n",
        "        mel_target.requires_grad = False\n",
        "\n",
        "        log_d_predicted = log_d_predicted.masked_select(src_mask)\n",
        "        log_d_target = log_d_target.masked_select(src_mask)\n",
        "        p_predicted = p_predicted.masked_select(src_mask)\n",
        "        p_target = p_target.masked_select(src_mask)\n",
        "        e_predicted = e_predicted.masked_select(src_mask)\n",
        "        e_target = e_target.masked_select(src_mask)\n",
        "\n",
        "        mel = mel.masked_select(mel_mask.unsqueeze(-1))\n",
        "        mel_postnet = mel_postnet.masked_select(mel_mask.unsqueeze(-1))\n",
        "        mel_target = mel_target.masked_select(mel_mask.unsqueeze(-1))\n",
        "\n",
        "        mel_loss = self.mse_loss(mel, mel_target)\n",
        "        mel_postnet_loss = self.mse_loss(mel_postnet, mel_target)\n",
        "\n",
        "        d_loss = self.mae_loss(log_d_predicted, log_d_target)\n",
        "        p_loss = self.mae_loss(p_predicted, p_target)\n",
        "        e_loss = self.mae_loss(e_predicted, e_target)\n",
        "\n",
        "        return mel_loss, mel_postnet_loss, d_loss, p_loss, e_loss"
      ],
      "metadata": {
        "id": "_AssZbT4Ooqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#modules.py"
      ],
      "metadata": {
        "id": "kgtfuaUgOwmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class VarianceAdaptor(nn.Module): #variance adatptor 모듈 선언\n",
        "    \"\"\" Variance Adaptor \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VarianceAdaptor, self).__init__()\n",
        "        self.duration_predictor = VariancePredictor()\n",
        "        self.length_regulator = LengthRegulator()\n",
        "        self.pitch_predictor = VariancePredictor()\n",
        "        self.energy_predictor = VariancePredictor()\n",
        "\n",
        "        self.energy_embedding_producer = Conv(1, hp.encoder_hidden, kernel_size=9, bias=False, padding=4)\n",
        "        self.pitch_embedding_producer = Conv(1, hp.encoder_hidden, kernel_size=9, bias=False, padding=4)\n",
        "\n",
        "    def forward(self, x, src_mask, mel_mask=None, duration_target=None, pitch_target=None, energy_target=None, max_len=None):\n",
        "        log_duration_prediction = self.duration_predictor(x, src_mask)\n",
        "\n",
        "        pitch_prediction = self.pitch_predictor(x, src_mask)\n",
        "        if pitch_target is not None:\n",
        "            pitch_embedding = self.pitch_embedding_producer(pitch_target.unsqueeze(2))\n",
        "        else:\n",
        "            pitch_embedding = self.pitch_embedding_producer(pitch_prediction.unsqueeze(2))\n",
        "\n",
        "        energy_prediction = self.energy_predictor(x, src_mask)\n",
        "        if energy_target is not None:\n",
        "            energy_embedding = self.energy_embedding_producer(energy_target.unsqueeze(2))\n",
        "        else:\n",
        "            energy_embedding = self.energy_embedding_producer(energy_prediction.unsqueeze(2))\n",
        "\n",
        "        x = x + pitch_embedding + energy_embedding #피치 임베딩, 에너지 임베딩을 최종값에 더함\n",
        "\n",
        "        if duration_target is not None:\n",
        "            x, mel_len = self.length_regulator(x, duration_target, max_len)\n",
        "        else:\n",
        "            duration_rounded = torch.clamp(torch.round(torch.exp(log_duration_prediction)-hp.log_offset), min=0)\n",
        "            x, mel_len = self.length_regulator(x, duration_rounded, max_len)\n",
        "            mel_mask = get_mask_from_lengths(mel_len)\n",
        "\n",
        "        return x, log_duration_prediction, pitch_prediction, energy_prediction, mel_len, mel_mask\n",
        "\n",
        "\n",
        "class LengthRegulator(nn.Module): #길이 조정\n",
        "    \"\"\" Length Regulator \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LengthRegulator, self).__init__()\n",
        "\n",
        "    def LR(self, x, duration, max_len):\n",
        "        output = list()\n",
        "        mel_len = list()\n",
        "        for batch, expand_target in zip(x, duration):\n",
        "            expanded = self.expand(batch, expand_target)\n",
        "            output.append(expanded)\n",
        "            mel_len.append(expanded.shape[0])\n",
        "\n",
        "        if max_len is not None:\n",
        "            output = pad(output, max_len)\n",
        "        else:\n",
        "            output = pad(output)\n",
        "\n",
        "        return output, torch.LongTensor(mel_len).to(device)\n",
        "\n",
        "    def expand(self, batch, predicted):\n",
        "        out = list()\n",
        "\n",
        "        for i, vec in enumerate(batch):\n",
        "            expand_size = predicted[i].item()\n",
        "            out.append(vec.expand(int(expand_size), -1))\n",
        "        out = torch.cat(out, 0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, duration, max_len):\n",
        "        output, mel_len = self.LR(x, duration, max_len)\n",
        "        return output, mel_len\n",
        "\n",
        "\n",
        "class VariancePredictor(nn.Module): #길이, 피치, 에너지 추측\n",
        "    \"\"\" Duration, Pitch and Energy Predictor \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VariancePredictor, self).__init__()\n",
        "\n",
        "        self.input_size = hp.encoder_hidden\n",
        "        self.filter_size = hp.variance_predictor_filter_size\n",
        "        self.kernel = hp.variance_predictor_kernel_size\n",
        "        self.conv_output_size = hp.variance_predictor_filter_size\n",
        "        self.dropout = hp.variance_predictor_dropout\n",
        "\n",
        "        self.conv_layer = nn.Sequential(OrderedDict([\n",
        "            (\"conv1d_1\", Conv(self.input_size,\n",
        "                              self.filter_size,\n",
        "                              kernel_size=self.kernel,\n",
        "                              padding=(self.kernel-1)//2)),\n",
        "            (\"relu_1\", nn.ReLU()),\n",
        "            (\"layer_norm_1\", nn.LayerNorm(self.filter_size)),\n",
        "            (\"dropout_1\", nn.Dropout(self.dropout)),\n",
        "            (\"conv1d_2\", Conv(self.filter_size,\n",
        "                              self.filter_size,\n",
        "                              kernel_size=self.kernel,\n",
        "                              padding=1)),\n",
        "            (\"relu_2\", nn.ReLU()),\n",
        "            (\"layer_norm_2\", nn.LayerNorm(self.filter_size)),\n",
        "            (\"dropout_2\", nn.Dropout(self.dropout))\n",
        "        ]))\n",
        "\n",
        "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
        "\n",
        "    def forward(self, encoder_output, mask):\n",
        "        out = self.conv_layer(encoder_output)\n",
        "        out = self.linear_layer(out)\n",
        "        out = out.squeeze(-1)\n",
        "\n",
        "        if mask is not None:\n",
        "            out = out.masked_fill(mask, 0.)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution Module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=1,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 bias=True,\n",
        "                 w_init='linear'):\n",
        "        \"\"\"\n",
        "        :param in_channels: dimension of input\n",
        "        :param out_channels: dimension of output\n",
        "        :param kernel_size: size of kernel\n",
        "        :param stride: size of stride\n",
        "        :param padding: size of padding\n",
        "        :param dilation: dilation rate\n",
        "        :param bias: boolean. if True, bias is included.\n",
        "        :param w_init: str. weight inits with xavier initialization.\n",
        "        \"\"\"\n",
        "        super(Conv, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels,\n",
        "                              out_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride,\n",
        "                              padding=padding,\n",
        "                              dilation=dilation,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().transpose(1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = x.contiguous().transpose(1, 2)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kV0VlMWLOy39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#optimizer.py"
      ],
      "metadata": {
        "id": "yAYwLYUhO3v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ScheduledOptim(): #옵티마이저 생성\n",
        "    ''' A simple wrapper class for learning rate scheduling '''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps, current_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = current_steps\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # print(self.init_lr)\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "xYzvV9-HO5z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 원본 vocgan 보코더"
      ],
      "metadata": {
        "id": "X6eE3dRlORvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\t[VocGAN] Generator\n",
        "\n",
        "\t\tthis source code is implemenation of the modified-VocGAN from rishikksh20\n",
        "\t\tgit repository: https://github.com/rishikksh20/VocGAN\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class ResStack(nn.Module):\n",
        "    def __init__(self, channel, dilation=1):\n",
        "        super(ResStack, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.ReflectionPad1d(dilation),\n",
        "                nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=3, dilation=dilation)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=1)),\n",
        "            )\n",
        "\n",
        "\n",
        "        self.shortcut = nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.shortcut(x) + self.block(x)\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        nn.utils.remove_weight_norm(self.block[2])\n",
        "        nn.utils.remove_weight_norm(self.block[4])\n",
        "        nn.utils.remove_weight_norm(self.shortcut)\n",
        "\n",
        "\n",
        "# Modified VocGAN\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, mel_channel, n_residual_layers, ratios=[4, 4, 2, 2, 2, 2], mult=256, out_band=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.mel_channel = mel_channel\n",
        "\n",
        "        self.start = nn.Sequential(\n",
        "            nn.ReflectionPad1d(3),\n",
        "            nn.utils.weight_norm(nn.Conv1d(mel_channel, mult * 2, kernel_size=7, stride=1))\n",
        "        )\n",
        "        r = ratios[0]\n",
        "        self.upsample_1 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "        self.res_stack_1 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "        r = ratios[1]\n",
        "        mult = mult // 2\n",
        "        self.upsample_2 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "        self.res_stack_2 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "        r = ratios[2]\n",
        "        mult = mult // 2\n",
        "        self.upsample_3 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "\n",
        "        self.skip_upsample_1 = nn.utils.weight_norm(nn.ConvTranspose1d(mel_channel, mult,\n",
        "                                                                       kernel_size=64, stride=32,\n",
        "                                                                       padding=16,\n",
        "                                                                       output_padding=0)\n",
        "                                                    )\n",
        "        self.res_stack_3 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "\n",
        "\n",
        "        r = ratios[3]\n",
        "        mult = mult // 2\n",
        "        self.upsample_4 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "\n",
        "        self.skip_upsample_2 = nn.utils.weight_norm(nn.ConvTranspose1d(mel_channel, mult,\n",
        "                                                                       kernel_size=128, stride=64,\n",
        "                                                                       padding=32,\n",
        "                                                                       output_padding=0)\n",
        "                                                    )\n",
        "        self.res_stack_4 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "\n",
        "        r = ratios[4]\n",
        "        mult = mult // 2\n",
        "        self.upsample_5 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "\n",
        "        self.skip_upsample_3 = nn.utils.weight_norm(nn.ConvTranspose1d(mel_channel, mult,\n",
        "                                                                       kernel_size=256, stride=128,\n",
        "                                                                       padding=64,\n",
        "                                                                       output_padding=0)\n",
        "                                                    )\n",
        "        self.res_stack_5 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "\n",
        "        r = ratios[5]\n",
        "        mult = mult // 2\n",
        "        self.upsample_6 = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.weight_norm(nn.ConvTranspose1d(mult * 2, mult,\n",
        "                                                    kernel_size=r * 2, stride=r,\n",
        "                                                    padding=r // 2 + r % 2,\n",
        "                                                    output_padding=r % 2)\n",
        "                                 )\n",
        "        )\n",
        "\n",
        "        self.skip_upsample_4 = nn.utils.weight_norm(nn.ConvTranspose1d(mel_channel, mult,\n",
        "                                                                       kernel_size=512, stride=256,\n",
        "                                                                       padding=128,\n",
        "                                                                       output_padding=0)\n",
        "                                                    )\n",
        "        self.res_stack_6 = nn.Sequential(*[ResStack(mult, dilation=3 ** j) for j in range(n_residual_layers)])\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ReflectionPad1d(3),\n",
        "            nn.utils.weight_norm(nn.Conv1d(mult, out_band, kernel_size=7, stride=1)),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, mel):\n",
        "        mel = (mel + 5.0) / 5.0  # roughly normalize spectrogram\n",
        "        # Mel Shape [B, num_mels, T] -> torch.Size([3, 80, 10])\n",
        "        x = self.start(mel)  # [B, dim*2, T] -> torch.Size([3, 512, 10])\n",
        "\n",
        "        x = self.upsample_1(x)\n",
        "        x = self.res_stack_1(x)  # [B, dim, T*4] -> torch.Size([3, 256, 40])\n",
        "\n",
        "        x = self.upsample_2(x)\n",
        "        x = self.res_stack_2(x)  # [B, dim/2, T*16] -> torch.Size([3, 128, 160])\n",
        "\n",
        "        x = self.upsample_3(x)\n",
        "        x = x + self.skip_upsample_1(mel)\n",
        "        x = self.res_stack_3(x)  # [B, dim/4, T*32] -> torch.Size([3, 64, 320])\n",
        "\n",
        "        x = self.upsample_4(x)\n",
        "        x = x + self.skip_upsample_2(mel)\n",
        "        x = self.res_stack_4(x)  # [B, dim/8, T*64] -> torch.Size([3, 32, 640])\n",
        "\n",
        "        x = self.upsample_5(x)\n",
        "        x = x + self.skip_upsample_3(mel)\n",
        "        x = self.res_stack_5(x)  # [B, dim/16, T*128] -> torch.Size([3, 16, 1280])\n",
        "\n",
        "        x = self.upsample_6(x)\n",
        "        x = x + self.skip_upsample_4(mel)\n",
        "        x = self.res_stack_6(x)  # [B, dim/32, T*256] -> torch.Size([3, 8, 2560])\n",
        "\n",
        "        out = self.out(x)  # [B, 1, T*256] -> torch.Size([3, 1, 2560])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def eval(self, inference=False):\n",
        "        super(Generator, self).eval()\n",
        "\n",
        "        # don't remove weight norm while validation in training loop\n",
        "        if inference:\n",
        "            self.remove_weight_norm()\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        \"\"\"Remove weight normalization module from all of the layers.\"\"\"\n",
        "\n",
        "        def _remove_weight_norm(m):\n",
        "            try:\n",
        "                torch.nn.utils.remove_weight_norm(m)\n",
        "            except ValueError:  # this module didn't have weight norm\n",
        "                return\n",
        "\n",
        "        self.apply(_remove_weight_norm)\n",
        "\n",
        "    def apply_weight_norm(self):\n",
        "        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n",
        "\n",
        "        def _apply_weight_norm(m):\n",
        "            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.ConvTranspose1d):\n",
        "                torch.nn.utils.weight_norm(m)\n",
        "\n",
        "        self.apply(_apply_weight_norm)\n",
        "\n",
        "\n",
        "    def infer(self, mel):\n",
        "        hop_length = 256\n",
        "        # pad input mel with zeros to cut artifact\n",
        "        # see https://github.com/seungwonpark/melgan/issues/8\n",
        "        zero = torch.full((1, self.mel_channel, 10), -11.5129).to(mel.device)\n",
        "        mel = torch.cat((mel, zero), dim=2)\n",
        "\n",
        "        audio = self.forward(mel)\n",
        "        return audio\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5PU-PDQVOVDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset.py"
      ],
      "metadata": {
        "id": "citoTcN0POt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Dataset(Dataset): #데이터셋 객체 지정\n",
        "    def __init__(self, filename=\"train.txt\", sort=True):\n",
        "        self.basename, self.text = process_meta(os.path.join(hp.preprocessed_path, filename))\n",
        "\n",
        "        self.mean_mel, self.std_mel = np.load(os.path.join(hp.preprocessed_path, \"mel_stat.npy\"))\n",
        "        self.mean_f0, self.std_f0 = np.load(os.path.join(hp.preprocessed_path, \"f0_stat.npy\"))\n",
        "        self.mean_energy, self.std_energy = np.load(os.path.join(hp.preprocessed_path, \"energy_stat.npy\"))\n",
        "\n",
        "        self.sort = sort\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx): #멜 데이터셋 불러오는 경우, 각각의 파일에 대해 mel, f0, alignment\n",
        "        t=self.text[idx]\n",
        "        basename=self.basename[idx]\n",
        "        phone = np.array(text_to_sequence(t, []))\n",
        "\n",
        "        mel_path = os.path.join(\n",
        "            hp.preprocessed_path, \"mel\", \"{}-mel-{}.npy\".format(hp.dataset, basename))\n",
        "        mel_target = np.load(mel_path)\n",
        "        D_path = os.path.join(\n",
        "            hp.preprocessed_path, \"my_voice_dataset_aligned\", \"{}-ali-{}.npy\".format(hp.dataset, basename))\n",
        "        D = np.load(D_path)\n",
        "        f0_path = os.path.join(\n",
        "            hp.preprocessed_path, \"f0\", \"{}-f0-{}.npy\".format(hp.dataset, basename))\n",
        "        f0 = np.load(f0_path)\n",
        "        energy_path = os.path.join(\n",
        "            hp.preprocessed_path, \"energy\", \"{}-energy-{}.npy\".format(hp.dataset, basename))\n",
        "        energy = np.load(energy_path)\n",
        "\n",
        "        sample = {\"id\": basename,\n",
        "                  \"text\": phone,\n",
        "                  \"mel_target\": mel_target,\n",
        "                  \"D\": D,\n",
        "                  \"f0\": f0,\n",
        "                  \"energy\": energy}\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def reprocess(self, batch, cut_list):\n",
        "        ids = [batch[ind][\"id\"] for ind in cut_list]\n",
        "        texts = [batch[ind][\"text\"] for ind in cut_list]\n",
        "        mel_targets = [standard_norm(batch[ind][\"mel_target\"], self.mean_mel, self.std_mel, is_mel=True) for ind in cut_list]\n",
        "        Ds = [batch[ind][\"D\"] for ind in cut_list]\n",
        "        f0s = [standard_norm(batch[ind][\"f0\"], self.mean_f0, self.std_f0) for ind in cut_list]\n",
        "        energies = [standard_norm(batch[ind][\"energy\"], self.mean_energy, self.std_energy) for ind in cut_list]\n",
        "\n",
        "        for text, D, id_ in zip(texts, Ds, ids):\n",
        "            if len(text) != len(D):\n",
        "                print('the dimension of text and duration should be the same')\n",
        "                print('text: ',sequence_to_text(text))\n",
        "                print(text, text.shape, D, D.shape, id_)\n",
        "        length_text = np.array(list())\n",
        "        for text in texts:\n",
        "            length_text = np.append(length_text, text.shape[0])\n",
        "\n",
        "        length_mel = np.array(list())\n",
        "        for mel in mel_targets:\n",
        "            length_mel = np.append(length_mel, mel.shape[0])\n",
        "\n",
        "        texts = pad_1D(texts)\n",
        "        Ds = pad_1D(Ds)\n",
        "        mel_targets = pad_2D(mel_targets)\n",
        "        f0s = pad_1D(f0s)\n",
        "        energies = pad_1D(energies)\n",
        "        log_Ds = np.log(Ds + hp.log_offset)\n",
        "\n",
        "        out = {\"id\": ids,\n",
        "               \"text\": texts,\n",
        "               \"mel_target\": mel_targets,\n",
        "               \"D\": Ds,\n",
        "               \"log_D\": log_Ds,\n",
        "               \"f0\": f0s,\n",
        "               \"energy\": energies,\n",
        "               \"src_len\": length_text,\n",
        "               \"mel_len\": length_mel}\n",
        "\n",
        "        return out\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        len_arr = np.array([d[\"text\"].shape[0] for d in batch])\n",
        "        index_arr = np.argsort(-len_arr)\n",
        "        batchsize = len(batch)\n",
        "        real_batchsize = int(math.sqrt(batchsize))\n",
        "\n",
        "        cut_list = list()\n",
        "        for i in range(real_batchsize):\n",
        "            if self.sort:\n",
        "                cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n",
        "            else:\n",
        "                cut_list.append(np.arange(i*real_batchsize, (i+1)*real_batchsize))\n",
        "\n",
        "        output = list()\n",
        "        for i in range(real_batchsize):\n",
        "            output.append(self.reprocess(batch, cut_list[i]))\n",
        "\n",
        "        return output\n",
        "\n",
        "#테스트인듯?\n",
        "if __name__ == \"__main__\":\n",
        "    # Test\n",
        "    dataset = Dataset('ljs_audio_text_val_filelist.txt')\n",
        "    training_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=dataset.collate_fn,\n",
        "        drop_last=True, num_workers=0)\n",
        "    total_step = hp.epochs * len(training_loader) * hp.batch_size\n",
        "\n",
        "    cnt = 0\n",
        "    for i, batchs in enumerate(training_loader):\n",
        "        for j, data_of_batch in enumerate(batchs):\n",
        "            mel_target = torch.from_numpy(\n",
        "                data_of_batch[\"mel_target\"]).float().to(device)\n",
        "            D = torch.from_numpy(data_of_batch[\"D\"]).int().to(device)\n",
        "            if mel_target.shape[1] == D.sum().item():\n",
        "                cnt += 1\n"
      ],
      "metadata": {
        "id": "q6fQGX0KPecF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fastspeech2.py"
      ],
      "metadata": {
        "id": "AGACyl2GPcE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class FastSpeech2(nn.Module): #전체 모듈\n",
        "    \"\"\" FastSpeech2 \"\"\"\n",
        "\n",
        "    def __init__(self, use_postnet=True):\n",
        "        super(FastSpeech2, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder()\n",
        "        self.variance_adaptor = VarianceAdaptor()\n",
        "\n",
        "        self.decoder = Decoder()\n",
        "        self.mel_linear = nn.Linear(hp.decoder_hidden, hp.n_mel_channels)\n",
        "\n",
        "        self.use_postnet = use_postnet\n",
        "        if self.use_postnet:\n",
        "            self.postnet = PostNet()\n",
        "\n",
        "    def forward(self, src_seq, src_len, mel_len=None, d_target=None, p_target=None, e_target=None, max_src_len=None, max_mel_len=None):\n",
        "        src_mask = get_mask_from_lengths(src_len, max_src_len)\n",
        "        mel_mask = get_mask_from_lengths(mel_len, max_mel_len) if mel_len is not None else None\n",
        "\n",
        "        encoder_output = self.encoder(src_seq, src_mask)\n",
        "        if d_target is not None:\n",
        "            variance_adaptor_output, d_prediction, p_prediction, e_prediction, _, _ = self.variance_adaptor(\n",
        "                encoder_output, src_mask, mel_mask, d_target, p_target, e_target, max_mel_len)\n",
        "        else:\n",
        "            variance_adaptor_output, d_prediction, p_prediction, e_prediction, mel_len, mel_mask = self.variance_adaptor(\n",
        "                    encoder_output, src_mask, mel_mask, d_target, p_target, e_target, max_mel_len)\n",
        "\n",
        "        decoder_output = self.decoder(variance_adaptor_output, mel_mask)\n",
        "        mel_output = self.mel_linear(decoder_output)\n",
        "\n",
        "        if self.use_postnet:\n",
        "            mel_output_postnet = self.postnet(mel_output) + mel_output\n",
        "        else:\n",
        "            mel_output_postnet = mel_output\n",
        "\n",
        "        return mel_output, mel_output_postnet, d_prediction, p_prediction, e_prediction, src_mask, mel_mask, mel_len\n",
        "\n",
        "\n",
        "#파라미터 테스트\n",
        "if __name__ == \"__main__\":\n",
        "    # Test\n",
        "    model = FastSpeech2(use_postnet=False)\n",
        "    print(model)\n",
        "    print(sum(param.numel() for param in model.parameters()))"
      ],
      "metadata": {
        "id": "5kMVzvsZP8Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 하이퍼 파라미터 재선언"
      ],
      "metadata": {
        "id": "ZC2NcUuL8z0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from easydict import EasyDict as edict\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/fast_speech2\"\n",
        "\n",
        "hp = edict({\n",
        "    # Dataset paths\n",
        "    'dataset': \"my_voice_dataset\",\n",
        "    'data_path': os.path.join(path, \"dataset\", \"my_voice_dataset\"),  # kss 데이터셋 위치\n",
        "    'meta_name': \"metadata.txt\",  # 음성 데이터 발화내용\n",
        "    'textgrid_name': \"my_voice_dataset_aligned.zip\",  # TextGrid 파일 이름 (대소문자 주의)\n",
        "\n",
        "    # GPU settings\n",
        "    'train_visible_devices': \"0,1\",  # 학습에 사용할 GPU ID\n",
        "    'synth_visible_devices': \"0\",  # 합성에 사용할 GPU ID\n",
        "\n",
        "    # Text settings\n",
        "    'text_cleaners': ['korean_cleaners'],  # 텍스트 전처리 설정\n",
        "\n",
        "    # Audio and mel spectrogram settings\n",
        "    'sampling_rate': 22050,  # 샘플링 속도\n",
        "    'filter_length': 1024,  # 필터 길이\n",
        "    'hop_length': 256,  # hop 길이\n",
        "    'win_length': 1024,  # 윈도우 길이\n",
        "    'max_wav_value': 32768.0,  # WAV 데이터 최대 값\n",
        "    'n_mel_channels': 80,  # mel 스펙트로그램 채널 수\n",
        "    'mel_fmin': 0,  # mel 스펙트로그램 최소 주파수\n",
        "    'mel_fmax': 8000,  # mel 스펙트로그램 최대 주파수\n",
        "\n",
        "    # Pitch (F0) and Energy settings\n",
        "    'f0_min': 71.0,  # 최소 기본 주파수\n",
        "    'f0_max': 792.8,  # 최대 기본 주파수\n",
        "    'energy_min': 0.0,  # 최소 에너지 값\n",
        "    'energy_max': 283.72,  # 최대 에너지 값\n",
        "\n",
        "    # FastSpeech 2 model settings\n",
        "    'encoder_layer': 4,  # 인코더 레이어 수\n",
        "    'encoder_head': 2,  # 인코더 헤드 수\n",
        "    'encoder_hidden': 256,  # 인코더 히든 크기\n",
        "    'decoder_layer': 4,  # 디코더 레이어 수\n",
        "    'decoder_head': 2,  # 디코더 헤드 수\n",
        "    'decoder_hidden': 256,  # 디코더 히든 크기\n",
        "    'fft_conv1d_filter_size': 1024,  # FFT 필터 크기\n",
        "    'fft_conv1d_kernel_size': (9, 1),  # FFT 커널 크기\n",
        "    'encoder_dropout': 0.2,  # 인코더 드롭아웃 비율\n",
        "    'decoder_dropout': 0.2,  # 디코더 드롭아웃 비율\n",
        "\n",
        "    # Variance predictor settings\n",
        "    'variance_predictor_filter_size': 256,  # 분산 예측기 필터 크기\n",
        "    'variance_predictor_kernel_size': 3,  # 분산 예측기 커널 크기\n",
        "    'variance_predictor_dropout': 0.5,  # 분산 예측기 드롭아웃 비율\n",
        "\n",
        "    # Sequence settings\n",
        "    'max_seq_len': 1000,  # 최대 시퀀스 길이\n",
        "\n",
        "    # Checkpoints and synthesis paths\n",
        "    'preprocessed_path': os.path.join(path, \"dataset\", \"preprocessed\", \"my_voice_dataset\"),\n",
        "    'checkpoint_path': os.path.join(path, \"checkpoint\", \"my_voice_dataset\"),\n",
        "    'eval_path': os.path.join(path, \"dataset\", \"eval\", \"my_voice_dataset\"),\n",
        "    'log_path': os.path.join(path, \"log\", \"my_voice_dataset\"),\n",
        "    'test_path': os.path.join(path, \"results\", \"test\"),\n",
        "\n",
        "    # Optimizer settings\n",
        "    'batch_size': 4,  # 배치 크기\n",
        "    'epochs': 1000,  # 학습 에포크 수\n",
        "    'n_warm_up_step': 4000,  # Warm-up 스텝\n",
        "    'grad_clip_thresh': 1.0,  # 그래디언트 클리핑 한계\n",
        "    'acc_steps': 1,  # Accumulation 스텝\n",
        "    'betas': (0.9, 0.98),  # Adam 옵티마이저 베타 값\n",
        "    'eps': 1e-9,  # Adam 옵티마이저 epsilon 값\n",
        "    'weight_decay': 0.0,  # 가중치 감소 비율\n",
        "\n",
        "    # Vocoder settings\n",
        "    'vocoder': 'vocgan',  # 보코더 유형\n",
        "    'vocoder_pretrained_model_name': \"vocgan_kss_pretrained_model_epoch_4500.pt\",  # 보코더 사전 학습 모델 이름\n",
        "    'vocoder_pretrained_model_path': os.path.join(path, \"checkpoint\", \"vocgan_kss_pretrained_model_epoch_4500.pt\"),\n",
        "\n",
        "\n",
        "    # Log-scaled duration\n",
        "    'log_offset': 1.0,\n",
        "\n",
        "    # Save, log, and synthesis settings\n",
        "    'save_step': 10000,  # 체크포인트 저장 간격\n",
        "    'eval_step': 1000,  # 평가 간격\n",
        "    'eval_size': 256,  # 평가 데이터 크기\n",
        "    'log_step': 1000,  # 로그 출력 간격\n",
        "    'clear_Time': 20,  # 로그 클리어 간격\n",
        "\n",
        "    'restore_step':500000\n",
        "})\n"
      ],
      "metadata": {
        "id": "LtkVnXBNUiVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#evaluate"
      ],
      "metadata": {
        "id": "62x4uTmYQJu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_FastSpeech2(num):\n",
        "    checkpoint_path = os.path.join(hp.checkpoint_path, \"checkpoint_{}.pth.tar\".format(num))\n",
        "    model = nn.DataParallel(FastSpeech2())\n",
        "    model.load_state_dict(torch.load(checkpoint_path)['model'])\n",
        "    model.requires_grad = False\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def evaluate(model, step, vocoder=None):#평가모드\n",
        "    model.eval()\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    mean_mel, std_mel = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"mel_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_f0, std_f0 = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"f0_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_energy, std_energy = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"energy_stat.npy\")), dtype=torch.float).to(device)\n",
        "\n",
        "    eval_path = hp.eval_path\n",
        "    if not os.path.exists(eval_path):\n",
        "        os.makedirs(eval_path)\n",
        "\n",
        "    # Get dataset\n",
        "    dataset = Dataset(\"val.txt\", sort=False)\n",
        "    loader = DataLoader(dataset, batch_size=hp.batch_size**2, shuffle=False, collate_fn=dataset.collate_fn, drop_last=False, num_workers=0, )\n",
        "\n",
        "    # Get loss function\n",
        "    Loss = FastSpeech2Loss().to(device)\n",
        "\n",
        "    # Evaluation\n",
        "    d_l = []\n",
        "    f_l = []\n",
        "    e_l = []\n",
        "    mel_l = []\n",
        "    mel_p_l = []\n",
        "    current_step = 0\n",
        "    idx = 0\n",
        "    for i, batchs in enumerate(loader):\n",
        "        for j, data_of_batch in enumerate(batchs):\n",
        "            # Get Data\n",
        "            id_ = data_of_batch[\"id\"]\n",
        "            text = torch.from_numpy(data_of_batch[\"text\"]).long().to(device)\n",
        "            mel_target = torch.from_numpy(data_of_batch[\"mel_target\"]).float().to(device)\n",
        "            D = torch.from_numpy(data_of_batch[\"D\"]).int().to(device)\n",
        "            log_D = torch.from_numpy(data_of_batch[\"log_D\"]).int().to(device)\n",
        "            f0 = torch.from_numpy(data_of_batch[\"f0\"]).float().to(device)\n",
        "            energy = torch.from_numpy(data_of_batch[\"energy\"]).float().to(device)\n",
        "            src_len = torch.from_numpy(data_of_batch[\"src_len\"]).long().to(device)\n",
        "            mel_len = torch.from_numpy(data_of_batch[\"mel_len\"]).long().to(device)\n",
        "            max_src_len = np.max(data_of_batch[\"src_len\"]).astype(np.int32)\n",
        "            max_mel_len = np.max(data_of_batch[\"mel_len\"]).astype(np.int32)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Forward\n",
        "                mel_output, mel_postnet_output, log_duration_output, f0_output, energy_output, src_mask, mel_mask, out_mel_len = model(\n",
        "                        text, src_len, mel_len, D, f0, energy, max_src_len, max_mel_len)\n",
        "\n",
        "                # Cal Loss\n",
        "                mel_loss, mel_postnet_loss, d_loss, f_loss, e_loss = Loss(\n",
        "                        log_duration_output, log_D, f0_output, f0, energy_output, energy, mel_output, mel_postnet_output, mel_target, ~src_mask, ~mel_mask)\n",
        "\n",
        "                d_l.append(d_loss.item())\n",
        "                f_l.append(f_loss.item())\n",
        "                e_l.append(e_loss.item())\n",
        "                mel_l.append(mel_loss.item())\n",
        "                mel_p_l.append(mel_postnet_loss.item())\n",
        "\n",
        "                if idx == 0 and vocoder is not None:\n",
        "                    # Run vocoding and plotting spectrogram only when the vocoder is defined\n",
        "                    for k in range(1):\n",
        "                        basename = id_[k]\n",
        "                        gt_length = mel_len[k]\n",
        "                        out_length = out_mel_len[k]\n",
        "\n",
        "                        mel_target_torch = mel_target[k:k+1, :gt_length]\n",
        "                        mel_target_ = mel_target[k, :gt_length]\n",
        "                        mel_postnet_torch = mel_postnet_output[k:k+1, :out_length]\n",
        "                        mel_postnet = mel_postnet_output[k, :out_length]\n",
        "\n",
        "                        mel_target_torch = de_norm(mel_target_torch, mean_mel, std_mel).transpose(1, 2).detach()\n",
        "                        mel_target_ = de_norm(mel_target_, mean_mel, std_mel).cpu().transpose(0, 1).detach()\n",
        "                        mel_postnet_torch = de_norm(mel_postnet_torch, mean_mel, std_mel).transpose(1, 2).detach()\n",
        "                        mel_postnet = de_norm(mel_postnet, mean_mel, std_mel).cpu().transpose(0, 1).detach()\n",
        "\n",
        "                        if hp.vocoder == \"vocgan\":\n",
        "                            vocgan_infer(mel_target_torch, vocoder, path=os.path.join(hp.eval_path, 'eval_groundtruth_{}_{}.wav'.format(basename, hp.vocoder)))\n",
        "                            vocgan_infer(mel_postnet_torch, vocoder, path=os.path.join(hp.eval_path, 'eval_step_{}_{}_{}.wav'.format(step, basename, hp.vocoder)))\n",
        "                        np.save(os.path.join(hp.eval_path, 'eval_step_{}_{}_mel.npy'.format(step, basename)), mel_postnet.numpy())\n",
        "\n",
        "                        f0_ = f0[k, :gt_length]\n",
        "                        energy_ = energy[k, :gt_length]\n",
        "                        f0_output_ = f0_output[k, :out_length]\n",
        "                        energy_output_ = energy_output[k, :out_length]\n",
        "\n",
        "                        f0_ = de_norm(f0_, mean_f0, std_f0).detach().cpu().numpy()\n",
        "                        f0_output_ = de_norm(f0_output, mean_f0, std_f0).detach().cpu().numpy()\n",
        "                        energy_ = de_norm(energy_, mean_energy, std_energy).detach().cpu().numpy()\n",
        "                        energy_output_ = de_norm(energy_output_, mean_energy, std_energy).detach().cpu().numpy()\n",
        "\n",
        "                        plot_data([(mel_postnet.numpy(), f0_output_, energy_output_), (mel_target_.numpy(), f0_, energy_)],\n",
        "                            ['Synthesized Spectrogram', 'Ground-Truth Spectrogram'], filename=os.path.join(hp.eval_path, 'eval_step_{}_{}.png'.format(step, basename)))\n",
        "                        idx += 1\n",
        "                    print(\"done\")\n",
        "            current_step += 1\n",
        "\n",
        "    d_l = sum(d_l) / len(d_l)\n",
        "    f_l = sum(f_l) / len(f_l)\n",
        "    e_l = sum(e_l) / len(e_l)\n",
        "    mel_l = sum(mel_l) / len(mel_l)\n",
        "    mel_p_l = sum(mel_p_l) / len(mel_p_l)\n",
        "\n",
        "    str1 = \"FastSpeech2 Step {},\".format(step)\n",
        "    str2 = \"Duration Loss: {}\".format(d_l)\n",
        "    str3 = \"F0 Loss: {}\".format(f_l)\n",
        "    str4 = \"Energy Loss: {}\".format(e_l)\n",
        "    str5 = \"Mel Loss: {}\".format(mel_l)\n",
        "    str6 = \"Mel Postnet Loss: {}\".format(mel_p_l)\n",
        "\n",
        "    print(\"\\n\" + str1)\n",
        "    print(str2)\n",
        "    print(str3)\n",
        "    print(str4)\n",
        "    print(str5)\n",
        "    print(str6)\n",
        "\n",
        "    with open(os.path.join(hp.log_path, \"eval.txt\"), \"a\") as f_log:\n",
        "        f_log.write(str1 + \"\\n\")\n",
        "        f_log.write(str2 + \"\\n\")\n",
        "        f_log.write(str3 + \"\\n\")\n",
        "        f_log.write(str4 + \"\\n\")\n",
        "        f_log.write(str5 + \"\\n\")\n",
        "        f_log.write(str6 + \"\\n\")\n",
        "        f_log.write(\"\\n\")\n",
        "    model.train()\n",
        "\n",
        "    return d_l, f_l, e_l, mel_l, mel_p_l\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--step', type=int, default=30000)\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     # Get model\n",
        "#     model = get_FastSpeech2(args.step).to(device) #모델 파라미터 여기서 불러오는 것 같음\n",
        "#     print(\"Model Has Been Defined\")\n",
        "#     num_param = utils.get_param_num(model)\n",
        "#     print('Number of FastSpeech2 Parameters:', num_param)\n",
        "\n",
        "#     # Load vocoder\n",
        "#     if hp.vocoder == 'vocgan': #보코더 종류 변환을 여기서 해야할듯?\n",
        "#         vocoder = utils.get_vocgan(ckpt_path=hp.vocoder_pretrained_model_path)\n",
        "#     vocoder.to(device)\n",
        "\n",
        "#     # Init directories\n",
        "#     if not os.path.exists(hp.log_path):\n",
        "#         os.makedirs(hp.log_path)\n",
        "#     if not os.path.exists(hp.eval_path):\n",
        "#         os.makedirs(hp.eval_path)\n",
        "#     evaluate(model, args.step, vocoder)\n",
        "\n",
        "#수정?\n",
        "if __name__ == \"__main__\":\n",
        "    # Argument parser setup\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--step', type=int, default=hp.restore_step)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Device setup\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = hp.train_visible_devices\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # # Get model\n",
        "    # model = get_FastSpeech2(args.step).to(device)\n",
        "    # print(\"Model Has Been Defined\")\n",
        "    # num_param = get_param_num(model)\n",
        "    # print('Number of FastSpeech2 Parameters:', num_param)\n",
        "\n",
        "    # # Load vocoder\n",
        "    # if hp.vocoder == 'vocgan':  # Load VocGAN vocoder\n",
        "    #     vocoder = get_vocgan(ckpt_path=hp.vocoder_pretrained_model_path)\n",
        "    #     vocoder.to(device)\n",
        "    # else:\n",
        "    #     raise ValueError(f\"Unsupported vocoder type: {hp.vocoder}\")\n",
        "\n",
        "    # # Initialize directories\n",
        "    # os.makedirs(hp.log_path, exist_ok=True)\n",
        "    # os.makedirs(hp.eval_path, exist_ok=True)\n",
        "\n",
        "    # # Evaluate model\n",
        "    # evaluate(model, args.step, vocoder)"
      ],
      "metadata": {
        "id": "WG2kjXVhQKs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train.py"
      ],
      "metadata": {
        "id": "aDBVRN3zQB8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=hp.train_visible_devices\n",
        "\n",
        "def main(args): #학습모드\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Get device\n",
        "    device = torch.device('cuda'if torch.cuda.is_available()else 'cpu')\n",
        "\n",
        "    # Get dataset\n",
        "    dataset = Dataset(\"train.txt\")\n",
        "    loader = DataLoader(dataset, batch_size=hp.batch_size**2, shuffle=True,\n",
        "        collate_fn=dataset.collate_fn, drop_last=True, num_workers=0)\n",
        "\n",
        "    print(len(loader))\n",
        "\n",
        "    # Define model\n",
        "    model = nn.DataParallel(FastSpeech2()).to(device)\n",
        "    print(\"Model Has Been Defined\")\n",
        "    num_param = get_param_num(model)\n",
        "    print('Number of FastSpeech2 Parameters:', num_param)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), betas=hp.betas, eps=hp.eps, weight_decay = hp.weight_decay)\n",
        "    scheduled_optim = ScheduledOptim(optimizer, hp.decoder_hidden, hp.n_warm_up_step, args.restore_step)\n",
        "    Loss = FastSpeech2Loss().to(device)\n",
        "    print(\"Optimizer and Loss Function Defined.\")\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_path = os.path.join(hp.checkpoint_path)\n",
        "    try:\n",
        "        checkpoint = torch.load(os.path.join(\n",
        "            checkpoint_path, 'checkpoint_{}.pth.tar'.format(args.restore_step)))\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"\\n---Model Restored at Step {}---\\n\".format(args.restore_step))\n",
        "    except:\n",
        "        print(\"\\n---Start New Training---\\n\")\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            os.makedirs(checkpoint_path)\n",
        "\n",
        "    # read params\n",
        "    mean_mel, std_mel = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"mel_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_f0, std_f0 = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"f0_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_energy, std_energy = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"energy_stat.npy\")), dtype=torch.float).to(device)\n",
        "\n",
        "    mean_mel, std_mel = mean_mel.reshape(1, -1), std_mel.reshape(1, -1)\n",
        "    mean_f0, std_f0 = mean_f0.reshape(1, -1), std_f0.reshape(1, -1)\n",
        "    mean_energy, std_energy = mean_energy.reshape(1, -1), std_energy.reshape(1, -1)\n",
        "\n",
        "\n",
        "    # Load vocoder\n",
        "    if hp.vocoder == 'vocgan': #보코더 로드\n",
        "        vocoder = get_vocgan(ckpt_path = hp.vocoder_pretrained_model_path)\n",
        "        vocoder.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # 보코더 지정, avocodo 끝나면 여기 넣으면 될 듯?\n",
        "    # if hp.vocoder =='avocodo':\n",
        "    #     vocoder = get_avocodo(ckpt_path = hp.vocoder_pretrained_model_path)\n",
        "    #     vocoder.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        vocoder = None\n",
        "\n",
        "    # Init logger\n",
        "    log_path = hp.log_path\n",
        "    if not os.path.exists(log_path):\n",
        "        os.makedirs(log_path)\n",
        "        os.makedirs(os.path.join(log_path, 'train'))\n",
        "        os.makedirs(os.path.join(log_path, 'validation'))\n",
        "    train_logger = SummaryWriter(os.path.join(log_path, 'train'))\n",
        "    val_logger = SummaryWriter(os.path.join(log_path, 'validation'))\n",
        "\n",
        "    # Define Some Information\n",
        "    Time = np.array([])\n",
        "    Start = time.perf_counter()\n",
        "\n",
        "    # Training\n",
        "    model = model.train()\n",
        "\n",
        "    for epoch in range(hp.epochs):\n",
        "        # Get Training Loader\n",
        "        total_step = hp.epochs * len(loader) * hp.batch_size\n",
        "\n",
        "        for i, batchs in enumerate(loader):\n",
        "            for j, data_of_batch in enumerate(batchs):\n",
        "                start_time = time.perf_counter()\n",
        "\n",
        "                current_step = i*hp.batch_size + j + args.restore_step + epoch*len(loader)*hp.batch_size + 1\n",
        "\n",
        "                # Get Data\n",
        "                text = torch.from_numpy(data_of_batch[\"text\"]).long().to(device)\n",
        "                mel_target = torch.from_numpy(data_of_batch[\"mel_target\"]).float().to(device)\n",
        "                D = torch.from_numpy(data_of_batch[\"D\"]).long().to(device)\n",
        "                log_D = torch.from_numpy(data_of_batch[\"log_D\"]).float().to(device)\n",
        "                f0 = torch.from_numpy(data_of_batch[\"f0\"]).float().to(device)\n",
        "                energy = torch.from_numpy(data_of_batch[\"energy\"]).float().to(device)\n",
        "                src_len = torch.from_numpy(data_of_batch[\"src_len\"]).long().to(device)\n",
        "                mel_len = torch.from_numpy(data_of_batch[\"mel_len\"]).long().to(device)\n",
        "                max_src_len = np.max(data_of_batch[\"src_len\"]).astype(np.int32)\n",
        "                max_mel_len = np.max(data_of_batch[\"mel_len\"]).astype(np.int32)\n",
        "\n",
        "                # Forward\n",
        "                mel_output, mel_postnet_output, log_duration_output, f0_output, energy_output, src_mask, mel_mask, _ = model(\n",
        "                    text, src_len, mel_len, D, f0, energy, max_src_len, max_mel_len)\n",
        "\n",
        "                # Cal Loss\n",
        "                mel_loss, mel_postnet_loss, d_loss, f_loss, e_loss = Loss(\n",
        "                        log_duration_output, log_D, f0_output, f0, energy_output, energy, mel_output, mel_postnet_output, mel_target, ~src_mask, ~mel_mask)\n",
        "                total_loss = mel_loss + mel_postnet_loss + d_loss + f_loss + e_loss\n",
        "\n",
        "                # Logger\n",
        "                t_l = total_loss.item()\n",
        "                m_l = mel_loss.item()\n",
        "                m_p_l = mel_postnet_loss.item()\n",
        "                d_l = d_loss.item()\n",
        "                f_l = f_loss.item()\n",
        "                e_l = e_loss.item()\n",
        "                with open(os.path.join(log_path, \"total_loss.txt\"), \"a\") as f_total_loss:\n",
        "                    f_total_loss.write(str(t_l)+\"\\n\")\n",
        "                with open(os.path.join(log_path, \"mel_loss.txt\"), \"a\") as f_mel_loss:\n",
        "                    f_mel_loss.write(str(m_l)+\"\\n\")\n",
        "                with open(os.path.join(log_path, \"mel_postnet_loss.txt\"), \"a\") as f_mel_postnet_loss:\n",
        "                    f_mel_postnet_loss.write(str(m_p_l)+\"\\n\")\n",
        "                with open(os.path.join(log_path, \"duration_loss.txt\"), \"a\") as f_d_loss:\n",
        "                    f_d_loss.write(str(d_l)+\"\\n\")\n",
        "                with open(os.path.join(log_path, \"f0_loss.txt\"), \"a\") as f_f_loss:\n",
        "                    f_f_loss.write(str(f_l)+\"\\n\")\n",
        "                with open(os.path.join(log_path, \"energy_loss.txt\"), \"a\") as f_e_loss:\n",
        "                    f_e_loss.write(str(e_l)+\"\\n\")\n",
        "\n",
        "                # Backward\n",
        "                total_loss = total_loss / hp.acc_steps\n",
        "                total_loss.backward()\n",
        "                if current_step % hp.acc_steps != 0:\n",
        "                    continue\n",
        "\n",
        "                # Clipping gradients to avoid gradient explosion\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), hp.grad_clip_thresh)\n",
        "\n",
        "                # Update weights\n",
        "                scheduled_optim.step_and_update_lr()\n",
        "                scheduled_optim.zero_grad()\n",
        "\n",
        "                # Print\n",
        "                if current_step % hp.log_step == 0:\n",
        "                    Now = time.perf_counter()\n",
        "\n",
        "                    str1 = \"Epoch [{}/{}], Step [{}/{}]:\".format(\n",
        "                        epoch+1, hp.epochs, current_step, total_step)\n",
        "                    str2 = \"Total Loss: {:.4f}, Mel Loss: {:.4f}, Mel PostNet Loss: {:.4f}, Duration Loss: {:.4f}, F0 Loss: {:.4f}, Energy Loss: {:.4f};\".format(\n",
        "                        t_l, m_l, m_p_l, d_l, f_l, e_l)\n",
        "                    str3 = \"Time Used: {:.3f}s, Estimated Time Remaining: {:.3f}s.\".format(\n",
        "                        (Now-Start), (total_step-current_step)*np.mean(Time))\n",
        "\n",
        "                    print(\"\\n\" + str1)\n",
        "                    print(str2)\n",
        "                    print(str3)\n",
        "\n",
        "                    with open(os.path.join(log_path, \"log.txt\"), \"a\") as f_log:\n",
        "                        f_log.write(str1 + \"\\n\")\n",
        "                        f_log.write(str2 + \"\\n\")\n",
        "                        f_log.write(str3 + \"\\n\")\n",
        "                        f_log.write(\"\\n\")\n",
        "\n",
        "                train_logger.add_scalar('Loss/total_loss', t_l, current_step)\n",
        "                train_logger.add_scalar('Loss/mel_loss', m_l, current_step)\n",
        "                train_logger.add_scalar('Loss/mel_postnet_loss', m_p_l, current_step)\n",
        "                train_logger.add_scalar('Loss/duration_loss', d_l, current_step)\n",
        "                train_logger.add_scalar('Loss/F0_loss', f_l, current_step)\n",
        "                train_logger.add_scalar('Loss/energy_loss', e_l, current_step)\n",
        "\n",
        "                if current_step % hp.save_step == 0:\n",
        "                    torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(\n",
        "                    )}, os.path.join(checkpoint_path, 'checkpoint_{}.pth.tar'.format(current_step)))\n",
        "                    print(\"save model at step {} ...\".format(current_step))\n",
        "\n",
        "                if current_step % hp.eval_step == 0:\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        d_l, f_l, e_l, m_l, m_p_l = evaluate(model, current_step, vocoder)\n",
        "                        t_l = d_l + f_l + e_l + m_l + m_p_l\n",
        "\n",
        "                        val_logger.add_scalar('Loss/total_loss', t_l, current_step)\n",
        "                        val_logger.add_scalar('Loss/mel_loss', m_l, current_step)\n",
        "                        val_logger.add_scalar('Loss/mel_postnet_loss', m_p_l, current_step)\n",
        "                        val_logger.add_scalar('Loss/duration_loss', d_l, current_step)\n",
        "                        val_logger.add_scalar('Loss/F0_loss', f_l, current_step)\n",
        "                        val_logger.add_scalar('Loss/energy_loss', e_l, current_step)\n",
        "\n",
        "                    model.train()\n",
        "\n",
        "                end_time = time.perf_counter()\n",
        "                Time = np.append(Time, end_time - start_time)\n",
        "                if len(Time) == hp.clear_Time:\n",
        "                    temp_value = np.mean(Time)\n",
        "                    Time = np.delete(\n",
        "                        Time, [i for i in range(len(Time))], axis=None)\n",
        "                    Time = np.append(Time, temp_value)\n"
      ],
      "metadata": {
        "id": "4yl95YkOQDi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": #학습 시작\n",
        "      main(hp)"
      ],
      "metadata": {
        "id": "BveBs8RUUrdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#synthesis.py"
      ],
      "metadata": {
        "id": "OqUPlV6pQUaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def kor_preprocess(text):\n",
        "    text = text.rstrip(punctuation)\n",
        "\n",
        "    g2p=G2p()\n",
        "    phone = g2p(text)\n",
        "    print('after g2p: ',phone)\n",
        "    phone = h2j(phone)\n",
        "    print('after h2j: ',phone)\n",
        "    phone = list(filter(lambda p: p != ' ', phone))\n",
        "    phone = '{' + '}{'.join(phone) + '}'\n",
        "    print('phone: ',phone)\n",
        "    phone = re.sub(r'\\{[^\\w\\s]?\\}', '{sp}', phone)\n",
        "    print('after re.sub: ',phone)\n",
        "    phone = phone.replace('}{', ' ')\n",
        "\n",
        "    print('|' + phone + '|')\n",
        "    sequence = np.array(text_to_sequence(phone,hp.text_cleaners))\n",
        "    sequence = np.stack([sequence])\n",
        "    return torch.from_numpy(sequence).long().to(device)\n",
        "\n",
        "def get_FastSpeech2(num):\n",
        "    checkpoint_path = os.path.join(hp.checkpoint_path, \"checkpoint_{}.pth.tar\".format(num))\n",
        "    model = nn.DataParallel(FastSpeech2())\n",
        "    model.load_state_dict(torch.load(checkpoint_path)['model'])\n",
        "    model.requires_grad = False\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def synthesize(model, vocoder, text, sentence, prefix=''):\n",
        "    sentence = sentence[:10] # long filename will result in OS Error\n",
        "\n",
        "    mean_mel, std_mel = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"mel_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_f0, std_f0 = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"f0_stat.npy\")), dtype=torch.float).to(device)\n",
        "    mean_energy, std_energy = torch.tensor(np.load(os.path.join(hp.preprocessed_path, \"energy_stat.npy\")), dtype=torch.float).to(device)\n",
        "\n",
        "    mean_mel, std_mel = mean_mel.reshape(1, -1), std_mel.reshape(1, -1)\n",
        "    mean_f0, std_f0 = mean_f0.reshape(1, -1), std_f0.reshape(1, -1)\n",
        "    mean_energy, std_energy = mean_energy.reshape(1, -1), std_energy.reshape(1, -1)\n",
        "\n",
        "    src_len = torch.from_numpy(np.array([text.shape[1]])).to(device)\n",
        "\n",
        "    mel, mel_postnet, log_duration_output, f0_output, energy_output, _, _, mel_len = model(text, src_len)\n",
        "\n",
        "    mel_torch = mel.transpose(1, 2).detach()\n",
        "    mel_postnet_torch = mel_postnet.transpose(1, 2).detach()\n",
        "    f0_output = f0_output[0]\n",
        "    energy_output = energy_output[0]\n",
        "\n",
        "    mel_torch = de_norm(mel_torch.transpose(1, 2), mean_mel, std_mel)\n",
        "    mel_postnet_torch = de_norm(mel_postnet_torch.transpose(1, 2), mean_mel, std_mel).transpose(1, 2)\n",
        "    f0_output = de_norm(f0_output, mean_f0, std_f0).squeeze().detach().cpu().numpy()\n",
        "    energy_output = de_norm(energy_output, mean_energy, std_energy).squeeze().detach().cpu().numpy()\n",
        "\n",
        "    if not os.path.exists(hp.test_path):\n",
        "        os.makedirs(hp.test_path)\n",
        "\n",
        "    tools.inv_mel_spec(mel_postnet_torch[0], os.path.join(hp.test_path, '{}_griffin_lim_{}.wav'.format(prefix, sentence)))\n",
        "\n",
        "    if vocoder is not None:\n",
        "        if hp.vocoder.lower() == \"vocgan\":\n",
        "            vocgan_infer(mel_postnet_torch, vocoder, path=os.path.join(hp.test_path, '{}_{}_{}.wav'.format(prefix, hp.vocoder, sentence)))\n",
        "\n",
        "    plot_data([(mel_postnet_torch[0].detach().cpu().numpy(), f0_output, energy_output)], ['Synthesized Spectrogram'], filename=os.path.join(hp.test_path, '{}_{}.png'.format(prefix, sentence)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--step', type=int, default=30000)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "    model = get_FastSpeech2(args.step).to(device)\n",
        "    if hp.vocoder == 'vocgan':\n",
        "        vocoder = get_vocgan(ckpt_path=hp.vocoder_pretrained_model_path)\n",
        "        # 보코더 지정\n",
        "    if hp.vocoder =='avocodo':\n",
        "        vocoder = utils.get_avocodo(ckpt_path = hp.vocoder_pretrained_model_path)\n",
        "        vocoder.to(device)\n",
        "    else:\n",
        "        vocoder = None\n",
        "\n",
        "    #kss\n",
        "    eval_sentence=['그는 괜찮은 척하려고 애쓰는 것 같았다','그녀의 사랑을 얻기 위해 애썼지만 헛수고였다','용돈을 아껴써라','그는 아내를 많이 아낀다','요즘 공부가 안돼요','한 여자가 내 옆에 앉았다']\n",
        "    train_sentence=['가까운 시일 내에 한번, 댁으로 찾아가겠습니다','우리의 승리는 기적에 가까웠다','아이들의 얼굴에는 행복한 미소가 가득했다','헬륨은 공기보다 가볍다','이것은 간단한 문제가 아니다']\n",
        "    test_sentence=['안녕하세요, 한동대학교 딥러닝 연구실입니다.', '이 프로젝트가 여러분에게 도움이 되었으면 좋겠습니다.', '시간이 촉박해요','이런, 큰일 났어','좀 더 먹지 그래?','제가 뭘 잘못했죠?','더 이상 묻지마']\n",
        "\n",
        "    g2p=G2p()\n",
        "    print('which sentence do you want?')\n",
        "    print('1.eval_sentence 2.train_sentence 3.test_sentence 4.create new sentence')\n",
        "\n",
        "    mode=input()\n",
        "    print('you went for mode {}'.format(mode))\n",
        "    if mode=='4':\n",
        "        print('input sentence')\n",
        "        sentence = input()\n",
        "    elif mode=='1':\n",
        "        sentence = eval_sentence\n",
        "    elif mode=='2':\n",
        "        sentence = train_sentence\n",
        "    elif mode=='3':\n",
        "        sentence = test_sentence\n",
        "\n",
        "    print('sentence that will be synthesized: ')\n",
        "    print(sentence)\n",
        "    if mode != '4':\n",
        "        for s in sentence:\n",
        "            text = kor_preprocess(s)\n",
        "            synthesize(model, vocoder, text, s, prefix='step_{}'.format(args.step))\n",
        "    else:\n",
        "        text = kor_preprocess(sentence)\n",
        "        synthesize(model, vocoder, text, sentence, prefix='step_{}'.format(args.step))"
      ],
      "metadata": {
        "id": "5ryqqI6NQFdr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}