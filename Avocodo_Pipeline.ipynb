{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5S7waehhoSRy",
        "LuKV0hhkiCFp",
        "9owc_vcWhiAC",
        "98DxXGMdhmT5",
        "zSQ3Q0mqht0K",
        "NPeFVq4_hzq6",
        "4ttjqPrPh4sC",
        "MfcMzi-hiajZ",
        "BUyxi8fxiHbD",
        "PGTk_rGDiPHB"
      ],
      "authorship_tag": "ABX9TyPwuxqb/bk3oHEi0FV2mzW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avocodo 사전 설정\n"
      ],
      "metadata": {
        "id": "5S7waehhoSRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE6aC-JGFmw_",
        "outputId": "3b4ae005-1764-492e-ce17-e7a683b3930c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlbN1YmJkwVm",
        "outputId": "7d54487f-8cc9-45fc-da4f-e08163a1ec51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OmegaConf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwxv46aFl6tl",
        "outputId": "fa2e1343-9118-4434-8855-174ec883dbe9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: OmegaConf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "from scipy.signal.windows import kaiser\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning import LightningModule\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#meldataset.py"
      ],
      "metadata": {
        "id": "LuKV0hhkiCFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Hkgki34ooeUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mel_basis = {}\n",
        "hann_window = {}\n"
      ],
      "metadata": {
        "id": "SKv2Mxbvoiba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ],
      "metadata": {
        "id": "bdi0Xs8BohoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_filelist(\n",
        "    input_wavs_dir,\n",
        "    input_training_file,\n",
        "    input_validation_file\n",
        "):\n",
        "    with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n"
      ],
      "metadata": {
        "id": "NcaNp8vHokMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split\n",
        "        self.n_fft = n_fft\n",
        "        self.num_mels = num_mels\n",
        "        self.hop_size = hop_size\n",
        "        self.win_size = win_size\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "        self.fmax_loss = fmax_loss\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.fine_tuning = fine_tuning\n",
        "        self.base_mels_path = base_mels_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename)\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning:\n",
        "                audio = normalize(audio) * 0.95\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate:\n",
        "                raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio)\n",
        "        audio = audio.unsqueeze(0)\n",
        "\n",
        "        if not self.fine_tuning:\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "        else:\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "\n",
        "            if len(mel.shape) < 3:\n",
        "                mel = mel.unsqueeze(0)\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n",
        "\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ],
      "metadata": {
        "id": "MDcNWVMHiD3R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py\n"
      ],
      "metadata": {
        "id": "9owc_vcWhiAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)"
      ],
      "metadata": {
        "id": "KxozRiQYhj-y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#losses.py\n"
      ],
      "metadata": {
        "id": "98DxXGMdhmT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            _loss = torch.mean(torch.abs(rl - gl))\n",
        "            loss += _loss\n",
        "        losses.append(_loss)\n",
        "\n",
        "    return loss*2, losses\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ],
      "metadata": {
        "id": "8VpuHS2KhoVJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CoMBD.py"
      ],
      "metadata": {
        "id": "zSQ3Q0mqht0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoMBDBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        h_u: List[int],\n",
        "        d_k: List[int],\n",
        "        d_s: List[int],\n",
        "        d_d: List[int],\n",
        "        d_g: List[int],\n",
        "        d_p: List[int],\n",
        "        op_f: int,\n",
        "        op_k: int,\n",
        "        op_g: int,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(CoMBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters = [[1, h_u[0]]]\n",
        "        for i in range(len(h_u) - 1):\n",
        "            filters.append([h_u[i], h_u[i + 1]])\n",
        "        for _f, _k, _s, _d, _g, _p in zip(filters, d_k, d_s, d_d, d_g, d_p):\n",
        "            self.convs.append(norm_f(\n",
        "                Conv1d(\n",
        "                    in_channels=_f[0],\n",
        "                    out_channels=_f[1],\n",
        "                    kernel_size=_k,\n",
        "                    stride=_s,\n",
        "                    dilation=_d,\n",
        "                    groups=_g,\n",
        "                    padding=_p\n",
        "                )\n",
        "            ))\n",
        "        self.projection_conv = norm_f(\n",
        "            Conv1d(\n",
        "                in_channels=filters[-1][1],\n",
        "                out_channels=op_f,\n",
        "                kernel_size=op_k,\n",
        "                groups=op_g\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for block in self.convs:\n",
        "            x = block(x)\n",
        "            x = F.leaky_relu(x, 0.2)\n",
        "            fmap.append(x)\n",
        "        x = self.projection_conv(x)\n",
        "        return x, fmap\n"
      ],
      "metadata": {
        "id": "HgOcC75wouAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoMBD(torch.nn.Module):\n",
        "    def __init__(self, h, pqmf_list=None, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        self.h = h\n",
        "        if pqmf_list is not None:\n",
        "            self.pqmf = pqmf_list\n",
        "        else:\n",
        "            self.pqmf = [\n",
        "                PQMF(*h.pqmf_config[\"lv2\"]),\n",
        "                PQMF(*h.pqmf_config[\"lv1\"])\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for _h_u, _d_k, _d_s, _d_d, _d_g, _d_p, _op_f, _op_k, _op_g in zip(\n",
        "            h.combd_h_u,\n",
        "            h.combd_d_k,\n",
        "            h.combd_d_s,\n",
        "            h.combd_d_d,\n",
        "            h.combd_d_g,\n",
        "            h.combd_d_p,\n",
        "            h.combd_op_f,\n",
        "            h.combd_op_k,\n",
        "            h.combd_op_g,\n",
        "        ):\n",
        "            self.blocks.append(CoMBDBlock(\n",
        "                _h_u,\n",
        "                _d_k,\n",
        "                _d_s,\n",
        "                _d_d,\n",
        "                _d_g,\n",
        "                _d_p,\n",
        "                _op_f,\n",
        "                _op_k,\n",
        "                _op_g,\n",
        "            ))\n",
        "\n",
        "    def _block_forward(self, input, blocks, outs, f_maps):\n",
        "        for x, block in zip(input, blocks):\n",
        "            out, f_map = block(x)\n",
        "            outs.append(out)\n",
        "            f_maps.append(f_map)\n",
        "        return outs, f_maps\n",
        "\n",
        "    def _pqmf_forward(self, ys, ys_hat):\n",
        "        # preprocess for multi_scale forward\n",
        "        multi_scale_inputs = []\n",
        "        multi_scale_inputs_hat = []\n",
        "        for pqmf in self.pqmf:\n",
        "            multi_scale_inputs.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys[-1])[:, :1, :]\n",
        "            )\n",
        "            multi_scale_inputs_hat.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys_hat[-1])[:, :1, :]\n",
        "            )\n",
        "\n",
        "        outs_real = []\n",
        "        f_maps_real = []\n",
        "        # real\n",
        "        # for hierarchical forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            ys, self.blocks, outs_real, f_maps_real)\n",
        "        # for multi_scale forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            multi_scale_inputs, self.blocks[:-1], outs_real, f_maps_real)\n",
        "\n",
        "        outs_fake = []\n",
        "        f_maps_fake = []\n",
        "        # predicted\n",
        "        # for hierarchical forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            ys_hat, self.blocks, outs_fake, f_maps_fake)\n",
        "        # for multi_scale forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            multi_scale_inputs_hat, self.blocks[:-1], outs_fake, f_maps_fake)\n",
        "\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake\n",
        "\n",
        "    def forward(self, ys, ys_hat):\n",
        "        outs_real, outs_fake, f_maps_real, f_maps_fake = self._pqmf_forward(\n",
        "            ys, ys_hat)\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake"
      ],
      "metadata": {
        "id": "7-ga7kQJhs-0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SBD.py\n"
      ],
      "metadata": {
        "id": "NPeFVq4_hzq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDC(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.d_convs = nn.ModuleList()\n",
        "        for _k, _d in zip(kernel_size, dilations):\n",
        "            self.d_convs.append(\n",
        "                norm_f(Conv1d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=_k,\n",
        "                    dilation=_d,\n",
        "                    padding=get_padding(_k, _d)\n",
        "                ))\n",
        "            )\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=strides,\n",
        "            padding=get_padding(_k, _d)\n",
        "        ))\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _out = None\n",
        "        for _l in self.d_convs:\n",
        "            _x = torch.unsqueeze(_l(x), -1)\n",
        "            _x = F.leaky_relu(_x, 0.2)\n",
        "            if _out is None:\n",
        "                _out = _x\n",
        "            else:\n",
        "                _out = torch.cat([_out, _x], axis=-1)\n",
        "        x = torch.sum(_out, dim=-1)\n",
        "        x = self.post_conv(x)\n",
        "        x = F.leaky_relu(x, 0.2)  # @@\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "OBMUu1GRozy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SBDBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        segment_dim,\n",
        "        strides,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(SBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters_in_out = [(segment_dim, filters[0])]\n",
        "        for i in range(len(filters) - 1):\n",
        "            filters_in_out.append([filters[i], filters[i + 1]])\n",
        "\n",
        "        for _s, _f, _k, _d in zip(\n",
        "            strides,\n",
        "            filters_in_out,\n",
        "            kernel_size,\n",
        "            dilations\n",
        "        ):\n",
        "            self.convs.append(MDC(\n",
        "                in_channels=_f[0],\n",
        "                out_channels=_f[1],\n",
        "                strides=_s,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=_f[1],\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=3 // 2\n",
        "        ))  # @@\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for _l in self.convs:\n",
        "            x = _l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.post_conv(x)  # @@\n",
        "\n",
        "        return x, fmap\n",
        "\n"
      ],
      "metadata": {
        "id": "i2OaM69uh0_5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDCDConfig:\n",
        "    def __init__(self, h):\n",
        "        self.pqmf_params = h.pqmf_config[\"sbd\"]\n",
        "        self.f_pqmf_params = h.pqmf_config[\"fsbd\"]\n",
        "        self.filters = h.sbd_filters\n",
        "        self.kernel_sizes = h.sbd_kernel_sizes\n",
        "        self.dilations = h.sbd_dilations\n",
        "        self.strides = h.sbd_strides\n",
        "        self.band_ranges = h.sbd_band_ranges\n",
        "        self.transpose = h.sbd_transpose\n",
        "        self.segment_size = h.segment_size"
      ],
      "metadata": {
        "id": "wuGqG0y4o4oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SBD(torch.nn.Module):\n",
        "    def __init__(self, h, use_spectral_norm=False):\n",
        "        super(SBD, self).__init__()\n",
        "        self.config = MDCDConfig(h)\n",
        "        self.pqmf = PQMF(\n",
        "            *self.config.pqmf_params\n",
        "        )\n",
        "        if True in h.sbd_transpose:\n",
        "            self.f_pqmf = PQMF(\n",
        "                *self.config.f_pqmf_params\n",
        "            )\n",
        "        else:\n",
        "            self.f_pqmf = None\n",
        "\n",
        "        self.discriminators = torch.nn.ModuleList()\n",
        "\n",
        "        for _f, _k, _d, _s, _br, _tr in zip(\n",
        "            self.config.filters,\n",
        "            self.config.kernel_sizes,\n",
        "            self.config.dilations,\n",
        "            self.config.strides,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if _tr:\n",
        "                segment_dim = self.config.segment_size // _br[1] - _br[0]\n",
        "            else:\n",
        "                segment_dim = _br[1] - _br[0]\n",
        "\n",
        "            self.discriminators.append(SBDBlock(\n",
        "                segment_dim=segment_dim,\n",
        "                filters=_f,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                strides=_s,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        y_in = self.pqmf.analysis(y)\n",
        "        y_hat_in = self.pqmf.analysis(y_hat)\n",
        "        if self.f_pqmf is not None:\n",
        "            y_in_f = self.f_pqmf.analysis(y)\n",
        "            y_hat_in_f = self.f_pqmf.analysis(y_hat)\n",
        "\n",
        "        for d, br, tr in zip(\n",
        "            self.discriminators,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if tr:\n",
        "                _y_in = y_in_f[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in_f[:, br[0]:br[1], :]\n",
        "                _y_in = torch.transpose(_y_in, 1, 2)\n",
        "                _y_hat_in = torch.transpose(_y_hat_in, 1, 2)\n",
        "            else:\n",
        "                _y_in = y_in[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in[:, br[0]:br[1], :]\n",
        "            y_d_r, fmap_r = d(_y_in)\n",
        "            y_d_g, fmap_g = d(_y_hat_in)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs"
      ],
      "metadata": {
        "id": "2H-kRPzzo3dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#generator.py\n"
      ],
      "metadata": {
        "id": "4ttjqPrPh4sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.2)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, 0.2)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for _l in self.convs1:\n",
        "            remove_weight_norm(_l)\n",
        "        for _l in self.convs2:\n",
        "            remove_weight_norm(_l)\n"
      ],
      "metadata": {
        "id": "HHcgmHJEo90g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.resblock = h.resblock\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            _ups = nn.ModuleList()\n",
        "            for _i, (_u, _k) in enumerate(zip(u, k)):\n",
        "                in_channel = h.upsample_initial_channel // (2**i)\n",
        "                out_channel = h.upsample_initial_channel // (2**(i + 1))\n",
        "                _ups.append(weight_norm(\n",
        "                    ConvTranspose1d(in_channel, out_channel, _k, _u, padding=(_k - _u) // 2)))\n",
        "            self.ups.append(_ups)\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        self.conv_post = nn.ModuleList()\n",
        "        for i in range(self.num_upsamples):\n",
        "            ch = h.upsample_initial_channel // (2**(i + 1))\n",
        "            temp = nn.ModuleList()\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                temp.append(resblock(h, ch, k, d))\n",
        "            self.resblocks.append(temp)\n",
        "\n",
        "            if self.h.projection_filters[i] != 0:\n",
        "                self.conv_post.append(\n",
        "                    weight_norm(\n",
        "                        Conv1d(\n",
        "                            ch, self.h.projection_filters[i],\n",
        "                            self.h.projection_kernels[i], 1, padding=self.h.projection_kernels[i] // 2\n",
        "                        )))\n",
        "            else:\n",
        "                self.conv_post.append(torch.nn.Identity())\n",
        "\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.conv_pre(x)\n",
        "        for i, (ups, resblocks, conv_post) in enumerate(zip(self.ups, self.resblocks, self.conv_post)):\n",
        "            x = F.leaky_relu(x, 0.2)\n",
        "            for _ups in ups:\n",
        "                x = _ups(x)\n",
        "            xs = None\n",
        "            for j, resblock in enumerate(resblocks):\n",
        "                if xs is None:\n",
        "                    xs = resblock(x)\n",
        "                else:\n",
        "                    xs += resblock(x)\n",
        "            x = xs / self.num_kernels\n",
        "            if i >= (self.num_upsamples-3):\n",
        "                _x = F.leaky_relu(x)\n",
        "                _x = conv_post(_x)\n",
        "                _x = torch.tanh(_x)\n",
        "                outs.append(_x)\n",
        "            else:\n",
        "                x = conv_post(x)\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for ups in self.ups:\n",
        "            for _l in ups:\n",
        "                remove_weight_norm(_l)\n",
        "        for resblock in self.resblocks:\n",
        "            for _l in resblock:\n",
        "                _l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        for _l in self.conv_post:\n",
        "            if not isinstance(_l, torch.nn.Identity):\n",
        "                remove_weight_norm(_l)"
      ],
      "metadata": {
        "id": "TUBqwfZgh3dx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pqmf.py"
      ],
      "metadata": {
        "id": "MfcMzi-hiajZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def design_prototype_filter(taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "    \"\"\"Design prototype filter for PQMF.\n",
        "    This method is based on `A Kaiser window approach for the design of prototype\n",
        "    filters of cosine modulated filterbanks`_.\n",
        "    Args:\n",
        "        taps (int): The number of filter taps.\n",
        "        cutoff_ratio (float): Cut-off frequency ratio.\n",
        "        beta (float): Beta coefficient for kaiser window.\n",
        "    Returns:\n",
        "        ndarray: Impluse response of prototype filter (taps + 1,).\n",
        "    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n",
        "        https://ieeexplore.ieee.org/abstract/document/681427\n",
        "    \"\"\"\n",
        "    # check the arguments are valid\n",
        "    assert taps % 2 == 0, \"The number of taps mush be even number.\"\n",
        "    assert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\"\n",
        "\n",
        "    # make initial filter\n",
        "    omega_c = np.pi * cutoff_ratio\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n",
        "            np.pi * (np.arange(taps + 1) - 0.5 * taps)\n",
        "        )\n",
        "    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # fix nan due to indeterminate form\n",
        "\n",
        "    # apply kaiser window\n",
        "    w = kaiser(taps + 1, beta)\n",
        "    h = h_i * w\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "ZAzKgcDTpCEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PQMF(torch.nn.Module):\n",
        "    \"\"\"PQMF module.\n",
        "    This module is based on `Near-perfect-reconstruction pseudo-QMF banks`_.\n",
        "    .. _`Near-perfect-reconstruction pseudo-QMF banks`:\n",
        "        https://ieeexplore.ieee.org/document/258122\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "        \"\"\"Initilize PQMF module.\n",
        "        The cutoff_ratio and beta parameters are optimized for #subbands = 4.\n",
        "        See dicussion in https://github.com/kan-bayashi/ParallelWaveGAN/issues/195.\n",
        "        Args:\n",
        "            subbands (int): The number of subbands.\n",
        "            taps (int): The number of filter taps.\n",
        "            cutoff_ratio (float): Cut-off frequency ratio.\n",
        "            beta (float): Beta coefficient for kaiser window.\n",
        "        \"\"\"\n",
        "        super(PQMF, self).__init__()\n",
        "\n",
        "        # build analysis & synthesis filter coefficients\n",
        "        h_proto = design_prototype_filter(taps, cutoff_ratio, beta)\n",
        "        h_analysis = np.zeros((subbands, len(h_proto)))\n",
        "        h_synthesis = np.zeros((subbands, len(h_proto)))\n",
        "        for k in range(subbands):\n",
        "            h_analysis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    + (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "            h_synthesis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    - (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # convert to tensor\n",
        "        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1)\n",
        "        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n",
        "\n",
        "        # register coefficients as beffer\n",
        "        self.register_buffer(\"analysis_filter\", analysis_filter)\n",
        "        self.register_buffer(\"synthesis_filter\", synthesis_filter)\n",
        "\n",
        "        # filter for downsampling & upsampling\n",
        "        updown_filter = torch.zeros((subbands, subbands, subbands)).float()\n",
        "        for k in range(subbands):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.subbands = subbands\n",
        "\n",
        "        # keep padding info\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n",
        "\n",
        "    def analysis(self, x):\n",
        "        \"\"\"Analysis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, 1, T).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, subbands, T // subbands).\n",
        "        \"\"\"\n",
        "        x = F.conv1d(self.pad_fn(x), self.analysis_filter)\n",
        "        return F.conv1d(x, self.updown_filter, stride=self.subbands)\n",
        "\n",
        "    def synthesis(self, x):\n",
        "        \"\"\"Synthesis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, subbands, T // subbands).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, 1, T).\n",
        "        \"\"\"\n",
        "        # NOTE(kan-bayashi): Power will be dreased so here multipy by # subbands.\n",
        "        #   Not sure this is the correct way, it is better to check again.\n",
        "        # TODO(kan-bayashi): Understand the reconstruction procedure\n",
        "        x = F.conv_transpose1d(\n",
        "            x, self.updown_filter * self.subbands, stride=self.subbands\n",
        "        )\n",
        "        return F.conv1d(self.pad_fn(x), self.synthesis_filter)"
      ],
      "metadata": {
        "id": "gob2R_Voicxx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data_module.py"
      ],
      "metadata": {
        "id": "BUyxi8fxiHbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AvocodoDataConfig:\n",
        "    segment_size: int\n",
        "    num_mels: int\n",
        "    num_freq: int\n",
        "    sampling_rate: int\n",
        "    n_fft: int\n",
        "    hop_size: int\n",
        "    win_size: int\n",
        "    fmin: int\n",
        "    fmax: int\n",
        "    batch_size: int\n",
        "    num_workers: int\n",
        "\n",
        "    fine_tuning: bool\n",
        "    base_mels_path: str\n",
        "\n",
        "    input_wavs_dir: str\n",
        "    input_mels_dir: str\n",
        "    input_training_file: str\n",
        "    input_validation_file: str"
      ],
      "metadata": {
        "id": "2RVIsixRpGYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AvocodoData(LightningDataModule):\n",
        "    def __init__(self, h: AvocodoDataConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        '''\n",
        "            download and prepare data\n",
        "        '''\n",
        "        self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "            self.hparams.input_wavs_dir,\n",
        "            self.hparams.input_training_file,\n",
        "            self.hparams.input_validation_file\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.trainset = MelDataset(\n",
        "            self.training_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.trainset,\n",
        "            num_workers=self.hparams.num_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "    @rank_zero_only\n",
        "    def val_dataloader(self):\n",
        "        validset = MelDataset(\n",
        "            self.validation_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            False,\n",
        "            False,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "        return DataLoader(validset, num_workers=self.hparams.num_workers, shuffle=False,\n",
        "                          sampler=None,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)"
      ],
      "metadata": {
        "id": "R3ikwJZniJKo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lightning_module.py"
      ],
      "metadata": {
        "id": "PGTk_rGDiPHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Avocodo(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        h\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "        self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])\n",
        "        self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "        self.generator = Generator(self.hparams.generator)\n",
        "        self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])\n",
        "        self.sbd = SBD(self.hparams.sbd)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        h = self.hparams.optimizer\n",
        "        opt_g = torch.optim.AdamW(self.generator.parameters(\n",
        "        ), h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "        opt_d = torch.optim.AdamW(itertools.chain(self.combd.parameters(), self.sbd.parameters()),\n",
        "                                  h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)[-1]\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        y = y.unsqueeze(1)\n",
        "        ys = [\n",
        "            self.pqmf_lv2.analysis(\n",
        "                y\n",
        "            )[:, :self.hparams.generator.projection_filters[1]],\n",
        "            self.pqmf_lv1.analysis(\n",
        "                y\n",
        "            )[:, :self.hparams.generator.projection_filters[2]],\n",
        "            y\n",
        "        ]\n",
        "\n",
        "        y_g_hats = self.generator(x)\n",
        "\n",
        "        # train generator\n",
        "        if optimizer_idx == 0:\n",
        "            y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(\n",
        "                ys, y_g_hats)\n",
        "            loss_fm_u, losses_fm_u = feature_loss(fmap_u_r, fmap_u_g)\n",
        "            loss_gen_u, losses_gen_u = generator_loss(y_du_hat_g)\n",
        "\n",
        "            y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(\n",
        "                y, y_g_hats[-1])\n",
        "            loss_fm_s, losses_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n",
        "            loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n",
        "\n",
        "            # L1 Mel-Spectrogram Loss\n",
        "            y_g_hat_mel = mel_spectrogram(\n",
        "                y_g_hats[-1].squeeze(1),\n",
        "                self.hparams.audio.n_fft,\n",
        "                self.hparams.audio.num_mels,\n",
        "                self.hparams.audio.sampling_rate,\n",
        "                self.hparams.audio.hop_size,\n",
        "                self.hparams.audio.win_size,\n",
        "                self.hparams.audio.fmin,\n",
        "                self.hparams.audio.fmax_for_loss\n",
        "            )\n",
        "            loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "            self.log(\"train/l1_loss\", loss_mel, prog_bar=True)\n",
        "            loss_mel = loss_mel * self.hparams.loss_scale_mel\n",
        "\n",
        "            g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel\n",
        "\n",
        "            self.log(\"train/g_loss\", g_loss, prog_bar=True)\n",
        "            loss = g_loss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            detached_y_g_hats = [x.detach() for x in y_g_hats]\n",
        "\n",
        "            y_du_hat_r, y_du_hat_g, _, _ = self.combd(\n",
        "                ys, detached_y_g_hats)\n",
        "            loss_disc_u, losses_disc_u_r, losses_disc_u_g = discriminator_loss(\n",
        "                y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "            y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "            loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(\n",
        "                y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "            d_loss = loss_disc_s + loss_disc_u\n",
        "            self.log(\"train/d_loss\", d_loss, prog_bar=True)\n",
        "            loss = d_loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        y_g_hat = self(x)\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hat.squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        val_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "        self.logger.experiment.add_audio(\n",
        "            f'pred/{batch_idx}', y_g_hat.squeeze(), self.current_epoch, self.hparams.audio.sampling_rate)\n",
        "        self.logger.experiment.add_audio(\n",
        "            f'gt/{batch_idx}', y[0].squeeze(), self.current_epoch, self.hparams.audio.sampling_rate)\n",
        "        return val_loss\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        val_loss = torch.mean(torch.stack(validation_step_outputs))\n",
        "        self.log(\"validation/l1_loss\", val_loss, prog_bar=False)"
      ],
      "metadata": {
        "id": "HsaB8xZGiOeh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference.py"
      ],
      "metadata": {
        "id": "_32FgGv8ihNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "h = None\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(\n",
        "        x,\n",
        "        1024,\n",
        "        80,\n",
        "        22050,\n",
        "        256,\n",
        "        1024,\n",
        "        0,\n",
        "        8000\n",
        "    )\n"
      ],
      "metadata": {
        "id": "6baN8L8CmXVM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def inference(a, conf):#추론 config 로그 저장\n",
        "    avocodo = Avocodo.load_from_checkpoint(\n",
        "        f\"{a.checkpoint_path}/version_{a.version}/checkpoints/{a.checkpoint_file_id}\",\n",
        "        map_location='cpu'\n",
        "    )\n",
        "    avocodo_data = AvocodoData(conf.audio)\n",
        "    avocodo_data.prepare_data()\n",
        "    validation_dataloader = avocodo_data.val_dataloader()\n",
        "\n",
        "    output_path = f'{a.output_dir}/version_{a.version}/'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    avocodo.generator.to(a.device)\n",
        "    avocodo.generator.remove_weight_norm()\n",
        "\n",
        "    m = torch.jit.script(avocodo.generator)\n",
        "    torch.jit.save(\n",
        "        m,\n",
        "        os.path.join(output_path, \"scripted.pt\")\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(validation_dataloader):\n",
        "            mels, _, file_ids, _ = batch\n",
        "\n",
        "            y_g_hat = avocodo(mels.to(a.device))\n",
        "\n",
        "            for _y_g_hat, file_id in zip(y_g_hat, file_ids):\n",
        "                audio = _y_g_hat.squeeze(0)\n",
        "                audio = audio * MAX_WAV_VALUE\n",
        "                audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "                output_file = os.path.join(\n",
        "                    output_path,\n",
        "                    file_id.split('/')[-1]\n",
        "                )\n",
        "                print(file_id)\n",
        "                write(output_file, conf.audio.sampling_rate, audio)\n",
        "    print('Done inference')\n"
      ],
      "metadata": {
        "id": "o6rbIWAomjLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--checkpoint_path', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint')\n",
        "    parser.add_argument('--version', type=int, required=True)\n",
        "    parser.add_argument('--checkpoint_file_id', type=str, default='', required=True)\n",
        "    parser.add_argument('--output_dir', type=str, default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result')\n",
        "    parser.add_argument('--script', type=bool, default=True)\n",
        "    parser.add_argument('--device', type=str, default='cuda')\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    conf = OmegaConf.load(os.path.join(a.checkpoint_path, f\"version_{a.version}\", \"hparams.yaml\"))\n",
        "    inference(a, conf)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "69y4YDJZiilp",
        "outputId": "3aa7ed15-f211-472b-f523-a27823728af4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Inference Process..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--checkpoint_path CHECKPOINT_PATH] --version VERSION\n",
            "                                --checkpoint_file_id CHECKPOINT_FILE_ID [--output_dir OUTPUT_DIR]\n",
            "                                [--script SCRIPT] [--device DEVICE]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --version, --checkpoint_file_id\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "cn0baVSmmlsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}