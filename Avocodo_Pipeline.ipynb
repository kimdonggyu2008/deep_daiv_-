{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRJ8/qb2hdBk+R3d9VhbzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE6aC-JGFmw_",
        "outputId": "856ee205-934e-488e-b20c-d523a14198d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#env.py"
      ],
      "metadata": {
        "id": "JWndV1ACOTrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 사전 설정\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def build_env(config, config_name, path):\n",
        "    t_path = os.path.join(path, config_name)\n",
        "    if config != t_path:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        shutil.copyfile(config, os.path.join(path, config_name))"
      ],
      "metadata": {
        "id": "xB4DIgZ9OVLm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#meldataset.py\n"
      ],
      "metadata": {
        "id": "kS16ZNKaN4DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#멜 데이터 로딩 및 전처리\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "mel_basis = {}\n",
        "hann_window = {}\n",
        "\n",
        "\n",
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n",
        "\n",
        "\n",
        "def get_dataset_filelist(a):\n",
        "    with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n",
        "\n",
        "\n",
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 device=None, fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split\n",
        "        self.n_fft = n_fft\n",
        "        self.num_mels = num_mels\n",
        "        self.hop_size = hop_size\n",
        "        self.win_size = win_size\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "        self.fmax_loss = fmax_loss\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.device = device\n",
        "        self.fine_tuning = fine_tuning\n",
        "        self.base_mels_path = base_mels_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename)\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning:\n",
        "                audio = normalize(audio) * 0.95\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate:\n",
        "                raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio)\n",
        "        audio = audio.unsqueeze(0)\n",
        "\n",
        "        if not self.fine_tuning:\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "        else:\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "\n",
        "            if len(mel.shape) < 3:\n",
        "                mel = mel.unsqueeze(0)\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size)\n",
        "\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant')\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ],
      "metadata": {
        "id": "EbQN9Q-iN4ZX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py"
      ],
      "metadata": {
        "id": "aHPJHFNBNuw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#스펙트로그램 상 사용 함수 및 가중치 계산, 체크포인트 확인\n",
        "def plot_spectrogram(spectrogram):\n",
        "    fig, ax = plt.subplots(figsize=(10, 2))\n",
        "    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n",
        "                   interpolation='none')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    plt.close()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "\n",
        "def apply_weight_norm(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        weight_norm(m)\n",
        "\n",
        "\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def save_checkpoint(filepath, obj):\n",
        "    print(\"Saving checkpoint to {}\".format(filepath))\n",
        "    torch.save(obj, filepath)\n",
        "    print(\"Complete.\")\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '????????')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return None\n",
        "    return sorted(cp_list)[-1]"
      ],
      "metadata": {
        "id": "-5-vrrfxNuL-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#module.py\n"
      ],
      "metadata": {
        "id": "l7mjIQGRNZtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 내 사용되는 레이어\n",
        "\n",
        "class CoMBD(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, filters, kernels, groups, strides, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        init_channel = 1\n",
        "        for i, (f, k, g, s) in enumerate(zip(filters, kernels, groups, strides)):\n",
        "            self.convs.append(norm_f(Conv1d(init_channel, f, k, s, padding=get_padding(k, 1), groups=g)))\n",
        "            init_channel = f\n",
        "        self.conv_post = norm_f(Conv1d(filters[-1], 1, 3, 1, padding=get_padding(3, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        #fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MDC(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, channel, kernel, stride, dilations, use_spectral_norm=False):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.num_dilations = len(dilations)\n",
        "        for d in dilations:\n",
        "            self.convs.append(norm_f(Conv1d(in_channel, channel, kernel, stride=1, padding=get_padding(kernel, d),\n",
        "                                            dilation=d)))\n",
        "\n",
        "        self.conv_out = norm_f(Conv1d(channel, channel, 3, stride=stride, padding=get_padding(3, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = None\n",
        "        for l in self.convs:\n",
        "            if xs is None:\n",
        "                xs = l(x)\n",
        "            else:\n",
        "                xs += l(x)\n",
        "\n",
        "        x = xs / self.num_dilations\n",
        "\n",
        "        x = self.conv_out(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SubBandDiscriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, init_channel, channels, kernel, strides, dilations, use_spectral_norm=False):\n",
        "        super(SubBandDiscriminator, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "\n",
        "        self.mdcs = torch.nn.ModuleList()\n",
        "\n",
        "        for c, s, d in zip(channels, strides, dilations):\n",
        "            self.mdcs.append(MDC(init_channel, c, kernel, s, d))\n",
        "            init_channel = c\n",
        "        self.conv_post = norm_f(Conv1d(init_channel, 1, 3, padding=get_padding(3, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        for l in self.mdcs:\n",
        "            x = l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        #fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# adapted from\n",
        "# https://github.com/kan-bayashi/ParallelWaveGAN/tree/master/parallel_wavegan\n",
        "class PQMF(torch.nn.Module):\n",
        "    def __init__(self, N=4, taps=62, cutoff=0.15, beta=9.0):\n",
        "        super(PQMF, self).__init__()\n",
        "\n",
        "        self.N = N\n",
        "        self.taps = taps\n",
        "        self.cutoff = cutoff\n",
        "        self.beta = beta\n",
        "\n",
        "        QMF = sig.firwin(taps + 1, cutoff, window=('kaiser', beta))\n",
        "        H = np.zeros((N, len(QMF)))\n",
        "        G = np.zeros((N, len(QMF)))\n",
        "        for k in range(N):\n",
        "            constant_factor = (2 * k + 1) * (np.pi /\n",
        "                                             (2 * N)) * (np.arange(taps + 1) -\n",
        "                                                         ((taps - 1) / 2))  # TODO: (taps - 1) -> taps\n",
        "            phase = (-1)**k * np.pi / 4\n",
        "            H[k] = 2 * QMF * np.cos(constant_factor + phase)\n",
        "\n",
        "            G[k] = 2 * QMF * np.cos(constant_factor - phase)\n",
        "\n",
        "        H = torch.from_numpy(H[:, None, :]).float()\n",
        "        G = torch.from_numpy(G[None, :, :]).float()\n",
        "\n",
        "        self.register_buffer(\"H\", H)\n",
        "        self.register_buffer(\"G\", G)\n",
        "\n",
        "        updown_filter = torch.zeros((N, N, N)).float()\n",
        "        for k in range(N):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.N = N\n",
        "\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.analysis(x)\n",
        "\n",
        "    def analysis(self, x):\n",
        "        return F.conv1d(x, self.H, padding=self.taps // 2, stride=self.N)\n",
        "\n",
        "    def synthesis(self, x):\n",
        "        x = F.conv_transpose1d(x,\n",
        "                               self.updown_filter * self.N,\n",
        "                               stride=self.N)\n",
        "        x = F.conv1d(x, self.G, padding=self.taps // 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GMIJ5_w4NYj_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#models.py"
      ],
      "metadata": {
        "id": "W9bstayaNcN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#각 레이어들을 합쳐 모델로 구성\n",
        "\n",
        "LRELU_SLOPE = 0.1\n",
        "\n",
        "class ResBlock1(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock1, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs1:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.convs2:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class ResBlock2(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1])))\n",
        "        ])\n",
        "        self.convs.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c in self.convs:\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(h.num_mels, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(h.upsample_initial_channel // (2 ** i), h.upsample_initial_channel // (2 ** (i + 1)),\n",
        "                                k, u, padding=(k - u) // 2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = h.upsample_initial_channel // (2 ** (i + 1))\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(h, ch, k, d))\n",
        "\n",
        "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
        "        print(self.conv_post)\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "        self.out_proj_x1 = weight_norm(Conv1d(h.upsample_initial_channel // 4, 1, 7, 1, padding=3))\n",
        "        self.out_proj_x2 = weight_norm(Conv1d(h.upsample_initial_channel // 8, 1, 7, 1, padding=3))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x1 = None\n",
        "        x2 = None\n",
        "        x = self.conv_pre(x)\n",
        "\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i * self.num_kernels + j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i * self.num_kernels + j](x)\n",
        "            x = xs / self.num_kernels\n",
        "\n",
        "            if i == 1:\n",
        "                x1 = self.out_proj_x1(x)\n",
        "            elif i == 2:\n",
        "                x2 = self.out_proj_x2(x)\n",
        "\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x, x2, x1\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        remove_weight_norm(self.conv_post)\n",
        "\n",
        "\n",
        "class DiscriminatorP(torch.nn.Module):\n",
        "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
        "        super(DiscriminatorP, self).__init__()\n",
        "        self.period = period\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        # 1d to 2d\n",
        "        b, c, t = x.shape\n",
        "        if t % self.period != 0:  # pad first\n",
        "            n_pad = self.period - (t % self.period)\n",
        "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
        "            t = t + n_pad\n",
        "        x = x.view(b, c, t // self.period, self.period)\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiPeriodDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiPeriodDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorP(2),\n",
        "            DiscriminatorP(3),\n",
        "            DiscriminatorP(5),\n",
        "            DiscriminatorP(7),\n",
        "            DiscriminatorP(11),\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "class DiscriminatorS(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(DiscriminatorS, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n",
        "            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
        "            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiScaleDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorS(use_spectral_norm=True),\n",
        "            DiscriminatorS(),\n",
        "            DiscriminatorS(),\n",
        "        ])\n",
        "        self.meanpools = nn.ModuleList([\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            if i != 0:\n",
        "                y = self.meanpools[i - 1](y)\n",
        "                y_hat = self.meanpools[i - 1](y_hat)\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "class MultiCoMBDiscriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, kernels, channels, groups, strides):\n",
        "        super(MultiCoMBDiscriminator, self).__init__()\n",
        "        self.combd_1 = CoMBD(filters=channels, kernels=kernels[0], groups=groups, strides=strides)\n",
        "        self.combd_2 = CoMBD(filters=channels, kernels=kernels[1], groups=groups, strides=strides)\n",
        "        self.combd_3 = CoMBD(filters=channels, kernels=kernels[2], groups=groups, strides=strides)\n",
        "\n",
        "        self.pqmf_2 = PQMF(N=2, taps=256, cutoff=0.25, beta=10.0)\n",
        "        self.pqmf_4 = PQMF(N=4, taps=192, cutoff=0.13, beta=10.0)\n",
        "\n",
        "    def forward(self, x, x_hat, x2_hat, x1_hat):\n",
        "        y = []\n",
        "        y_hat = []\n",
        "        fmap = []\n",
        "        fmap_hat = []\n",
        "\n",
        "        p3, p3_fmap = self.combd_3(x)\n",
        "        y.append(p3)\n",
        "        fmap.append(p3_fmap)\n",
        "\n",
        "        p3_hat, p3_fmap_hat = self.combd_3(x_hat)\n",
        "        y_hat.append(p3_hat)\n",
        "        fmap_hat.append(p3_fmap_hat)\n",
        "\n",
        "        x2_ = self.pqmf_2(x)[:, :1, :]  # Select first band\n",
        "        x1_ = self.pqmf_4(x)[:, :1, :]  # Select first band\n",
        "\n",
        "        x2_hat_ = self.pqmf_2(x_hat)[:, :1, :]\n",
        "        x1_hat_ = self.pqmf_4(x_hat)[:, :1, :]\n",
        "\n",
        "        p2_, p2_fmap_ = self.combd_2(x2_)\n",
        "        y.append(p2_)\n",
        "        fmap.append(p2_fmap_)\n",
        "\n",
        "        p2_hat_, p2_fmap_hat_ = self.combd_2(x2_hat)\n",
        "        y_hat.append(p2_hat_)\n",
        "        fmap_hat.append(p2_fmap_hat_)\n",
        "\n",
        "        p1_, p1_fmap_ = self.combd_1(x1_)\n",
        "        y.append(p1_)\n",
        "        fmap.append(p1_fmap_)\n",
        "\n",
        "        p1_hat_, p1_fmap_hat_ = self.combd_1(x1_hat)\n",
        "        y_hat.append(p1_hat_)\n",
        "        fmap_hat.append(p1_fmap_hat_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        p2, p2_fmap = self.combd_2(x2_)\n",
        "        y.append(p2)\n",
        "        fmap.append(p2_fmap)\n",
        "\n",
        "        p2_hat, p2_fmap_hat = self.combd_2(x2_hat_)\n",
        "        y_hat.append(p2_hat)\n",
        "        fmap_hat.append(p2_fmap_hat)\n",
        "\n",
        "        p1, p1_fmap = self.combd_1(x1_)\n",
        "        y.append(p1)\n",
        "        fmap.append(p1_fmap)\n",
        "\n",
        "        p1_hat, p1_fmap_hat = self.combd_1(x1_hat_)\n",
        "        y_hat.append(p1_hat)\n",
        "        fmap_hat.append(p1_fmap_hat)\n",
        "\n",
        "        return y, y_hat, fmap, fmap_hat\n",
        "\n",
        "\n",
        "class MultiSubBandDiscriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, tkernels, fkernel, tchannels, fchannels, tstrides, fstride, tdilations, fdilations, tsubband,\n",
        "                 n, m, freq_init_ch):\n",
        "\n",
        "        super(MultiSubBandDiscriminator, self).__init__()\n",
        "\n",
        "        self.fsbd = SubBandDiscriminator(init_channel=freq_init_ch, channels=fchannels, kernel=fkernel,\n",
        "                                         strides=fstride, dilations=fdilations)\n",
        "\n",
        "        self.tsubband1 = tsubband[0]\n",
        "        self.tsbd1 = SubBandDiscriminator(init_channel=self.tsubband1, channels=tchannels, kernel=tkernels[0],\n",
        "                                          strides=tstrides[0], dilations=tdilations[0])\n",
        "\n",
        "        self.tsubband2 = tsubband[1]\n",
        "        self.tsbd2 = SubBandDiscriminator(init_channel=self.tsubband2, channels=tchannels, kernel=tkernels[1],\n",
        "                                          strides=tstrides[1], dilations=tdilations[1])\n",
        "\n",
        "        self.tsubband3 = tsubband[2]\n",
        "        self.tsbd3 = SubBandDiscriminator(init_channel=self.tsubband3, channels=tchannels, kernel=tkernels[2],\n",
        "                                          strides=tstrides[2], dilations=tdilations[2])\n",
        "\n",
        "\n",
        "        self.pqmf_n = PQMF(N=n, taps=256, cutoff=0.03, beta=10.0)\n",
        "        self.pqmf_m = PQMF(N=m, taps=256, cutoff=0.1, beta=9.0)\n",
        "\n",
        "    def forward(self, x, x_hat):\n",
        "        fmap = []\n",
        "        fmap_hat = []\n",
        "        y = []\n",
        "        y_hat = []\n",
        "\n",
        "        # Time analysis\n",
        "        xn = self.pqmf_n(x)\n",
        "        xn_hat = self.pqmf_n(x_hat)\n",
        "\n",
        "        q3, feat_q3 = self.tsbd3(xn[:, :self.tsubband3, :])\n",
        "        q3_hat, feat_q3_hat = self.tsbd3(xn_hat[:, :self.tsubband3, :])\n",
        "        y.append(q3)\n",
        "        y_hat.append(q3_hat)\n",
        "        fmap.append(feat_q3)\n",
        "        fmap_hat.append(feat_q3_hat)\n",
        "\n",
        "        q2, feat_q2 = self.tsbd2(xn[:, :self.tsubband2, :])\n",
        "        q2_hat, feat_q2_hat = self.tsbd2(xn_hat[:, :self.tsubband2, :])\n",
        "        y.append(q2)\n",
        "        y_hat.append(q2_hat)\n",
        "        fmap.append(feat_q2)\n",
        "        fmap_hat.append(feat_q2_hat)\n",
        "\n",
        "        q1, feat_q1 = self.tsbd1(xn[:, :self.tsubband1, :])\n",
        "        q1_hat, feat_q1_hat = self.tsbd1(xn_hat[:, :self.tsubband1, :])\n",
        "        y.append(q1)\n",
        "        y_hat.append(q1_hat)\n",
        "        fmap.append(feat_q1)\n",
        "        fmap_hat.append(feat_q1_hat)\n",
        "\n",
        "        # Frequency analysis\n",
        "        xm = self.pqmf_m(x)\n",
        "        xm_hat = self.pqmf_m(x_hat)\n",
        "\n",
        "        xm = xm.transpose(-2, -1)\n",
        "        xm_hat = xm_hat.transpose(-2, -1)\n",
        "\n",
        "        q4, feat_q4 = self.fsbd(xm)\n",
        "        q4_hat, feat_q4_hat = self.fsbd(xm_hat)\n",
        "        y.append(q4)\n",
        "        y_hat.append(q4_hat)\n",
        "        fmap.append(feat_q4)\n",
        "        fmap_hat.append(feat_q4_hat)\n",
        "\n",
        "        return y, y_hat, fmap, fmap_hat\n",
        "\n"
      ],
      "metadata": {
        "id": "DnlB_bSNNfaQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#로스값 계산 함수 지정\n",
        "\n",
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            loss += torch.mean(torch.abs(rl - gl))\n",
        "\n",
        "    return loss * 2\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1 - dr) ** 2)\n",
        "        g_loss = torch.mean(dg ** 2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1 - dg) ** 2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ],
      "metadata": {
        "id": "HMKvNrMYQhHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train.py\n"
      ],
      "metadata": {
        "id": "f07QfBWYN_4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 설정, config_v1,v2,v3가 있음, 필요에 따라 변경하기(깃허브에서 찾기)\n",
        "config={\n",
        "    \"resblock\": \"1\",\n",
        "    \"num_gpus\": 0,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"adam_b1\": 0.8,\n",
        "    \"adam_b2\": 0.99,\n",
        "    \"lr_decay\": 0.999,\n",
        "    \"seed\": 1234,\n",
        "\n",
        "    \"upsample_rates\": [8,8,2,2],\n",
        "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
        "    \"upsample_initial_channel\": 512,\n",
        "    \"resblock_kernel_sizes\": [3,7,11],\n",
        "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
        "\n",
        "    \"combd_channels\": [16, 64, 256, 1024, 1024, 1024],\n",
        "    \"combd_kernels\" : [[7, 11, 11, 11, 11, 5], [11, 21, 21, 21, 21, 5], [15, 41, 41, 41, 41, 5]],\n",
        "    \"combd_groups\" : [1, 4, 16, 64, 256, 1],\n",
        "    \"combd_strides\": [1, 1, 4, 4, 4, 1],\n",
        "\n",
        "    \"tkernels\" : [7, 5, 3],\n",
        "    \"fkernel\" : 5,\n",
        "    \"tchannels\" : [64, 128, 256, 256, 256],\n",
        "    \"fchannels\" : [32, 64, 128, 128, 128],\n",
        "    \"tstrides\" : [[1, 1, 3, 3, 1], [1, 1, 3, 3, 1], [1, 1, 3, 3, 1]],\n",
        "    \"fstride\" : [1, 1, 3, 3, 1],\n",
        "    \"tdilations\" : [[[5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11], [5, 7, 11]], [[3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7], [3, 5, 7]], [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]],\n",
        "    \"fdilations\" : [[1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 5], [2, 3, 5]],\n",
        "    \"pqmf_n\" : 16,\n",
        "    \"pqmf_m\" : 64,\n",
        "    \"freq_init_ch\" : 128,\n",
        "    \"tsubband\" : [6, 11, 16],\n",
        "\n",
        "    \"segment_size\": 8192,\n",
        "    \"num_mels\": 80,\n",
        "    \"num_freq\": 1025,\n",
        "    \"n_fft\": 1024,\n",
        "    \"hop_size\": 256,\n",
        "    \"win_size\": 1024,\n",
        "\n",
        "    \"sampling_rate\": 22050,\n",
        "\n",
        "    \"fmin\": 0,\n",
        "    \"fmax\": 8000,\n",
        "    \"fmax_for_loss\": None,\n",
        "\n",
        "    \"num_workers\": 4,\n",
        "\n",
        "    \"dist_config\": {\n",
        "        \"dist_backend\": \"nccl\",\n",
        "        \"dist_url\": \"tcp://localhost:54321\",\n",
        "        \"world_size\": 1\n",
        "    }\n",
        "}\n",
        "\n",
        "#지정한 config 생성 및 저장\n",
        "\"\"\"\n",
        "with open(\"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/config/config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(\"config.json 생성 완료\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81I30vsCOfp-",
        "outputId": "707eb44d-f7c7-439f-b8e1-e55da7cafae1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json 생성 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 코드\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def train(rank, a, h):\n",
        "    if h.num_gpus > 1:\n",
        "        init_process_group(backend=h.dist_config['dist_backend'], init_method=h.dist_config['dist_url'],\n",
        "                           world_size=h.dist_config['world_size'] * h.num_gpus, rank=rank)\n",
        "\n",
        "    torch.cuda.manual_seed(h.seed)\n",
        "    device = torch.device('cuda:{:d}'.format(rank))\n",
        "\n",
        "    generator = Generator(h).to(device)\n",
        "    mcmbd = MultiCoMBDiscriminator(h.combd_kernels, h.combd_channels, h.combd_groups, h.combd_strides).to(device)\n",
        "    msbd = MultiSubBandDiscriminator(h.tkernels, h.fkernel, h.tchannels, h.fchannels, h.tstrides, h.fstride,\n",
        "                                     h.tdilations, h.fdilations, h.tsubband, h.pqmf_n, h.pqmf_m, h.freq_init_ch).to(device)\n",
        "\n",
        "    if rank == 0:\n",
        "        print(generator)\n",
        "        os.makedirs(a.checkpoint_path, exist_ok=True)\n",
        "        print(\"checkpoints directory : \", a.checkpoint_path)\n",
        "\n",
        "    if os.path.isdir(a.checkpoint_path):\n",
        "        cp_g = scan_checkpoint(a.checkpoint_path, 'g_')\n",
        "        cp_do = scan_checkpoint(a.checkpoint_path, 'do_')\n",
        "\n",
        "    steps = 0\n",
        "    if cp_g is None or cp_do is None:\n",
        "        state_dict_do = None\n",
        "        last_epoch = -1\n",
        "    else:\n",
        "        state_dict_g = load_checkpoint(cp_g, device)\n",
        "        state_dict_do = load_checkpoint(cp_do, device)\n",
        "        generator.load_state_dict(state_dict_g['generator'])\n",
        "        mcmbd.load_state_dict(state_dict_do['mcmbd'])\n",
        "        msbd.load_state_dict(state_dict_do['msbd'])\n",
        "        steps = state_dict_do['steps'] + 1\n",
        "        last_epoch = state_dict_do['epoch']\n",
        "\n",
        "    if h.num_gpus > 1:\n",
        "        generator = DistributedDataParallel(generator, device_ids=[rank]).to(device)\n",
        "        mcmbd = DistributedDataParallel(mcmbd, device_ids=[rank]).to(device)\n",
        "        msbd = DistributedDataParallel(msbd, device_ids=[rank]).to(device)\n",
        "\n",
        "    optim_g = torch.optim.AdamW(generator.parameters(), h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "    optim_d = torch.optim.AdamW(itertools.chain(msbd.parameters(), mcmbd.parameters()),\n",
        "                                h.learning_rate, betas=[h.adam_b1, h.adam_b2])\n",
        "\n",
        "    if state_dict_do is not None:\n",
        "        optim_g.load_state_dict(state_dict_do['optim_g'])\n",
        "        optim_d.load_state_dict(state_dict_do['optim_d'])\n",
        "\n",
        "    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=h.lr_decay, last_epoch=last_epoch)\n",
        "    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=h.lr_decay, last_epoch=last_epoch)\n",
        "\n",
        "    training_filelist, validation_filelist = get_dataset_filelist(a)\n",
        "\n",
        "    trainset = MelDataset(training_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
        "                          h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, n_cache_reuse=0,\n",
        "                          shuffle=False if h.num_gpus > 1 else True, fmax_loss=h.fmax_for_loss, device=device,\n",
        "                          fine_tuning=a.fine_tuning, base_mels_path=a.input_mels_dir)\n",
        "\n",
        "    train_sampler = DistributedSampler(trainset) if h.num_gpus > 1 else None\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=h.num_workers, shuffle=False,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=h.batch_size,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    if rank == 0:\n",
        "        validset = MelDataset(validation_filelist, h.segment_size, h.n_fft, h.num_mels,\n",
        "                              h.hop_size, h.win_size, h.sampling_rate, h.fmin, h.fmax, False, False, n_cache_reuse=0,\n",
        "                              fmax_loss=h.fmax_for_loss, device=device, fine_tuning=a.fine_tuning,\n",
        "                              base_mels_path=a.input_mels_dir)\n",
        "        validation_loader = DataLoader(validset, num_workers=1, shuffle=False,\n",
        "                                       sampler=None,\n",
        "                                       batch_size=1,\n",
        "                                       pin_memory=True,\n",
        "                                       drop_last=True)\n",
        "\n",
        "        sw = SummaryWriter(os.path.join(a.checkpoint_path, 'logs'))\n",
        "\n",
        "    generator.train() #생성기 학습\n",
        "    mcmbd.train() #mcmbd 학습\n",
        "    msbd.train() #msbd 학습\n",
        "\n",
        "\n",
        "    for epoch in range(max(0, last_epoch), a.training_epochs):\n",
        "        if rank == 0:\n",
        "            start = time.time()\n",
        "            print(\"Epoch: {}\".format(epoch+1))\n",
        "\n",
        "        if h.num_gpus > 1:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if rank == 0:\n",
        "                start_b = time.time()\n",
        "            x, y, _, y_mel = batch\n",
        "            x = torch.autograd.Variable(x.to(device, non_blocking=True))\n",
        "            y = torch.autograd.Variable(y.to(device, non_blocking=True))\n",
        "            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n",
        "            y = y.unsqueeze(1)\n",
        "\n",
        "            y_g_hat, x2, x1 = generator(x)\n",
        "            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size,\n",
        "                                          h.fmin, h.fmax_for_loss)\n",
        "\n",
        "            optim_d.zero_grad()\n",
        "\n",
        "            # MPD\n",
        "            y_df_hat_r, y_df_hat_g, _, _ = mcmbd(y, y_g_hat.detach(), x2.detach(), x1.detach())\n",
        "            loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)\n",
        "\n",
        "            # MSD\n",
        "            y_ds_hat_r, y_ds_hat_g, _, _ = msbd(y, y_g_hat.detach())\n",
        "            loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "            loss_disc_all = loss_disc_s + loss_disc_f\n",
        "\n",
        "            loss_disc_all.backward()\n",
        "            optim_d.step()\n",
        "\n",
        "            # Generator\n",
        "            optim_g.zero_grad()\n",
        "\n",
        "            # L1 Mel-Spectrogram Loss\n",
        "            loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * 45\n",
        "\n",
        "            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mcmbd(y, y_g_hat, x2, x1)\n",
        "            y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = msbd(y, y_g_hat)\n",
        "            loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n",
        "            loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n",
        "            loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)\n",
        "            loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n",
        "            loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel\n",
        "\n",
        "            loss_gen_all.backward()\n",
        "            optim_g.step()\n",
        "\n",
        "            if rank == 0:\n",
        "                # STDOUT logging\n",
        "                if steps % a.stdout_interval == 0:\n",
        "                    with torch.no_grad():\n",
        "                        mel_error = F.l1_loss(y_mel, y_g_hat_mel).item()\n",
        "\n",
        "                    print('Steps : {:d}, Gen Loss Total : {:4.3f}, Mel-Spec. Error : {:4.3f}, s/b : {:4.3f}'.\n",
        "                          format(steps, loss_gen_all, mel_error, time.time() - start_b))\n",
        "\n",
        "                # checkpointing\n",
        "                if steps % a.checkpoint_interval == 0 and steps != 0:\n",
        "                    checkpoint_path = \"{}/g_{:08d}\".format(a.checkpoint_path, steps)\n",
        "                    save_checkpoint(checkpoint_path,\n",
        "                                    {'generator': (generator.module if h.num_gpus > 1 else generator).state_dict()})\n",
        "                    checkpoint_path = \"{}/do_{:08d}\".format(a.checkpoint_path, steps)\n",
        "                    save_checkpoint(checkpoint_path,\n",
        "                                    {'mcmbd': (mcmbd.module if h.num_gpus > 1\n",
        "                                                         else mcmbd).state_dict(),\n",
        "                                     'msbd': (msbd.module if h.num_gpus > 1\n",
        "                                                         else msbd).state_dict(),\n",
        "                                     'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': steps,\n",
        "                                     'epoch': epoch})\n",
        "\n",
        "                # Tensorboard summary logging\n",
        "                if steps % a.summary_interval == 0:\n",
        "                    sw.add_scalar(\"training/gen_loss_total\", loss_gen_all, steps)\n",
        "                    sw.add_scalar(\"training/mel_spec_error\", mel_error, steps)\n",
        "\n",
        "                # Validation\n",
        "                if steps % a.validation_interval == 0:  # and steps != 0:\n",
        "                    generator.eval()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    val_err_tot = 0\n",
        "                    with torch.no_grad():\n",
        "                        for j, batch in enumerate(validation_loader):\n",
        "                            x, y, _, y_mel = batch\n",
        "                            y_g_hat, _, _ = generator(x.to(device))\n",
        "                            y_mel = torch.autograd.Variable(y_mel.to(device, non_blocking=True))\n",
        "                            y_g_hat_mel = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels, h.sampling_rate,\n",
        "                                                          h.hop_size, h.win_size,\n",
        "                                                          h.fmin, h.fmax_for_loss)\n",
        "                            val_err_tot += F.l1_loss(y_mel, y_g_hat_mel).item()\n",
        "\n",
        "                            if j <= 4:\n",
        "                                if steps == 0:\n",
        "                                    sw.add_audio('gt/y_{}'.format(j), y[0], steps, h.sampling_rate)\n",
        "                                    sw.add_figure('gt/y_spec_{}'.format(j), plot_spectrogram(x[0]), steps)\n",
        "\n",
        "                                sw.add_audio('generated/y_hat_{}'.format(j), y_g_hat[0], steps, h.sampling_rate)\n",
        "                                y_hat_spec = mel_spectrogram(y_g_hat.squeeze(1), h.n_fft, h.num_mels,\n",
        "                                                             h.sampling_rate, h.hop_size, h.win_size,\n",
        "                                                             h.fmin, h.fmax)\n",
        "                                sw.add_figure('generated/y_hat_spec_{}'.format(j),\n",
        "                                              plot_spectrogram(y_hat_spec.squeeze(0).cpu().numpy()), steps)\n",
        "\n",
        "                        val_err = val_err_tot / (j+1)\n",
        "                        sw.add_scalar(\"validation/mel_spec_error\", val_err, steps)\n",
        "\n",
        "                    generator.train()\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "        scheduler_g.step()\n",
        "        scheduler_d.step()\n",
        "\n",
        "        if rank == 0:\n",
        "            print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, int(time.time() - start)))\n"
      ],
      "metadata": {
        "id": "ps8X8L-cQtaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#학습 시작\n",
        "def main():\n",
        "    print('Initializing Training Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--group_name', default=None)\n",
        "    parser.add_argument('--input_wavs_dir', default='LJSpeech-1.1/wavs')\n",
        "    parser.add_argument('--input_mels_dir', default='ft_dataset')\n",
        "    parser.add_argument('--input_training_file', default='LJSpeech-1.1/training.txt')\n",
        "    parser.add_argument('--input_validation_file', default='LJSpeech-1.1/validation.txt')\n",
        "    parser.add_argument('--checkpoint_path', default='cp_hifigan')\n",
        "    parser.add_argument('--config', default='')\n",
        "    parser.add_argument('--training_epochs', default=3100, type=int)\n",
        "    parser.add_argument('--stdout_interval', default=5, type=int)\n",
        "    parser.add_argument('--checkpoint_interval', default=5000, type=int)\n",
        "    parser.add_argument('--summary_interval', default=100, type=int)\n",
        "    parser.add_argument('--validation_interval', default=1000, type=int)\n",
        "    parser.add_argument('--fine_tuning', default=False, type=bool)\n",
        "\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    with open(a.config) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "    build_env(a.config, 'config.json', a.checkpoint_path)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        h.num_gpus = torch.cuda.device_count()\n",
        "        h.batch_size = int(h.batch_size / h.num_gpus)\n",
        "        print('Batch size per GPU :', h.batch_size)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    if h.num_gpus > 1:\n",
        "        mp.spawn(train, nprocs=h.num_gpus, args=(a, h,))\n",
        "    else:\n",
        "        train(0, a, h)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "XZl7qe-eOB-3",
        "outputId": "b80dd4b3-b351-45d1-9fd3-d444dca825fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Training Process..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--group_name GROUP_NAME] [--input_wavs_dir INPUT_WAVS_DIR]\n",
            "                                [--input_mels_dir INPUT_MELS_DIR]\n",
            "                                [--input_training_file INPUT_TRAINING_FILE]\n",
            "                                [--input_validation_file INPUT_VALIDATION_FILE]\n",
            "                                [--checkpoint_path CHECKPOINT_PATH] [--config CONFIG]\n",
            "                                [--training_epochs TRAINING_EPOCHS]\n",
            "                                [--stdout_interval STDOUT_INTERVAL]\n",
            "                                [--checkpoint_interval CHECKPOINT_INTERVAL]\n",
            "                                [--summary_interval SUMMARY_INTERVAL]\n",
            "                                [--validation_interval VALIDATION_INTERVAL]\n",
            "                                [--fine_tuning FINE_TUNING]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-99695fa0-e49a-4052-9ae7-6caf43e0c679.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference.py\n"
      ],
      "metadata": {
        "id": "nLwlLsXBOHev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(x, h.n_fft, h.num_mels, h.sampling_rate, h.hop_size, h.win_size, h.fmin, h.fmax)\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = os.listdir(a.input_wavs_dir)\n",
        "\n",
        "    os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "    with torch.no_grad():\n",
        "        for i, filname in enumerate(filelist):\n",
        "            wav, sr = load_wav(os.path.join(a.input_wavs_dir, filname))\n",
        "            wav = wav / MAX_WAV_VALUE\n",
        "            wav = torch.FloatTensor(wav).to(device)\n",
        "            x = get_mel(wav.unsqueeze(0))\n",
        "            y_g_hat, _, _ = generator(x)\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            output_file = os.path.join(a.output_dir, os.path.splitext(filname)[0] + '_generated.wav')\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_wavs_dir', default='test_files')\n",
        "    parser.add_argument('--output_dir', default='generated_files')\n",
        "    parser.add_argument('--checkpoint_file', required=True)\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "tIa1T2CjOJR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference_e2e.py"
      ],
      "metadata": {
        "id": "L-JmRnOyOMF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "h = None\n",
        "device = None\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, device):\n",
        "    assert os.path.isfile(filepath)\n",
        "    print(\"Loading '{}'\".format(filepath))\n",
        "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
        "    print(\"Complete.\")\n",
        "    return checkpoint_dict\n",
        "\n",
        "\n",
        "def scan_checkpoint(cp_dir, prefix):\n",
        "    pattern = os.path.join(cp_dir, prefix + '*')\n",
        "    cp_list = glob.glob(pattern)\n",
        "    if len(cp_list) == 0:\n",
        "        return ''\n",
        "    return sorted(cp_list)[-1]\n",
        "\n",
        "\n",
        "def inference(a):\n",
        "    generator = Generator(h).to(device)\n",
        "\n",
        "    state_dict_g = load_checkpoint(a.checkpoint_file, device)\n",
        "    generator.load_state_dict(state_dict_g['generator'])\n",
        "\n",
        "    filelist = os.listdir(a.input_mels_dir)\n",
        "\n",
        "    os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "    generator.eval()\n",
        "    generator.remove_weight_norm()\n",
        "    with torch.no_grad():\n",
        "        for i, filname in enumerate(filelist):\n",
        "            x = np.load(os.path.join(a.input_mels_dir, filname))\n",
        "            x = torch.FloatTensor(x).to(device)\n",
        "            y_g_hat, _, _ = generator(x)\n",
        "            audio = y_g_hat.squeeze()\n",
        "            audio = audio * MAX_WAV_VALUE\n",
        "            audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "            output_file = os.path.join(a.output_dir, os.path.splitext(filname)[0] + '_generated_e2e.wav')\n",
        "            write(output_file, h.sampling_rate, audio)\n",
        "            print(output_file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_mels_dir', default='test_mel_files')\n",
        "    parser.add_argument('--output_dir', default='generated_files_from_mel')\n",
        "    parser.add_argument('--checkpoint_file', required=True)\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "vR6dDOwuOOJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}