{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbz28Z0QIjogaoup0Ahnrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95410dd266df4077867731332bd2caec": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_615cb59826994d7f900a6f6c81c384ce",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;175;129;235mEpoch 0/4999\u001b[0m \u001b[38;2;139;233;254m━━━\u001b[0m\u001b[38;2;139;233;254m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[38;2;175;129;235m90/809\u001b[0m \u001b[38;2;19;99;223m0:04:19 • 0:32:20\u001b[0m \u001b[38;2;19;99;223m0.37it/s\u001b[0m \u001b[38;2;155;249;254mv_num: njbs train/loss_mel_step:  \u001b[0m\n                                                                                 \u001b[38;2;155;249;254m0.822 train/g_loss_step: 42.711   \u001b[0m\n                                                                                 \u001b[38;2;155;249;254mtrain/d_loss_step: 4.088          \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af81eb; text-decoration-color: #af81eb\">Epoch 0/4999</span> <span style=\"color: #8be9fe; text-decoration-color: #8be9fe\">━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #af81eb; text-decoration-color: #af81eb\">90/809</span> <span style=\"color: #1363df; text-decoration-color: #1363df\">0:04:19 • 0:32:20</span> <span style=\"color: #1363df; text-decoration-color: #1363df\">0.37it/s</span> <span style=\"color: #9bf9fe; text-decoration-color: #9bf9fe\">v_num: njbs train/loss_mel_step:  </span>\n                                                                                 <span style=\"color: #9bf9fe; text-decoration-color: #9bf9fe\">0.822 train/g_loss_step: 42.711   </span>\n                                                                                 <span style=\"color: #9bf9fe; text-decoration-color: #9bf9fe\">train/d_loss_step: 4.088          </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "615cb59826994d7f900a6f6c81c384ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#원본 코드\n",
        "\n",
        "https://github.com/ncsoft/avocodo"
      ],
      "metadata": {
        "id": "kZk8X1Cx3G3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avocodo 사전 설정\n"
      ],
      "metadata": {
        "id": "5S7waehhoSRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HE6aC-JGFmw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc82f65d-bbe0-4407-e9e7-e6ecf9b5037f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeTTabtv8WJ",
        "outputId": "4a1c25ad-a1de-406a-8ce9-ab6544dec5fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "mlbN1YmJkwVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4239356d-0127-4346-f07f-18697c9dbec4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OmegaConf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwxv46aFl6tl",
        "outputId": "d66689df-ce8e-4744-deb9-704d5bd7dbe5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: OmegaConf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqwXdH8ARxSb",
        "outputId": "92823dc8-b8c5-4220-bb71-07f51fd69911"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "from scipy.signal.windows import kaiser\n",
        "from omegaconf import OmegaConf\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"513a1f0c050fa7f60a76b5232e904d8df397082e\"\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uqn-XddwDcT",
        "outputId": "0681fcbe-ef59-40a3-a582-60dbf08a60df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdkkim2008\u001b[0m (\u001b[33mdkkim2008-hankuk-university-for-foreign-studies\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#meldataset.py"
      ],
      "metadata": {
        "id": "LuKV0hhkiCFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5): #튀는 부분 처리\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1): #작은 부분 키우기\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5): #토치버전 튀는 부분 처리\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1): #토치버전 작은 부분 키우기\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes): #토치 버전 스펙트로그램 정규화\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes): #토치버전 스펙트로그램 비정규화\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Hkgki34ooeUR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mel_basis = {}\n",
        "hann_window = {}"
      ],
      "metadata": {
        "id": "SKv2Mxbvoiba"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.: #정규화 여부\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ],
      "metadata": {
        "id": "bdi0Xs8BohoR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_filelist(\n",
        "    input_wavs_dir,\n",
        "    input_training_file,\n",
        "    input_validation_file\n",
        "):\n",
        "    with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n"
      ],
      "metadata": {
        "id": "NcaNp8vHokMx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate,  fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size #1개의 샘플 내에서 묶을 신호의 갯수\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split #segment_size로 자를지에 대한 여부\n",
        "        self.n_fft = n_fft #fft에서 나눌 주파수 대역의 갯수\n",
        "        self.num_mels = num_mels #멜 필터 사용 갯수\n",
        "        self.hop_size = hop_size #나눠진 각 구간에 대한\n",
        "        self.win_size = win_size #stft에서 분석할 샘플 갯수\n",
        "        self.fmin = fmin #사용할 최소 주파수\n",
        "        self.fmax = fmax #사용할 최대 주파수\n",
        "        self.fmax_loss = fmax_loss #멜 손실 최댓값\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.fine_tuning = fine_tuning #파인튜닝 여부\n",
        "        self.base_mels_path = base_mels_path# 저장된 멜 스펙트로그램 데이터경로\n",
        "\n",
        "    def __getitem__(self, index): #오디오 파일들 가져옴\n",
        "        filename = self.audio_files[index]\n",
        "        if self._cache_ref_count == 0:\n",
        "            audio, sampling_rate = load_wav(filename) #[num_samples]\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            if not self.fine_tuning: #파인 튜닝 여부에 따라 정규화\n",
        "                audio = normalize(audio) * 0.95 #[num_samples]\n",
        "            self.cached_wav = audio\n",
        "            if sampling_rate != self.sampling_rate: #샘플레이트 맞추기\n",
        "                raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "            self._cache_ref_count = self.n_cache_reuse\n",
        "        else:\n",
        "            audio = self.cached_wav\n",
        "            self._cache_ref_count -= 1\n",
        "\n",
        "        audio = torch.FloatTensor(audio)#텐서화\n",
        "        audio = audio.unsqueeze(0)#[1, num_samples]\n",
        "\n",
        "        if not self.fine_tuning:\n",
        "            if self.split:\n",
        "                if audio.size(1) >= self.segment_size: #샘플 길이가 세그먼트 갯수보다 긴 경우\n",
        "                    max_audio_start = audio.size(1) - self.segment_size\n",
        "                    audio_start = random.randint(0, max_audio_start)\n",
        "                    audio = audio[:, audio_start:audio_start+self.segment_size]\n",
        "                    #랜덤 시작위치에서 세그먼트 길이까지 자름,[1,segment_size]\n",
        "                else:\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    #짧으면 오디오에 제로 패딩 추가\n",
        "            mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                  center=False)\n",
        "            #num_mels랑 hop_size, win_size로 멜로 만듦, [num_mels, num_frames]\n",
        "        else:\n",
        "            mel = np.load(\n",
        "                os.path.join(self.base_mels_path, os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'))\n",
        "            mel = torch.from_numpy(mel)\n",
        "                # [num_mels, num_frames]\n",
        "            if len(mel.shape) < 3: #3개 값 안가지면, 차원 증강\n",
        "                mel = mel.unsqueeze(0)\n",
        "                # [1, num_mels,num_frames]\n",
        "\n",
        "            if self.split:\n",
        "                frames_per_seg = math.ceil(self.segment_size / self.hop_size) #프레임 갯수 계산\n",
        "                # [1, num_mels, num_frames]\n",
        "                if audio.size(1) >= self.segment_size:\n",
        "                    mel_start = random.randint(0, mel.size(2) - frames_per_seg - 1)\n",
        "                    mel = mel[:, :, mel_start:mel_start + frames_per_seg] #[ 1,num_mels,frames_per_seg]\n",
        "                    audio = audio[:, mel_start * self.hop_size:(mel_start + frames_per_seg) * self.hop_size]\n",
        "                    # [1,segment_size]\n",
        "                else:\n",
        "                    mel = torch.nn.functional.pad(mel, (0, frames_per_seg - mel.size(2)), 'constant') #[1,num_mels,num_frames]\n",
        "                    audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "                    # [1,num_mels, frames_per_seg]\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                   center=False)\n",
        "        # [1, segment_size] -> [num_mels, frames_per_seg]\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ],
      "metadata": {
        "id": "MDcNWVMHiD3R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py\n"
      ],
      "metadata": {
        "id": "9owc_vcWhiAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding(kernel_size, dilation=1): #제로 패딩 추가\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01): #가중치 초기화\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)"
      ],
      "metadata": {
        "id": "KxozRiQYhj-y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#losses.py\n"
      ],
      "metadata": {
        "id": "98DxXGMdhmT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_loss(fmap_r, fmap_g): #생성된 특성 맵, 실제 특성 맵 비교\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    for dr, dg in zip(fmap_r, fmap_g): #각각에 대해 절댓값으로 비교\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            _loss = torch.mean(torch.abs(rl - gl))\n",
        "            loss += _loss\n",
        "        losses.append(_loss)\n",
        "\n",
        "    return loss*2, losses\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs): #실제값, 생성값 로스값 비교\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs): #생성기 로스값\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ],
      "metadata": {
        "id": "8VpuHS2KhoVJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pqmf.py"
      ],
      "metadata": {
        "id": "MfcMzi-hiajZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def design_prototype_filter(taps=62, cutoff_ratio=0.142, beta=9.0): #프로토타입 필터 예시\n",
        "    \"\"\"Design prototype filter for PQMF.\n",
        "    This method is based on `A Kaiser window approach for the design of prototype\n",
        "    filters of cosine modulated filterbanks`_.\n",
        "    Args:\n",
        "        taps (int): The number of filter taps.\n",
        "        cutoff_ratio (float): Cut-off frequency ratio.\n",
        "        beta (float): Beta coefficient for kaiser window.\n",
        "    Returns:\n",
        "        ndarray: Impluse response of prototype filter (taps + 1,).\n",
        "    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n",
        "        https://ieeexplore.ieee.org/abstract/document/681427\n",
        "    \"\"\"\n",
        "    # check the arguments are valid, 제약조건\n",
        "    assert taps % 2 == 0, \"The number of taps mush be even number.\" #소수 지정\n",
        "    assert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\" #컷오프 비율\n",
        "\n",
        "    # make initial filter\n",
        "    omega_c = np.pi * cutoff_ratio #차단 주파수 계산\n",
        "    with np.errstate(invalid=\"ignore\"): #차단 주파수 기준으로 sinc함수로 필터 설계\n",
        "        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n",
        "            np.pi * (np.arange(taps + 1) - 0.5 * taps)\n",
        "        )\n",
        "    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # 완전 중심값의 경우, 분모가 0이므로 에러 발생하므로 예외처리\n",
        "\n",
        "    # apply kaiser window\n",
        "    w = kaiser(taps + 1, beta)# 양 옆으로 있고, 중간에 1개만 있으므로 +1, 베타를 가지는 카이저 윈도우\n",
        "    h = h_i * w #\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "ZAzKgcDTpCEh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "EVkne6sHDnXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PQMF(torch.nn.Module):\n",
        "    \"\"\"PQMF module.\n",
        "    This module is based on `Near-perfect-reconstruction pseudo-QMF banks`_.\n",
        "    .. _`Near-perfect-reconstruction pseudo-QMF banks`:\n",
        "        https://ieeexplore.ieee.org/document/258122\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "        \"\"\"Initilize PQMF module.\n",
        "        The cutoff_ratio and beta parameters are optimized for #subbands = 4.\n",
        "        See dicussion in https://github.com/kan-bayashi/ParallelWaveGAN/issues/195.\n",
        "        Args:\n",
        "            subbands (int): The number of subbands.\n",
        "            taps (int): The number of filter taps.\n",
        "            cutoff_ratio (float): Cut-off frequency ratio.\n",
        "            beta (float): Beta coefficient for kaiser window.\n",
        "        \"\"\"\n",
        "        super(PQMF, self).__init__()\n",
        "\n",
        "        # build analysis & synthesis filter coefficients\n",
        "        h_proto = design_prototype_filter(taps, cutoff_ratio, beta) #필터 생성\n",
        "        h_analysis = np.zeros((subbands, len(h_proto))) #서브 밴드 대역만큼 0으로 된 배열 생성\n",
        "        h_synthesis = np.zeros((subbands, len(h_proto))) #합성도 동일한 크기의 배열 생성\n",
        "        for k in range(subbands): #각 대역별로 분석, 합성을 진행\n",
        "            h_analysis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    + (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "            h_synthesis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    - (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # convert to tensor\n",
        "        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1)\n",
        "        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n",
        "\n",
        "        # register coefficients as beffer\n",
        "        self.register_buffer(\"analysis_filter\", analysis_filter)\n",
        "        self.register_buffer(\"synthesis_filter\", synthesis_filter)\n",
        "\n",
        "        # filter for downsampling & upsampling\n",
        "        updown_filter = torch.zeros((subbands, subbands, subbands)).float()\n",
        "        for k in range(subbands):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.subbands = subbands\n",
        "\n",
        "        # keep padding info\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n",
        "\n",
        "    def analysis(self, x): #다운 샘플링\n",
        "        \"\"\"Analysis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, 1, T).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, subbands, T // subbands).\n",
        "        \"\"\"\n",
        "        x = F.conv1d(self.pad_fn(x), self.analysis_filter) #패딩 추가, [batch_size, subbands, t]\n",
        "        return F.conv1d(x, self.updown_filter, stride=self.subbands)#(batch_size, sub_bands,t//sub_bands)\n",
        "\n",
        "    def synthesis(self, x): #업 샘플링\n",
        "        \"\"\"Synthesis with PQMF.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor (B, subbands, T // subbands).\n",
        "        Returns:\n",
        "            Tensor: Output tensor (B, 1, T).\n",
        "        \"\"\"\n",
        "        # NOTE(kan-bayashi): Power will be dreased so here multipy by # subbands.\n",
        "        #   Not sure this is the correct way, it is better to check again.\n",
        "        # TODO(kan-bayashi): Understand the reconstruction procedure\n",
        "        x = F.conv_transpose1d(\n",
        "            x, self.updown_filter * self.subbands, stride=self.subbands\n",
        "        )#[batch_size, subbands, t//subbands]\n",
        "        return F.conv1d(self.pad_fn(x), self.synthesis_filter)#[batch_size, 1, t]"
      ],
      "metadata": {
        "id": "gob2R_Voicxx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CoMBD.py"
      ],
      "metadata": {
        "id": "zSQ3Q0mqht0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoMBDBlock(torch.nn.Module):\n",
        "    def __init__( #CoMBD 블록 선언\n",
        "        self,\n",
        "        h_u: List[int], #각 계층별 출력 갯수(히든 유닛) 리스트\n",
        "        d_k: List[int], #각 계층의 커널 크기 리스트\n",
        "        d_s: List[int], #각 계층의 스트라이드 리스트\n",
        "        d_d: List[int], #각 계층의 dilation(확장 계수) 리스트\n",
        "        d_g: List[int], #각 계층의 그룹 수 리스트\n",
        "        d_p: List[int], #각 계층의 패딩 크기 리스트\n",
        "        op_f: int, #프로젝션 계층의 출력 채널 크기\n",
        "        op_k: int, #프로젝션 계층의 커널 크기\n",
        "        op_g: int, #프로젝션 계층의 그룹 수\n",
        "        use_spectral_norm=False #spectral normalization 적용여부\n",
        "    ):\n",
        "        super(CoMBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters = [[1, h_u[0]]] #출력 필터 지정\n",
        "        for i in range(len(h_u) - 1): #각 계층마다 필터 지정\n",
        "            filters.append([h_u[i], h_u[i + 1]]) #각 필터 시작지점, 끝지점으로 묶기\n",
        "        for _f, _k, _s, _d, _g, _p in zip(filters, d_k, d_s, d_d, d_g, d_p):#각 대역폭별로 사용될 데이터 묶기\n",
        "            self.convs.append(norm_f(\n",
        "                Conv1d(\n",
        "                    in_channels=_f[0],\n",
        "                    out_channels=_f[1],\n",
        "                    kernel_size=_k,\n",
        "                    stride=_s,\n",
        "                    dilation=_d,\n",
        "                    groups=_g,\n",
        "                    padding=_p\n",
        "                )\n",
        "            ))\n",
        "        self.projection_conv = norm_f( #제일 마지막 프로젝션 레이어\n",
        "            Conv1d(\n",
        "                in_channels=filters[-1][1],\n",
        "                out_channels=op_f,\n",
        "                kernel_size=op_k,\n",
        "                groups=op_g\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for block in self.convs:\n",
        "            x = block(x) #블록\n",
        "            x = F.leaky_relu(x, 0.2) #활성화 함수\n",
        "            fmap.append(x) #묶어서 추가\n",
        "        x = self.projection_conv(x)\n",
        "        return x, fmap    #CoMBD 블록 반환\n"
      ],
      "metadata": {
        "id": "HgOcC75wouAx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoMBD(torch.nn.Module):\n",
        "    def __init__(self, h, pqmf_list=None, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        self.h = h #하이퍼 파라미터 설정 객체\n",
        "        if pqmf_list is not None: #대역 분할 있으면, 지정\n",
        "            self.pqmf = pqmf_list\n",
        "        else: #대역 분할이 없으면 LV1, LV2로 분할\n",
        "            self.pqmf = [\n",
        "                PQMF(*h.pqmf_config[\"lv2\"]),\n",
        "                PQMF(*h.pqmf_config[\"lv1\"])\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.ModuleList() #CoMBD 블럭의 데이터 가져옴\n",
        "        for _h_u, _d_k, _d_s, _d_d, _d_g, _d_p, _op_f, _op_k, _op_g in zip(\n",
        "            h.combd_h_u,\n",
        "            h.combd_d_k,\n",
        "            h.combd_d_s,\n",
        "            h.combd_d_d,\n",
        "            h.combd_d_g,\n",
        "            h.combd_d_p,\n",
        "            h.combd_op_f,\n",
        "            h.combd_op_k,\n",
        "            h.combd_op_g,\n",
        "        ):\n",
        "            self.blocks.append(CoMBDBlock( #COMBD 블럭들 쌓기\n",
        "                _h_u,\n",
        "                _d_k,\n",
        "                _d_s,\n",
        "                _d_d,\n",
        "                _d_g,\n",
        "                _d_p,\n",
        "                _op_f,\n",
        "                _op_k,\n",
        "                _op_g,\n",
        "            ))\n",
        "\n",
        "    def _block_forward(self, input, blocks, outs, f_maps): #순차적 통과\n",
        "        for x, block in zip(input, blocks):\n",
        "            out, f_map = block(x) #각 계층마다의 중간맵 출력\n",
        "            outs.append(out)\n",
        "            f_maps.append(f_map)\n",
        "        return outs, f_maps\n",
        "\n",
        "    def _pqmf_forward(self, ys, ys_hat): #PQMF를 통한 신호의 대역폭 분할\n",
        "        # ys는 실제 신호 리스트 - training_step 파트에서 만들어짐\n",
        "        # ys_hat은 생성된 신호 리스트\n",
        "        #  ys = [\n",
        "        #self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],  # PQMF Level 2\n",
        "        #self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],  # PQMF Level 1\n",
        "        #y  ] # 원래 입력 신호\n",
        "        multi_scale_inputs = []\n",
        "        multi_scale_inputs_hat = []\n",
        "        for pqmf in self.pqmf: #lv1, lv2가 들어가있는 pqmf 객체 리스트에 다운샘플링\n",
        "            multi_scale_inputs.append(  #analysis 함수는 일종의 다운샘플링\n",
        "                pqmf.to(ys[-1]).analysis(ys[-1])[:, :1, :] #batch_size, subband,t\n",
        "            )\n",
        "            multi_scale_inputs_hat.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys_hat[-1])[:, :1, :]\n",
        "            )\n",
        "\n",
        "        outs_real = []\n",
        "        f_maps_real = []\n",
        "        # real\n",
        "        # for hierarchical forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            ys, self.blocks, outs_real, f_maps_real)\n",
        "        # for multi_scale forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            multi_scale_inputs, self.blocks[:-1], outs_real, f_maps_real)\n",
        "\n",
        "        outs_fake = []\n",
        "        f_maps_fake = []\n",
        "        # predicted\n",
        "        # for hierarchical forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            ys_hat, self.blocks, outs_fake, f_maps_fake)\n",
        "        # for multi_scale forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            multi_scale_inputs_hat, self.blocks[:-1], outs_fake, f_maps_fake)\n",
        "\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake\n",
        "\n",
        "    def forward(self, ys, ys_hat):\n",
        "        outs_real, outs_fake, f_maps_real, f_maps_fake = self._pqmf_forward(\n",
        "            ys, ys_hat)\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake"
      ],
      "metadata": {
        "id": "7-ga7kQJhs-0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SBD.py\n"
      ],
      "metadata": {
        "id": "NPeFVq4_hzq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDC(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.d_convs = nn.ModuleList()\n",
        "        for _k, _d in zip(kernel_size, dilations):\n",
        "            self.d_convs.append(\n",
        "                norm_f(Conv1d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=_k,\n",
        "                    dilation=_d,\n",
        "                    padding=get_padding(_k, _d)\n",
        "                ))\n",
        "            )\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=strides,\n",
        "            padding=get_padding(_k, _d)\n",
        "        ))\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _out = None\n",
        "        for _l in self.d_convs:\n",
        "            _x = torch.unsqueeze(_l(x), -1)\n",
        "            _x = F.leaky_relu(_x, 0.2)\n",
        "            if _out is None:\n",
        "                _out = _x\n",
        "            else:\n",
        "                _out = torch.cat([_out, _x], axis=-1)\n",
        "        x = torch.sum(_out, dim=-1)\n",
        "        x = self.post_conv(x)\n",
        "        x = F.leaky_relu(x, 0.2)  # @@\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "OBMUu1GRozy6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SBDBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        segment_dim,\n",
        "        strides,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(SBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters_in_out = [(segment_dim, filters[0])]\n",
        "        for i in range(len(filters) - 1):\n",
        "            filters_in_out.append([filters[i], filters[i + 1]])\n",
        "\n",
        "        for _s, _f, _k, _d in zip(\n",
        "            strides,\n",
        "            filters_in_out,\n",
        "            kernel_size,\n",
        "            dilations\n",
        "        ):\n",
        "            self.convs.append(MDC(\n",
        "                in_channels=_f[0],\n",
        "                out_channels=_f[1],\n",
        "                strides=_s,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=_f[1],\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=3 // 2\n",
        "        ))  # @@\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for _l in self.convs:\n",
        "            x = _l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.post_conv(x)  # @@\n",
        "\n",
        "        return x, fmap\n",
        "\n"
      ],
      "metadata": {
        "id": "i2OaM69uh0_5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDCDConfig:\n",
        "    def __init__(self, h):\n",
        "        self.pqmf_params = h.pqmf_config[\"sbd\"]\n",
        "        self.f_pqmf_params = h.pqmf_config[\"fsbd\"]\n",
        "        self.filters = h.sbd_filters\n",
        "        self.kernel_sizes = h.sbd_kernel_sizes\n",
        "        self.dilations = h.sbd_dilations\n",
        "        self.strides = h.sbd_strides\n",
        "        self.band_ranges = h.sbd_band_ranges\n",
        "        self.transpose = h.sbd_transpose\n",
        "        self.segment_size = h.segment_size"
      ],
      "metadata": {
        "id": "wuGqG0y4o4oB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SBD(torch.nn.Module):\n",
        "    def __init__(self, h, use_spectral_norm=False):\n",
        "        super(SBD, self).__init__()\n",
        "        self.config = MDCDConfig(h)\n",
        "        self.pqmf = PQMF(\n",
        "            *self.config.pqmf_params\n",
        "        )\n",
        "        if True in h.sbd_transpose:\n",
        "            self.f_pqmf = PQMF(\n",
        "                *self.config.f_pqmf_params\n",
        "            )\n",
        "        else:\n",
        "            self.f_pqmf = None\n",
        "\n",
        "        self.discriminators = torch.nn.ModuleList()\n",
        "\n",
        "        for _f, _k, _d, _s, _br, _tr in zip(\n",
        "            self.config.filters,\n",
        "            self.config.kernel_sizes,\n",
        "            self.config.dilations,\n",
        "            self.config.strides,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if _tr:\n",
        "                segment_dim = self.config.segment_size // _br[1] - _br[0]\n",
        "            else:\n",
        "                segment_dim = _br[1] - _br[0]\n",
        "\n",
        "            self.discriminators.append(SBDBlock(\n",
        "                segment_dim=segment_dim,\n",
        "                filters=_f,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                strides=_s,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        y_in = self.pqmf.analysis(y)\n",
        "        y_hat_in = self.pqmf.analysis(y_hat)\n",
        "        if self.f_pqmf is not None:\n",
        "            y_in_f = self.f_pqmf.analysis(y)\n",
        "            y_hat_in_f = self.f_pqmf.analysis(y_hat)\n",
        "\n",
        "        for d, br, tr in zip(\n",
        "            self.discriminators,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if tr:\n",
        "                _y_in = y_in_f[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in_f[:, br[0]:br[1], :]\n",
        "                _y_in = torch.transpose(_y_in, 1, 2)\n",
        "                _y_hat_in = torch.transpose(_y_hat_in, 1, 2)\n",
        "            else:\n",
        "                _y_in = y_in[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in[:, br[0]:br[1], :]\n",
        "            y_d_r, fmap_r = d(_y_in)\n",
        "            y_d_g, fmap_g = d(_y_hat_in)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs"
      ],
      "metadata": {
        "id": "2H-kRPzzo3dp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#generator.py\n"
      ],
      "metadata": {
        "id": "4ttjqPrPh4sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.2)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, 0.2)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for _l in self.convs1:\n",
        "            remove_weight_norm(_l)\n",
        "        for _l in self.convs2:\n",
        "            remove_weight_norm(_l)\n"
      ],
      "metadata": {
        "id": "HHcgmHJEo90g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.resblock = h.resblock\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            _ups = nn.ModuleList()\n",
        "            for _i, (_u, _k) in enumerate(zip(u, k)):\n",
        "                in_channel = h.upsample_initial_channel // (2**i)\n",
        "                out_channel = h.upsample_initial_channel // (2**(i + 1))\n",
        "                _ups.append(weight_norm(\n",
        "                    ConvTranspose1d(in_channel, out_channel, _k, _u, padding=(_k - _u) // 2)))\n",
        "            self.ups.append(_ups)\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        self.conv_post = nn.ModuleList()\n",
        "        for i in range(self.num_upsamples):\n",
        "            ch = h.upsample_initial_channel // (2**(i + 1))\n",
        "            temp = nn.ModuleList()\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                temp.append(resblock(h, ch, k, d))\n",
        "            self.resblocks.append(temp)\n",
        "\n",
        "            if self.h.projection_filters[i] != 0:\n",
        "                self.conv_post.append(\n",
        "                    weight_norm(\n",
        "                        Conv1d(\n",
        "                            ch, self.h.projection_filters[i],\n",
        "                            self.h.projection_kernels[i], 1, padding=self.h.projection_kernels[i] // 2\n",
        "                        )))\n",
        "            else:\n",
        "                self.conv_post.append(torch.nn.Identity())\n",
        "\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.conv_pre(x)\n",
        "        for i, (ups, resblocks, conv_post) in enumerate(zip(self.ups, self.resblocks, self.conv_post)):\n",
        "            x = F.leaky_relu(x, 0.2)\n",
        "            for _ups in ups:\n",
        "                x = _ups(x)\n",
        "            xs = None\n",
        "            for j, resblock in enumerate(resblocks):\n",
        "                if xs is None:\n",
        "                    xs = resblock(x)\n",
        "                else:\n",
        "                    xs += resblock(x)\n",
        "            x = xs / self.num_kernels\n",
        "            if i >= (self.num_upsamples-3):\n",
        "                _x = F.leaky_relu(x)\n",
        "                _x = conv_post(_x)\n",
        "                _x = torch.tanh(_x)\n",
        "                outs.append(_x)\n",
        "            else:\n",
        "                x = conv_post(x)\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for ups in self.ups:\n",
        "            for _l in ups:\n",
        "                remove_weight_norm(_l)\n",
        "        for resblock in self.resblocks:\n",
        "            for _l in resblock:\n",
        "                _l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        for _l in self.conv_post:\n",
        "            if not isinstance(_l, torch.nn.Identity):\n",
        "                remove_weight_norm(_l)"
      ],
      "metadata": {
        "id": "TUBqwfZgh3dx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data_module.py"
      ],
      "metadata": {
        "id": "BUyxi8fxiHbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AvocodoDataConfig:\n",
        "    segment_size: int\n",
        "    num_mels: int\n",
        "    num_freq: int\n",
        "    sampling_rate: int\n",
        "    n_fft: int\n",
        "    hop_size: int\n",
        "    win_size: int\n",
        "    fmin: int\n",
        "    fmax: int\n",
        "    batch_size: int\n",
        "    num_workers: int\n",
        "\n",
        "    fine_tuning: bool\n",
        "    base_mels_path: str\n",
        "\n",
        "    input_wavs_dir: str\n",
        "    input_mels_dir: str\n",
        "    input_training_file: str\n",
        "    input_validation_file: str"
      ],
      "metadata": {
        "id": "2RVIsixRpGYI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AvocodoData(LightningDataModule):\n",
        "    def __init__(self, h: AvocodoDataConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        '''\n",
        "            download and prepare data\n",
        "        '''\n",
        "        self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "            self.hparams.input_wavs_dir,\n",
        "            self.hparams.input_training_file,\n",
        "            self.hparams.input_validation_file\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.trainset = MelDataset(\n",
        "            self.training_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(\n",
        "            self.trainset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "    @rank_zero_only\n",
        "    def val_dataloader(self):\n",
        "        validset = MelDataset(\n",
        "            self.validation_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            False,\n",
        "            False,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "        max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "        return DataLoader(validset, num_workers=max_workers, shuffle=False,\n",
        "                          sampler=None,\n",
        "                          batch_size=1,\n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)"
      ],
      "metadata": {
        "id": "R3ikwJZniJKo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lightning_module.py"
      ],
      "metadata": {
        "id": "PGTk_rGDiPHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# WandB 프로젝트 초기화\n",
        "wandb_logger = WandbLogger(project=\"avocodo_train\", name=\"avocodo_training\")"
      ],
      "metadata": {
        "id": "jK6AJpMZxaOC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Avocodo(LightningModule):\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "        # Model components\n",
        "        self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])\n",
        "        self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "        self.generator = Generator(self.hparams.generator)\n",
        "        self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])\n",
        "        self.sbd = SBD(self.hparams.sbd)\n",
        "\n",
        "        # Validation outputs storage\n",
        "        self.validation_outputs = []\n",
        "\n",
        "        # Manual optimization\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        h = self.hparams.optimizer\n",
        "        opt_g = torch.optim.AdamW(\n",
        "            self.generator.parameters(),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        opt_d = torch.optim.AdamW(\n",
        "            itertools.chain(self.combd.parameters(), self.sbd.parameters()),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        return [opt_g, opt_d]\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)[-1]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        y = y.unsqueeze(1)\n",
        "        ys = [\n",
        "            self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],\n",
        "            self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],\n",
        "            y\n",
        "        ]\n",
        "        y_g_hats = self.generator(x)\n",
        "\n",
        "        # Get optimizers\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        # Train Generator\n",
        "        opt_g.zero_grad()\n",
        "        y_du_hat_r, y_du_hat_g, fmap_u_r, fmap_u_g = self.combd(ys, y_g_hats)\n",
        "        loss_fm_u, _ = feature_loss(fmap_u_r, fmap_u_g)\n",
        "        loss_gen_u, _ = generator_loss(y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.sbd(y, y_g_hats[-1])\n",
        "        loss_fm_s, _ = feature_loss(fmap_s_r, fmap_s_g)\n",
        "        loss_gen_s, _ = generator_loss(y_ds_hat_g)\n",
        "\n",
        "        # L1 Mel-Spectrogram Loss\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hats[-1].squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "        self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        loss_mel = loss_mel * self.hparams.loss_scale_mel\n",
        "\n",
        "        g_loss = loss_gen_s + loss_gen_u + loss_fm_s + loss_fm_u + loss_mel\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "        self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_d.zero_grad()\n",
        "        detached_y_g_hats = [x.detach() for x in y_g_hats]\n",
        "\n",
        "        y_du_hat_r, y_du_hat_g, _, _ = self.combd(ys, detached_y_g_hats)\n",
        "        loss_disc_u, _, _ = discriminator_loss(y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "        loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "        d_loss = loss_disc_s + loss_disc_u\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "        self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch\n",
        "        y_g_hat = self(x)\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hat.squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        val_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "        self.validation_outputs.append(val_loss)\n",
        "\n",
        "        # Log validation loss\n",
        "        self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.validation_outputs:\n",
        "            avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "            self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "        self.validation_outputs.clear()\n",
        "\n"
      ],
      "metadata": {
        "id": "HsaB8xZGiOeh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train.py\n",
        "\n"
      ],
      "metadata": {
        "id": "VohGwH8g_Dov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TBLogger(TensorBoardLogger):\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        metrics.pop('epoch', None)\n",
        "        return super().log_metrics(metrics, step)\n",
        "\n"
      ],
      "metadata": {
        "id": "OHLRL85QEBkW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--group_name', default=None)\n",
        "parser.add_argument('--input_wavs_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs')\n",
        "parser.add_argument('--input_mels_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram')\n",
        "parser.add_argument('--input_training_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt')\n",
        "parser.add_argument('--input_validation_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt')\n",
        "parser.add_argument('--config', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1.json')\n",
        "parser.add_argument('--training_epochs', default=5000, type=int)\n",
        "parser.add_argument('--fine_tuning', default=False, type=bool)\n",
        "\n",
        "\n",
        "\n",
        "if \"ipykernel_launcher\" in sys.argv[0]:\n",
        "    sys.argv = [\n",
        "        'script_name',\n",
        "        '--group_name', 'default_group',\n",
        "        '--input_wavs_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs',\n",
        "        '--input_mels_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram',\n",
        "        '--input_training_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt',\n",
        "        '--input_validation_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt',\n",
        "        '--config', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1.json',\n",
        "        '--training_epochs', '5000',\n",
        "        '--fine_tuning', 'False'\n",
        "    ]"
      ],
      "metadata": {
        "id": "NY4uFuC9EEZY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "\n",
        "# 샘플 Mel-spectrogram 생성 함수\n",
        "def generate_mel_spectrogram(sample_rate=22050, n_fft=1024, hop_length=256, n_mels=80, duration=1.0):\n",
        "    # 가상 오디오 데이터 생성 (White Noise)\n",
        "    raw_audio = torch.randn(int(sample_rate * duration))  # 1초 길이의 오디오\n",
        "\n",
        "    # Mel-spectrogram 변환\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(\n",
        "        y=raw_audio.numpy(),\n",
        "        sr=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    mel_tensor = torch.tensor(mel_spectrogram).unsqueeze(0)  # 배치 차원 추가\n",
        "    return mel_tensor\n",
        "\n",
        "# 예시 데이터 준비\n",
        "mel_input = generate_mel_spectrogram(duration=1.0)  # 1초 길이의 Mel-spectrogram\n",
        "mel_input = mel_input.unsqueeze(0)  # 배치 차원 추가 (1, 80, Frames)\n",
        "mel_input = mel_input.squeeze(1)\n",
        "print(f\"Input Shape: {mel_input.shape}\")  # (1, 80, Frames)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu96Tbu2Twyi",
        "outputId": "5eb96efb-a1f2-4578-fa81-a04d7e8feb1e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([1, 80, 87])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse Arguments\n",
        "a, unknown = parser.parse_known_args()\n",
        "\n",
        "# OmegaConf 설정\n",
        "OmegaConf.register_new_resolver(\"from_args\", lambda x: getattr(a, x))\n",
        "OmegaConf.register_new_resolver(\"dir\", lambda base_dir, string: os.path.join(base_dir, string))\n",
        "conf = OmegaConf.load(a.config)\n",
        "OmegaConf.resolve(conf)\n"
      ],
      "metadata": {
        "id": "nJuJSVJR_DRA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 데이터 및 모델 초기화\n",
        "dm = AvocodoData(conf.data)\n",
        "model = Avocodo(conf.model)\n",
        "\n",
        "# 모델 요약 출력\n",
        "summary(\n",
        "    model,\n",
        "    input_data=mel_input,  # 입력 데이터 전달\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\"],  # 표시할 열 선택\n",
        "    col_width=20,\n",
        "    depth=5  # 레이어 깊이 제한\n",
        ")\n",
        "\n",
        "limit_train_batches = 1.0\n",
        "limit_val_batches = 1.0\n",
        "log_every_n_steps = 50\n",
        "max_epochs = conf.model.train.training_epochs"
      ],
      "metadata": {
        "id": "BQYocu51VK1C"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',               # Validation 손실 기준\n",
        "    dirpath='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint',  # 체크포인트 저장 디렉토리\n",
        "    filename='best-checkpoint-{epoch:02d}-{val_loss:.2f}',  # 파일 이름 패턴\n",
        "    save_top_k=1,                     # 가장 좋은 k개의 모델만 저장\n",
        "    mode='min',                       # 손실 기준으로 최소값 저장\n",
        ")\n",
        "\n",
        "# 조기 종료 콜백: Validation 손실 개선이 없으면 중단\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',              # Validation 손실 기준\n",
        "    patience=100,                     # 몇 에포크 동안 개선 없으면 중단\n",
        "    verbose=True,                    # 중단 시 메시지 출력\n",
        "    mode='min'                       # 손실 기준으로 최소값 기준\n",
        ")\n"
      ],
      "metadata": {
        "id": "1hmp4fZrEasS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=\"auto\",\n",
        "    max_epochs=max_epochs,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        early_stopping_callback,\n",
        "        RichProgressBar(\n",
        "            refresh_rate=1,\n",
        "            theme=RichProgressBarTheme(\n",
        "                description=\"#AF81EB\",\n",
        "                progress_bar=\"#8BE9FE\",\n",
        "                progress_bar_finished=\"#8BE9FE\",\n",
        "                progress_bar_pulse=\"#1363DF\",\n",
        "                batch_progress=\"#AF81EB\",\n",
        "                time=\"#1363DF\",\n",
        "                processing_speed=\"#1363DF\",\n",
        "                metrics=\"#9BF9FE\",\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    # logger=TensorBoardLogger(\"logs\", name=\"Avocodo\"),\n",
        "    logger=wandb_logger,\n",
        "    limit_train_batches=limit_train_batches,\n",
        "    limit_val_batches=limit_val_batches,\n",
        "    log_every_n_steps=log_every_n_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "BUWFRnb9EJ33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765110b8-2a71-466a-e442-67239252eb1a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, dm)"
      ],
      "metadata": {
        "id": "aq4FBO9OArgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "95410dd266df4077867731332bd2caec",
            "615cb59826994d7f900a6f6c81c384ce"
          ]
        },
        "outputId": "5fafb9e0-7617-42a5-aac7-328009cc232a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ pqmf_lv2  │ PQMF      │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ pqmf_lv1  │ PQMF      │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ generator │ Generator │ 13.9 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ combd     │ CoMBD     │ 16.5 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ sbd       │ SBD       │ 10.6 M │ train │\n",
              "└───┴───────────┴───────────┴────────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ pqmf_lv2  │ PQMF      │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ pqmf_lv1  │ PQMF      │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ generator │ Generator │ 13.9 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ combd     │ CoMBD     │ 16.5 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ sbd       │ SBD       │ 10.6 M │ train │\n",
              "└───┴───────────┴───────────┴────────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 41.0 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 41.0 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 164                                                                        \n",
              "\u001b[1mModules in train mode\u001b[0m: 320                                                                                         \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 41.0 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 41.0 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 164                                                                        \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 320                                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95410dd266df4077867731332bd2caec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` \n",
              "from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., \n",
              "batch_size=batch_size)`.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` \n",
              "from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., \n",
              "batch_size=batch_size)`.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference.py"
      ],
      "metadata": {
        "id": "_32FgGv8ihNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "h = None\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(\n",
        "        x,\n",
        "        1024,\n",
        "        80,\n",
        "        22050,\n",
        "        256,\n",
        "        1024,\n",
        "        0,\n",
        "        8000\n",
        "    )\n"
      ],
      "metadata": {
        "id": "6baN8L8CmXVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def inference(a, conf):#추론 config 로그 저장\n",
        "    avocodo = Avocodo.load_from_checkpoint(\n",
        "        f\"{a.checkpoint_path}/version_{a.version}/checkpoints/{a.checkpoint_file_id}\",\n",
        "        map_location='cpu'\n",
        "    )\n",
        "    avocodo_data = AvocodoData(conf.audio)\n",
        "    avocodo_data.prepare_data()\n",
        "    validation_dataloader = avocodo_data.val_dataloader()\n",
        "\n",
        "    output_path = f'{a.output_dir}/version_{a.version}/'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    avocodo.generator.to(a.device)\n",
        "    avocodo.generator.remove_weight_norm()\n",
        "\n",
        "    m = torch.jit.script(avocodo.generator)\n",
        "    torch.jit.save(\n",
        "        m,\n",
        "        os.path.join(output_path, \"scripted.pt\")\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(validation_dataloader):\n",
        "            mels, _, file_ids, _ = batch\n",
        "\n",
        "            y_g_hat = avocodo(mels.to(a.device))\n",
        "\n",
        "            for _y_g_hat, file_id in zip(y_g_hat, file_ids):\n",
        "                audio = _y_g_hat.squeeze(0)\n",
        "                audio = audio * MAX_WAV_VALUE\n",
        "                audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "                output_file = os.path.join(\n",
        "                    output_path,\n",
        "                    file_id.split('/')[-1]\n",
        "                )\n",
        "                print(file_id)\n",
        "                write(output_file, conf.audio.sampling_rate, audio)\n",
        "    print('Done inference')\n"
      ],
      "metadata": {
        "id": "o6rbIWAomjLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--checkpoint_path', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint')\n",
        "    parser.add_argument('--version', type=int, required=True)\n",
        "    parser.add_argument('--checkpoint_file_id', type=str, default='', required=True)\n",
        "    parser.add_argument('--output_dir', type=str, default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result')\n",
        "    parser.add_argument('--script', type=bool, default=True)\n",
        "    parser.add_argument('--device', type=str, default='cuda')\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    conf = OmegaConf.load(os.path.join(a.checkpoint_path, f\"version_{a.version}\", \"hparams.yaml\"))\n",
        "    inference(a, conf)"
      ],
      "metadata": {
        "id": "69y4YDJZiilp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "cn0baVSmmlsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}