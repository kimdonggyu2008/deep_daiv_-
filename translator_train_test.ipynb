{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN70R3i0DsqdL5Z2XwXf4q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/translator_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#번역 모델(nllb) 학습 및 추론 코드"
      ],
      "metadata": {
        "id": "jLM6ZYjbE5Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVAebtHELUG0",
        "outputId": "0af09a5a-1523-4356-c631-727425956adc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjzhwiQkOhAZ",
        "outputId": "6c11300d-8683-4a65-b443-afec3315ede2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Lx27sdP0MabO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 병합 및 토크나이저, 처리(필요시 사용)"
      ],
      "metadata": {
        "id": "ZhqjyRD8TcCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. 데이터셋 로드 (예시 데이터셋)\n",
        "df = pd.read_csv('/content/drive/MyDrive/translator/data/train.csv')#train.csv파일 있으면 루트 변경\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "gqWBKk6mNHZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "ee3a6188-ddc2-47d8-d33d-93f4a67c9191"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/translator/data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a1e6099d780a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3. 데이터셋 로드 (예시 데이터셋)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/translator/data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#train.csv파일 있으면 루트 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/translator/data/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # combined_df를 Dataset으로 변환\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# # train, test 데이터셋 분리 (예시로 10%를 테스트 세트로 분리)\n",
        "# train_test = dataset.train_test_split(test_size=0.1)\n",
        "# datasets = DatasetDict({\n",
        "#     'train': train_test['train'],\n",
        "#     'test': train_test['test']\n",
        "# })\n",
        "\n",
        "# # 4. 전처리 함수 (한국어 -> 영어 번역)\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = examples['ko']  # 'ko' 열에서 한국어 텍스트를 가져옴\n",
        "#     targets = examples['en']  # 'en' 열에서 영어 번역 텍스트를 가져옴\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     # 타겟 텍스트도 토크나이징\n",
        "#     labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# # # 데이터셋을 전처리 및 토크나이징 (datasets가 이미 로드된 상태에서)\n",
        "# tokenized_datasets = datasets.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "jVUeMX-tNIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토크나이징된 데이터셋을 4개로 나눔\n",
        "\n",
        "\n",
        "# # 5. 4개의 파트로 분할하기 위해 총 데이터셋 크기 계산\n",
        "# dataset_length = len(tokenized_datasets['train'])\n",
        "\n",
        "# #파일을 4개로 나누어서 저장, 나중에 각각 불러와서 에폭마다 돌려야할듯\n",
        "# split_size = dataset_length // 4\n",
        "\n",
        "# # 7. 각 파트를 pkl 파일로 저장\n",
        "# for i in range(4):\n",
        "#     start_idx = i * split_size\n",
        "#     end_idx = (i + 1) * split_size if i < 3 else dataset_length  # 마지막 파트는 끝까지\n",
        "\n",
        "#     part_dataset = tokenized_datasets['train'].select(range(start_idx, end_idx))\n",
        "\n",
        "#     output_file = f\"/content/drive/MyDrive/combined/캡스톤 데이터 전처리 파일/translator/processed_dataset_part_{i+1}.pkl\"\n",
        "#     with open(output_file, 'wb') as f:\n",
        "#         pickle.dump(part_dataset, f)\n",
        "\n",
        "#     print(f\"Tokenized dataset part {i+1} saved to {output_file}\")"
      ],
      "metadata": {
        "id": "HcPSinmf4OXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#학습 세팅(토크나이징 된 데이터 받음)\n"
      ],
      "metadata": {
        "id": "Y-enH3CPTkb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #모델과 데이터셋 설정, 처음 학습시킬때, 온라인 모델 받아와서 사용\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oiEwZwd3JrgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델과 데이터셋 설정, 로컬로 불러와서 학습시키는 경우\n",
        "model_name=\"/content/drive/MyDrive/translator/model_part_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "_O9MPP5SH5uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습 파라미터 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/translator/checkpoints\",  # 모델 및 체크포인트가 저장될 디렉토리\n",
        "    evaluation_strategy=\"epoch\",  # 에폭마다 평가\n",
        "    save_strategy=\"epoch\",  # 에폭마다 체크포인트 저장\n",
        "    save_total_limit=3,  # 최근 3개의 체크포인트만 유지\n",
        "    logging_steps=100,  # 로그를 출력할 스텝 간격\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,  # 각 파일별로 5 에폭씩 학습\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,  # 학습 종료 시 가장 성능이 좋은 모델 불러오기\n",
        "    save_on_each_node=False,  # 모든 노드에서 체크포인트를 저장하지 않음 (분산 학습 시)\n",
        "    logging_dir=\"/content/drive/MyDrive/translator/logging\",  # 로그를 저장할 디렉토리\n",
        ")\n"
      ],
      "metadata": {
        "id": "L1YSsDMVNgek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_path = \"/content/drive/MyDrive/translator/checkpoints\""
      ],
      "metadata": {
        "id": "7TPKrnSqKjVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #모델이 존재하는 경우 사용할 필요가 없음\n",
        "#last_checkpoint = None\n",
        "#if os.path.isdir(model_checkpoint_path) and os.listdir(model_checkpoint_path):\n",
        "#    last_checkpoint = Trainer.get_last_checkpoint(model_checkpoint_path)\n",
        "#    if last_checkpoint is not None:\n",
        "#        print(f\"체크포인트에서 이어서 학습을 시작합니다: {last_checkpoint}\")\n",
        "#    else:\n",
        "#        print(\"체크포인트가 없습니다. 새로 학습을 시작합니다.\")\n",
        "#else:\n",
        " #   print(\"체크포인트 디렉토리가 존재하지 않습니다. 새로 학습을 시작합니다.\")\n"
      ],
      "metadata": {
        "id": "-OoBCklVKh3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#학습(학습 파일은 여러개로 되어있음)"
      ],
      "metadata": {
        "id": "LTj35pyhEMht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. 데이터셋 리스트\n",
        "# file_paths = [\n",
        "#     \"/content/drive/MyDrive/combined/캡스톤 데이터 전처리 파일/translator/processed_dataset_part_1.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/캡스톤 데이터 전처리 파일/translator/processed_dataset_part_2.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/캡스톤 데이터 전처리 파일/translator/processed_dataset_part_3.pkl\",\n",
        "#     \"/content/drive/MyDrive/combined/캡스톤 데이터 전처리 파일/translator/processed_dataset_part_4.pkl\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "WWdZzSK05pTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_preprocessed_data(file_path):\n",
        "#     with open(file_path, 'rb') as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "#     return tokenized_datasets"
      ],
      "metadata": {
        "id": "-BWsdHUzk40M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_preprocessed_data(file_path, split_ratio=0.8):\n",
        "#     with open(file_path, 'rb') as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "\n",
        "#     # 학습 데이터와 평가 데이터를 나눔 (default: 80% train, 20% eval)\n",
        "#     train_size = int(split_ratio * len(tokenized_datasets))\n",
        "#     train_dataset = tokenized_datasets[:train_size]\n",
        "#     eval_dataset = tokenized_datasets[train_size:]\n",
        "\n",
        "#     return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "6D-6Ecu4csnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from datasets import DatasetDict\n",
        "\n",
        "def load_preprocessed_data(file_path, split_ratio=0.8):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        tokenized_datasets = pickle.load(f)\n",
        "\n",
        "    # tokenized_datasets가 Dataset 형식일 경우 train_test_split 사용\n",
        "    if isinstance(tokenized_datasets, DatasetDict):\n",
        "        split_data = tokenized_datasets.train_test_split(test_size=(1 - split_ratio))\n",
        "        train_dataset = split_data['train']\n",
        "        eval_dataset = split_data['test']\n",
        "    else:\n",
        "        # 리스트나 배열인 경우 슬라이싱으로 나눔\n",
        "        train_size = int(split_ratio * len(tokenized_datasets))\n",
        "        train_dataset = tokenized_datasets[:train_size]\n",
        "        eval_dataset = tokenized_datasets[train_size:]\n",
        "\n",
        "    return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "w_YBXCIMECIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenized_data(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        tokenized_datasets = pickle.load(f)\n",
        "    return tokenized_datasets"
      ],
      "metadata": {
        "id": "HApeA9xfFQwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import pickle\n",
        "\n",
        "# 전처리 함수 (동일하게 사용)\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['ko']  # 'ko' 열에서 한국어 텍스트를 가져옴\n",
        "    targets = examples['en']  # 'en' 열에서 영어 번역 텍스트를 가져옴\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # 타겟 텍스트도 토크나이징\n",
        "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# 학습 파라미터 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"/content/drive/MyDrive/translator/checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 6개의 데이터셋을 순차적으로 불러와 학습 진행\n",
        "#한 데이터셋의 학습이 끝나면 하나씩 지우고 길이 줄이기 1~7 -> 2~7 -> 3~7...\n",
        "for i in range(3, 7):\n",
        "    print(f\"Loading dataset part {i}...\")\n",
        "\n",
        "    # 저장된 파일 불러오기\n",
        "    with open(f\"/content/drive/MyDrive/translator/data/part_{i}.pkl\", \"rb\") as f:\n",
        "        part = pickle.load(f)\n",
        "\n",
        "    # 파트마다 전처리 및 토큰화\n",
        "    tokenized_part = part.map(preprocess_function, batched=True)\n",
        "\n",
        "    # Trainer 설정\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_part,  # 현재 파트 데이터셋으로 학습\n",
        "        eval_dataset=tokenized_part,   # 평가도 동일한 데이터셋으로 설정\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # api키는 이걸로 넣으면 됨 513a1f0c050fa7f60a76b5232e904d8df397082e\n",
        "    # 학습 시작\n",
        "    print(f\"Training on dataset part {i}...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Training interrupted during part {i}. Last checkpoint saved.\")\n",
        "\n",
        "    # 체크포인트 저장\n",
        "    trainer.save_model(f\"/content/drive/MyDrive/translator/model/model_part_{i}\")\n",
        "    print(f\"Model checkpoint saved after part {i}\")\n"
      ],
      "metadata": {
        "id": "XJK2ovNrTXBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/translator/final_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/translator/final_model\")\n",
        "print(\"Final model and tokenizer saved.\")"
      ],
      "metadata": {
        "id": "ljY1cX3IDxZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#입력 및 예측\n"
      ],
      "metadata": {
        "id": "6xCogH9V-u0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 1. 토크나이저와 모델 로드 (학습이 된다면 변경 필요)\n",
        "\n",
        "translator_model_name = \"EXP442/nllb_translator_pretrained\"\n",
        "translator_tokenizer = AutoTokenizer.from_pretrained(translator_model_name, src_lang='eng_Latn', tgt_lang='kor_Hang')\n",
        "translator_model = AutoModelForSeq2SeqLM.from_pretrained(translator_model_name, forced_bos_token_id=256098).to(device)\n",
        "\n",
        "input_text=\"what should i say to you?\"\n",
        "\n",
        "inputs = translation_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "translated = translation_model.generate(**inputs)\n",
        "translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "# 6. 번역 결과 출력\n",
        "print(f\"번역된 영어 문장: {translated_text}\")\n"
      ],
      "metadata": {
        "id": "AVzeIKyx-yq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}