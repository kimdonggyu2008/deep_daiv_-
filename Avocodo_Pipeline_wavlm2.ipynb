{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/deep_daiv_-/blob/main/Avocodo_Pipeline_wavlm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZk8X1Cx3G3_"
      },
      "source": [
        "#원본 코드 + wavlm\n",
        "\n",
        "https://github.com/ncsoft/avocodo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7waehhoSRy"
      },
      "source": [
        "#Avocodo 사전 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE6aC-JGFmw_",
        "outputId": "fac33535-c74c-4079-e228-11e19ae56b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeTTabtv8WJ",
        "outputId": "7de022ee-364a-424e-8710-6e76dbe5593c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlbN1YmJkwVm",
        "outputId": "806f2164-a852-4c03-8ece-1a25d6b9199f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwxv46aFl6tl",
        "outputId": "3f7e3057-07d7-4326-b5f8-313cf2c0a220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: OmegaConf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqwXdH8ARxSb",
        "outputId": "43fa5916-c469-4b2f-cdc4-7fe0af643d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OkW3eBaFFtxT"
      },
      "outputs": [],
      "source": [
        "# 공통으로 사용되는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import warnings\n",
        "import itertools\n",
        "from itertools import chain\n",
        "from scipy import signal as sig\n",
        "from scipy.signal.windows import kaiser\n",
        "from omegaconf import OmegaConf\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 처리 관련 라이브러리\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal as sig\n",
        "import librosa\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "from librosa.util import normalize\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "# PyTorch 및 TensorBoard 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from torch.distributed import init_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 시각화 및 플롯 관련 라이브러리\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# 유틸리티 관련 모듈\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uqn-XddwDcT",
        "outputId": "41df8f34-4da4-47c1-d1a0-65f0685d1856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdkkim2008\u001b[0m (\u001b[33mdkkim2008-hankuk-university-for-foreign-studies\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"513a1f0c050fa7f60a76b5232e904d8df397082e\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuKV0hhkiCFp"
      },
      "source": [
        "#meldataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hkgki34ooeUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def load_wav(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return data, sampling_rate\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5): #튀는 부분 처리\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1): #작은 부분 키우기\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5): #토치버전 튀는 부분 처리\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1): #토치버전 작은 부분 키우기\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes): #토치 버전 스펙트로그램 정규화\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes): #토치버전 스펙트로그램 비정규화\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SKv2Mxbvoiba"
      },
      "outputs": [],
      "source": [
        "mel_basis = {}\n",
        "hann_window = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bdi0Xs8BohoR"
      },
      "outputs": [],
      "source": [
        "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.: #정규화 여부\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NcaNp8vHokMx"
      },
      "outputs": [],
      "source": [
        "def get_dataset_filelist(\n",
        "    input_wavs_dir,\n",
        "    input_training_file,\n",
        "    input_validation_file\n",
        "):\n",
        "    with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
        "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
        "\n",
        "    with open(input_validation_file, 'r', encoding='utf-8') as fi:\n",
        "        validation_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
        "                            for x in fi.read().split('\\n') if len(x) > 0]\n",
        "    return training_files, validation_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MDcNWVMHiD3R"
      },
      "outputs": [],
      "source": [
        "class MelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "                 hop_size, win_size, sampling_rate, fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "                 fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "        self.audio_files = training_files\n",
        "        random.seed(1234)\n",
        "        if shuffle:\n",
        "            random.shuffle(self.audio_files)\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.split = split\n",
        "        self.n_fft = n_fft\n",
        "        self.num_mels = num_mels\n",
        "        self.hop_size = hop_size\n",
        "        self.win_size = win_size\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "        self.fmax_loss = fmax_loss\n",
        "        self.cached_wav = None\n",
        "        self.n_cache_reuse = n_cache_reuse\n",
        "        self._cache_ref_count = 0\n",
        "        self.fine_tuning = fine_tuning\n",
        "        self.base_mels_path = base_mels_path\n",
        "\n",
        "        if self.base_mels_path:\n",
        "            os.makedirs(self.base_mels_path, exist_ok=True)\n",
        "\n",
        "    def __getitem__(self, index):  # 오디오 파일들 가져옴\n",
        "        filename = self.audio_files[index]\n",
        "        mel_filename = os.path.join(\n",
        "            self.base_mels_path,\n",
        "            os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'\n",
        "        )\n",
        "\n",
        "        # 멜 스펙트로그램 존재 여부 확인\n",
        "        if os.path.exists(mel_filename):\n",
        "            # 저장된 멜 스펙트로그램 로드\n",
        "            mel = np.load(mel_filename)\n",
        "            mel = torch.from_numpy(mel)\n",
        "            if len(mel.shape) < 3:  # 3D가 아니면 차원 확장\n",
        "                mel = mel.unsqueeze(0)  # [1, num_mels, num_frames]\n",
        "\n",
        "            # audio 초기화\n",
        "            audio, sampling_rate = load_wav(filename)\n",
        "            audio = audio / MAX_WAV_VALUE\n",
        "            audio = torch.FloatTensor(audio).unsqueeze(0)  # [1, num_samples]\n",
        "\n",
        "        else:\n",
        "            # 파일이 없을 경우 기존 로직 실행\n",
        "            if self._cache_ref_count == 0:\n",
        "                audio, sampling_rate = load_wav(filename)\n",
        "                audio = audio / MAX_WAV_VALUE\n",
        "                if not self.fine_tuning:  # 파인 튜닝 여부에 따라 정규화, 첫 학습인 경우 npy\n",
        "                    audio = normalize(audio) * 0.95  # [num_samples]\n",
        "                self.cached_wav = audio\n",
        "                if sampling_rate != self.sampling_rate:  # 샘플레이트 맞추기\n",
        "                    raise ValueError(f\"{sampling_rate} SR doesn't match target {self.sampling_rate}\")\n",
        "                self._cache_ref_count = self.n_cache_reuse\n",
        "            else:\n",
        "                audio = self.cached_wav\n",
        "                self._cache_ref_count -= 1\n",
        "\n",
        "            audio = torch.FloatTensor(audio).unsqueeze(0)  # [1, num_samples]\n",
        "\n",
        "            if not self.fine_tuning:\n",
        "                if self.split:\n",
        "                    if audio.size(1) >= self.segment_size:  # 샘플 길이가 세그먼트 갯수보다 긴 경우\n",
        "                        max_audio_start = audio.size(1) - self.segment_size\n",
        "                        audio_start = random.randint(0, max_audio_start)\n",
        "                        audio = audio[:, audio_start:audio_start + self.segment_size]\n",
        "                    else:\n",
        "                        audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "                mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                      self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "                                      center=False)\n",
        "                # 멜 스펙트로그램 저장\n",
        "                np.save(mel_filename, mel.numpy())\n",
        "\n",
        "        # Fine-tuning 또는 다른 로직 적용\n",
        "        mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "                                  self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "                                  center=False)\n",
        "        print(\"1\",mel.shape)\n",
        "        print(\"2\",audio.shape)\n",
        "        #print(\"3\",filename.shape)\n",
        "        print(\"4\",mel_loss.shape)\n",
        "        return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ba_1-isS0JQR"
      },
      "outputs": [],
      "source": [
        "#원본\n",
        "# class MelDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, training_files, segment_size, n_fft, num_mels,\n",
        "#                  hop_size, win_size, sampling_rate, fmin, fmax, split=True, shuffle=True, n_cache_reuse=1,\n",
        "#                  fmax_loss=None, fine_tuning=False, base_mels_path=None):\n",
        "#         self.audio_files = training_files\n",
        "#         random.seed(1234)\n",
        "#         if shuffle:\n",
        "#             random.shuffle(self.audio_files)\n",
        "#         self.segment_size = segment_size\n",
        "#         self.sampling_rate = sampling_rate\n",
        "#         self.split = split\n",
        "#         self.n_fft = n_fft\n",
        "#         self.num_mels = num_mels\n",
        "#         self.hop_size = hop_size\n",
        "#         self.win_size = win_size\n",
        "#         self.fmin = fmin\n",
        "#         self.fmax = fmax\n",
        "#         self.fmax_loss = fmax_loss\n",
        "#         self.cached_wav = None\n",
        "#         self.n_cache_reuse = n_cache_reuse\n",
        "#         self._cache_ref_count = 0\n",
        "#         self.fine_tuning = fine_tuning\n",
        "#         self.base_mels_path = base_mels_path\n",
        "\n",
        "#         if self.base_mels_path:\n",
        "#             os.makedirs(self.base_mels_path, exist_ok=True)\n",
        "\n",
        "#     def __getitem__(self, index):  # 오디오 파일들 가져옴\n",
        "#         filename = self.audio_files[index]\n",
        "#         mel_filename = os.path.join(\n",
        "#             self.base_mels_path,\n",
        "#             os.path.splitext(os.path.split(filename)[-1])[0] + '.npy'\n",
        "#         )\n",
        "\n",
        "#         # 멜 스펙트로그램 존재 여부 확인\n",
        "#         if os.path.exists(mel_filename):\n",
        "#             # 저장된 멜 스펙트로그램 로드\n",
        "#             mel = np.load(mel_filename)\n",
        "#             mel = torch.from_numpy(mel)\n",
        "#             if len(mel.shape) < 3:  # 3D가 아니면 차원 확장\n",
        "#                 mel = mel.unsqueeze(0)  # [1, num_mels, num_frames]\n",
        "\n",
        "#             # audio 초기화\n",
        "#             audio, sampling_rate = load_wav(filename)\n",
        "#             audio = audio / MAX_WAV_VALUE\n",
        "#             audio = torch.FloatTensor(audio).unsqueeze(0)  # [1, num_samples]\n",
        "\n",
        "#         else:\n",
        "#             # 파일이 없을 경우 기존 로직 실행\n",
        "#             if self._cache_ref_count == 0:\n",
        "#                 audio, sampling_rate = load_wav(filename)\n",
        "#                 audio = audio / MAX_WAV_VALUE\n",
        "#                 if not self.fine_tuning:  # 파인 튜닝 여부에 따라 정규화, 첫 학습인 경우 npy\n",
        "#                     audio = normalize(audio) * 0.95  # [num_samples]\n",
        "#                 self.cached_wav = audio\n",
        "#                 if sampling_rate != self.sampling_rate:  # 샘플레이트 맞추기\n",
        "#                     raise ValueError(f\"{sampling_rate} SR doesn't match target {self.sampling_rate}\")\n",
        "#                 self._cache_ref_count = self.n_cache_reuse\n",
        "#             else:\n",
        "#                 audio = self.cached_wav\n",
        "#                 self._cache_ref_count -= 1\n",
        "\n",
        "#             audio = torch.FloatTensor(audio).unsqueeze(0)  # [1, num_samples]\n",
        "\n",
        "#             if not self.fine_tuning:\n",
        "#                 if self.split:\n",
        "#                     if audio.size(1) >= self.segment_size:  # 샘플 길이가 세그먼트 갯수보다 긴 경우\n",
        "#                         max_audio_start = audio.size(1) - self.segment_size\n",
        "#                         audio_start = random.randint(0, max_audio_start)\n",
        "#                         audio = audio[:, audio_start:audio_start + self.segment_size]\n",
        "#                     else:\n",
        "#                         audio = torch.nn.functional.pad(audio, (0, self.segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "#                 mel = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "#                                       self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax,\n",
        "#                                       center=False)\n",
        "#                 # 멜 스펙트로그램 저장\n",
        "#                 np.save(mel_filename, mel.numpy())\n",
        "\n",
        "#         # Fine-tuning 또는 다른 로직 적용\n",
        "#         mel_loss = mel_spectrogram(audio, self.n_fft, self.num_mels,\n",
        "#                                   self.sampling_rate, self.hop_size, self.win_size, self.fmin, self.fmax_loss,\n",
        "#                                   center=False)\n",
        "#         return (mel.squeeze(), audio.squeeze(0), filename, mel_loss.squeeze())\n",
        "\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.audio_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EDpRRRZ8QFix"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "# # MelDataset 클래스에서 멜 스펙트로그램 생성 함수 필요\n",
        "# def generate_mel_and_save(audio_files, segment_size, n_fft, num_mels, hop_size, win_size,\n",
        "#                           sampling_rate, fmin, fmax, output_dir):\n",
        "#     for index, file_path in enumerate(audio_files):\n",
        "#         try:\n",
        "#             # 1. Load wav file\n",
        "#             audio, sampling_rate_loaded = load_wav(file_path)\n",
        "#             audio = audio / MAX_WAV_VALUE\n",
        "#             audio = normalize(audio) * 0.95  # Normalize audio\n",
        "\n",
        "#             # 2. Ensure correct sampling rate\n",
        "#             if sampling_rate != sampling_rate_loaded:\n",
        "#                 raise ValueError(f\"File {file_path} has mismatched sampling rate {sampling_rate_loaded}.\")\n",
        "\n",
        "#             # 3. Convert to PyTorch Tensor and add batch dimension\n",
        "#             audio = torch.FloatTensor(audio).unsqueeze(0)  # Shape: [1, num_samples]\n",
        "\n",
        "#             # # 4. Segment audio if it's longer than the segment size\n",
        "#             # if audio.size(1) >= segment_size:\n",
        "#             #     max_audio_start = audio.size(1) - segment_size\n",
        "#             #     audio_start = random.randint(0, max_audio_start)\n",
        "#             #     audio = audio[:, audio_start:audio_start + segment_size]\n",
        "#             # else:\n",
        "#             #     # Pad if shorter than segment size\n",
        "#             #     audio = torch.nn.functional.pad(audio, (0, segment_size - audio.size(1)), 'constant')\n",
        "\n",
        "#             # 5. Generate mel spectrogram\n",
        "#             mel = mel_spectrogram(audio, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False)\n",
        "\n",
        "#             # 6. Save mel spectrogram as .npy\n",
        "#             file_name = os.path.splitext(os.path.basename(file_path))[0]  # Get file name without extension\n",
        "#             npy_path = os.path.join(output_dir, f\"{file_name}.npy\")\n",
        "#             np.save(npy_path, mel.numpy())  # Save mel spectrogram as numpy file\n",
        "\n",
        "#             print(f\"Processed and saved: {npy_path}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "# # Example usage with configuration\n",
        "# data_config = {\n",
        "#     \"segment_size\": 8192,\n",
        "#     \"num_mels\": 80,\n",
        "#     \"n_fft\": 1024,\n",
        "#     \"hop_size\": 256,\n",
        "#     \"win_size\": 1024,\n",
        "#     \"sampling_rate\": 22050,\n",
        "#     \"fmin\": 0,\n",
        "#     \"fmax\": 8000\n",
        "# }\n",
        "\n",
        "# # File paths\n",
        "# input_wavs_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs'\n",
        "# output_mels_dir = '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram'\n",
        "\n",
        "# # Gather all wav files\n",
        "# wav_files = [os.path.join(input_wavs_dir, f) for f in os.listdir(input_wavs_dir) if f.endswith('.wav')]\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_mels_dir, exist_ok=True)\n",
        "\n",
        "# # Generate and save mel spectrograms\n",
        "# generate_mel_and_save(\n",
        "#     audio_files=wav_files,\n",
        "#     segment_size=data_config[\"segment_size\"],\n",
        "#     n_fft=data_config[\"n_fft\"],\n",
        "#     num_mels=data_config[\"num_mels\"],\n",
        "#     hop_size=data_config[\"hop_size\"],\n",
        "#     win_size=data_config[\"win_size\"],\n",
        "#     sampling_rate=data_config[\"sampling_rate\"],\n",
        "#     fmin=data_config[\"fmin\"],\n",
        "#     fmax=data_config[\"fmax\"],\n",
        "#     output_dir=output_mels_dir\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owc_vcWhiAC"
      },
      "source": [
        "#utils.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KxozRiQYhj-y"
      },
      "outputs": [],
      "source": [
        "def get_padding(kernel_size, dilation=1): #제로 패딩 추가\n",
        "    return int((kernel_size*dilation - dilation)/2)\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01): #가중치 초기화\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98DxXGMdhmT5"
      },
      "source": [
        "#losses.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8VpuHS2KhoVJ"
      },
      "outputs": [],
      "source": [
        "def feature_loss(fmap_r, fmap_g): #생성된 특성 맵, 실제 특성 맵 비교\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    for dr, dg in zip(fmap_r, fmap_g): #각각에 대해 절댓값으로 비교\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            _loss = torch.mean(torch.abs(rl - gl))\n",
        "            loss += _loss\n",
        "        losses.append(_loss)\n",
        "\n",
        "    return loss*2, losses\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs): #실제값, 생성값 로스값 비교\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1-dr)**2)\n",
        "        g_loss = torch.mean(dg**2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs): #생성기 로스값\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1-dg)**2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcMzi-hiajZ"
      },
      "source": [
        "#pqmf.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZAzKgcDTpCEh"
      },
      "outputs": [],
      "source": [
        "def design_prototype_filter(taps=62, cutoff_ratio=0.142, beta=9.0): #프로토타입 필터 예시\n",
        "    \"\"\"Design prototype filter for PQMF.\n",
        "    This method is based on `A Kaiser window approach for the design of prototype\n",
        "    filters of cosine modulated filterbanks`_.\n",
        "    Args:\n",
        "        taps (int): The number of filter taps.\n",
        "        cutoff_ratio (float): Cut-off frequency ratio.\n",
        "        beta (float): Beta coefficient for kaiser window.\n",
        "    Returns:\n",
        "        ndarray: Impluse response of prototype filter (taps + 1,).\n",
        "    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n",
        "        https://ieeexplore.ieee.org/abstract/document/681427\n",
        "    \"\"\"\n",
        "    # check the arguments are valid, 제약조건\n",
        "    assert taps % 2 == 0, \"The number of taps mush be even number.\" #소수 지정\n",
        "    assert 0.0 < cutoff_ratio < 1.0, \"Cutoff ratio must be > 0.0 and < 1.0.\" #컷오프 비율\n",
        "\n",
        "    # make initial filter\n",
        "    omega_c = np.pi * cutoff_ratio #차단 주파수 계산\n",
        "    with np.errstate(invalid=\"ignore\"): #차단 주파수 기준으로 sinc함수로 필터 설계\n",
        "        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) / (\n",
        "            np.pi * (np.arange(taps + 1) - 0.5 * taps)\n",
        "        )\n",
        "    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # 완전 중심값의 경우, 분모가 0이므로 에러 발생하므로 예외처리\n",
        "\n",
        "    # apply kaiser window\n",
        "    w = kaiser(taps + 1, beta)# 양 옆으로 있고, 중간에 1개만 있으므로 +1, 베타를 가지는 카이저 윈도우\n",
        "    h = h_i * w #\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVkne6sHDnXu"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gob2R_Voicxx"
      },
      "outputs": [],
      "source": [
        "class PQMF(torch.nn.Module):\n",
        "    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.142, beta=9.0):\n",
        "\n",
        "        super(PQMF, self).__init__()\n",
        "\n",
        "        # build analysis & synthesis filter coefficients\n",
        "        h_proto = design_prototype_filter(taps, cutoff_ratio, beta) #필터 생성\n",
        "        h_analysis = np.zeros((subbands, len(h_proto))) #서브 밴드 대역만큼 0으로 된 배열 생성\n",
        "        h_synthesis = np.zeros((subbands, len(h_proto))) #합성도 동일한 크기의 배열 생성\n",
        "        for k in range(subbands): #각 대역별로 분석, 합성을 진행\n",
        "            h_analysis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    + (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "            h_synthesis[k] = (\n",
        "                2\n",
        "                * h_proto\n",
        "                * np.cos(\n",
        "                    (2 * k + 1)\n",
        "                    * (np.pi / (2 * subbands))\n",
        "                    * (np.arange(taps + 1) - (taps / 2))\n",
        "                    - (-1) ** k * np.pi / 4\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # convert to tensor\n",
        "        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1)\n",
        "        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n",
        "\n",
        "        # register coefficients as beffer\n",
        "        self.register_buffer(\"analysis_filter\", analysis_filter)\n",
        "        self.register_buffer(\"synthesis_filter\", synthesis_filter)\n",
        "\n",
        "        # filter for downsampling & upsampling\n",
        "        updown_filter = torch.zeros((subbands, subbands, subbands)).float()\n",
        "        for k in range(subbands):\n",
        "            updown_filter[k, k, 0] = 1.0\n",
        "        self.register_buffer(\"updown_filter\", updown_filter)\n",
        "        self.subbands = subbands\n",
        "\n",
        "        # keep padding info\n",
        "        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n",
        "\n",
        "    def analysis(self, x): #다운 샘플링\n",
        "        x = F.conv1d(self.pad_fn(x), self.analysis_filter) #패딩 추가, [batch_size, subbands, t]\n",
        "        return F.conv1d(x, self.updown_filter, stride=self.subbands)#(batch_size, sub_bands,t//sub_bands)\n",
        "\n",
        "    def synthesis(self, x): #업 샘플링\n",
        "        x = F.conv_transpose1d(\n",
        "            x, self.updown_filter * self.subbands, stride=self.subbands\n",
        "        )#[batch_size, subbands, t//subbands]\n",
        "        return F.conv1d(self.pad_fn(x), self.synthesis_filter)#[batch_size, 1, t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQ3Q0mqht0K"
      },
      "source": [
        "#CoMBD.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HgOcC75wouAx"
      },
      "outputs": [],
      "source": [
        "class CoMBDBlock(torch.nn.Module):\n",
        "    def __init__( #CoMBD 블록 선언\n",
        "        self,\n",
        "        h_u: List[int], #각 계층별 출력 갯수(히든 유닛) 리스트\n",
        "        d_k: List[int], #각 계층의 커널 크기 리스트\n",
        "        d_s: List[int], #각 계층의 스트라이드 리스트\n",
        "        d_d: List[int], #각 계층의 dilation(확장 계수) 리스트\n",
        "        d_g: List[int], #각 계층의 그룹 수 리스트\n",
        "        d_p: List[int], #각 계층의 패딩 크기 리스트\n",
        "        op_f: int, #프로젝션 계층의 출력 채널 크기\n",
        "        op_k: int, #프로젝션 계층의 커널 크기\n",
        "        op_g: int, #프로젝션 계층의 그룹 수\n",
        "        use_spectral_norm=False #spectral normalization 적용여부\n",
        "    ):\n",
        "        super(CoMBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters = [[1, h_u[0]]] #출력 필터 지정\n",
        "        for i in range(len(h_u) - 1): #각 계층마다 필터 지정\n",
        "            filters.append([h_u[i], h_u[i + 1]]) #각 필터 시작지점, 끝지점으로 묶기\n",
        "        for _f, _k, _s, _d, _g, _p in zip(filters, d_k, d_s, d_d, d_g, d_p):#각 대역폭별로 사용될 데이터 묶기\n",
        "            self.convs.append(norm_f(\n",
        "                Conv1d(\n",
        "                    in_channels=_f[0],\n",
        "                    out_channels=_f[1],\n",
        "                    kernel_size=_k,\n",
        "                    stride=_s,\n",
        "                    dilation=_d,\n",
        "                    groups=_g,\n",
        "                    padding=_p\n",
        "                )\n",
        "            ))\n",
        "        self.projection_conv = norm_f( #제일 마지막 프로젝션 레이어\n",
        "            Conv1d(\n",
        "                in_channels=filters[-1][1],\n",
        "                out_channels=op_f,\n",
        "                kernel_size=op_k,\n",
        "                groups=op_g\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for block in self.convs:\n",
        "            x = block(x) #블록\n",
        "            x = F.leaky_relu(x, 0.2) #활성화 함수\n",
        "            fmap.append(x) #묶어서 추가\n",
        "        x = self.projection_conv(x)\n",
        "        return x, fmap    #CoMBD 블록 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7-ga7kQJhs-0"
      },
      "outputs": [],
      "source": [
        "class CoMBD(torch.nn.Module):\n",
        "    def __init__(self, h, pqmf_list=None, use_spectral_norm=False):\n",
        "        super(CoMBD, self).__init__()\n",
        "        self.h = h #하이퍼 파라미터 설정 객체\n",
        "        if pqmf_list is not None: #대역 분할 있으면, 지정\n",
        "            self.pqmf = pqmf_list\n",
        "        else: #대역 분할이 없으면 LV1, LV2로 분할\n",
        "            self.pqmf = [\n",
        "                PQMF(*h.pqmf_config[\"lv2\"]),\n",
        "                PQMF(*h.pqmf_config[\"lv1\"])\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.ModuleList() #CoMBD 블럭의 데이터 가져옴\n",
        "        for _h_u, _d_k, _d_s, _d_d, _d_g, _d_p, _op_f, _op_k, _op_g in zip(\n",
        "            h.combd_h_u,\n",
        "            h.combd_d_k,\n",
        "            h.combd_d_s,\n",
        "            h.combd_d_d,\n",
        "            h.combd_d_g,\n",
        "            h.combd_d_p,\n",
        "            h.combd_op_f,\n",
        "            h.combd_op_k,\n",
        "            h.combd_op_g,\n",
        "        ):\n",
        "            self.blocks.append(CoMBDBlock( #COMBD 블럭들 쌓기\n",
        "                _h_u,\n",
        "                _d_k,\n",
        "                _d_s,\n",
        "                _d_d,\n",
        "                _d_g,\n",
        "                _d_p,\n",
        "                _op_f,\n",
        "                _op_k,\n",
        "                _op_g,\n",
        "            ))\n",
        "\n",
        "    def _block_forward(self, input, blocks, outs, f_maps): #순차적 통과\n",
        "        for x, block in zip(input, blocks):\n",
        "            out, f_map = block(x) #각 계층마다의 중간맵 출력\n",
        "            outs.append(out)\n",
        "            f_maps.append(f_map)\n",
        "        return outs, f_maps\n",
        "\n",
        "    def _pqmf_forward(self, ys, ys_hat): #PQMF를 통한 신호의 대역폭 분할\n",
        "        # ys는 실제 신호 리스트 - training_step 파트에서 만들어짐\n",
        "        # ys_hat은 생성된 신호 리스트\n",
        "        #  ys = [\n",
        "        #self.pqmf_lv2.analysis(y)[:, :self.hparams.generator.projection_filters[1]],  # PQMF Level 2\n",
        "        #self.pqmf_lv1.analysis(y)[:, :self.hparams.generator.projection_filters[2]],  # PQMF Level 1\n",
        "        #y  ] # 원래 입력 신호\n",
        "        multi_scale_inputs = []\n",
        "        multi_scale_inputs_hat = []\n",
        "        for pqmf in self.pqmf: #lv1, lv2가 들어가있는 pqmf 객체 리스트에 다운샘플링\n",
        "            multi_scale_inputs.append(  #analysis 함수는 일종의 다운샘플링\n",
        "                pqmf.to(ys[-1]).analysis(ys[-1])[:, :1, :] #batch_size, subband,t\n",
        "            )\n",
        "            multi_scale_inputs_hat.append(\n",
        "                pqmf.to(ys[-1]).analysis(ys_hat[-1])[:, :1, :]\n",
        "            )\n",
        "\n",
        "        outs_real = []\n",
        "        f_maps_real = []\n",
        "        # real\n",
        "        # for hierarchical forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            ys, self.blocks, outs_real, f_maps_real)\n",
        "        # for multi_scale forward\n",
        "        outs_real, f_maps_real = self._block_forward(\n",
        "            multi_scale_inputs, self.blocks[:-1], outs_real, f_maps_real)\n",
        "\n",
        "        outs_fake = []\n",
        "        f_maps_fake = []\n",
        "        # predicted\n",
        "        # for hierarchical forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            ys_hat, self.blocks, outs_fake, f_maps_fake)\n",
        "        # for multi_scale forward\n",
        "        outs_fake, f_maps_fake = self._block_forward(\n",
        "            multi_scale_inputs_hat, self.blocks[:-1], outs_fake, f_maps_fake)\n",
        "\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake\n",
        "\n",
        "    def forward(self, ys, ys_hat):\n",
        "        outs_real, outs_fake, f_maps_real, f_maps_fake = self._pqmf_forward(\n",
        "            ys, ys_hat)\n",
        "        return outs_real, outs_fake, f_maps_real, f_maps_fake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeFVq4_hzq6"
      },
      "source": [
        "#SBD.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OBMUu1GRozy6"
      },
      "outputs": [],
      "source": [
        "class MDC(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(MDC, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.d_convs = nn.ModuleList()\n",
        "        for _k, _d in zip(kernel_size, dilations):\n",
        "            self.d_convs.append(\n",
        "                norm_f(Conv1d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=_k,\n",
        "                    dilation=_d,\n",
        "                    padding=get_padding(_k, _d)\n",
        "                ))\n",
        "            )\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=strides,\n",
        "            padding=get_padding(_k, _d)\n",
        "        ))\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _out = None\n",
        "        for _l in self.d_convs:\n",
        "            _x = torch.unsqueeze(_l(x), -1)\n",
        "            _x = F.leaky_relu(_x, 0.2)\n",
        "            if _out is None:\n",
        "                _out = _x\n",
        "            else:\n",
        "                _out = torch.cat([_out, _x], axis=-1)\n",
        "        x = torch.sum(_out, dim=-1)\n",
        "        x = self.post_conv(x)\n",
        "        x = F.leaky_relu(x, 0.2)  # @@\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i2OaM69uh0_5"
      },
      "outputs": [],
      "source": [
        "class SBDBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        segment_dim,\n",
        "        strides,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        dilations,\n",
        "        use_spectral_norm=False\n",
        "    ):\n",
        "        super(SBDBlock, self).__init__()\n",
        "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
        "        self.convs = nn.ModuleList()\n",
        "        filters_in_out = [(segment_dim, filters[0])]\n",
        "        for i in range(len(filters) - 1):\n",
        "            filters_in_out.append([filters[i], filters[i + 1]])\n",
        "\n",
        "        for _s, _f, _k, _d in zip(\n",
        "            strides,\n",
        "            filters_in_out,\n",
        "            kernel_size,\n",
        "            dilations\n",
        "        ):\n",
        "            self.convs.append(MDC(\n",
        "                in_channels=_f[0],\n",
        "                out_channels=_f[1],\n",
        "                strides=_s,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "        self.post_conv = norm_f(Conv1d(\n",
        "            in_channels=_f[1],\n",
        "            out_channels=1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=3 // 2\n",
        "        ))  # @@\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for _l in self.convs:\n",
        "            x = _l(x)\n",
        "            fmap.append(x)\n",
        "        x = self.post_conv(x)  # @@\n",
        "\n",
        "        return x, fmap\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wuGqG0y4o4oB"
      },
      "outputs": [],
      "source": [
        "class MDCDConfig:\n",
        "    def __init__(self, h):\n",
        "        self.pqmf_params = h.pqmf_config[\"sbd\"]\n",
        "        self.f_pqmf_params = h.pqmf_config[\"fsbd\"]\n",
        "        self.filters = h.sbd_filters\n",
        "        self.kernel_sizes = h.sbd_kernel_sizes\n",
        "        self.dilations = h.sbd_dilations\n",
        "        self.strides = h.sbd_strides\n",
        "        self.band_ranges = h.sbd_band_ranges\n",
        "        self.transpose = h.sbd_transpose\n",
        "        self.segment_size = h.segment_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2H-kRPzzo3dp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SBD(torch.nn.Module):\n",
        "    def __init__(self, h, use_spectral_norm=False):\n",
        "        super(SBD, self).__init__()\n",
        "        self.config = MDCDConfig(h)\n",
        "        self.pqmf = PQMF(\n",
        "            *self.config.pqmf_params\n",
        "        )\n",
        "        if True in h.sbd_transpose:\n",
        "            self.f_pqmf = PQMF(\n",
        "                *self.config.f_pqmf_params\n",
        "            )\n",
        "        else:\n",
        "            self.f_pqmf = None\n",
        "\n",
        "        self.discriminators = torch.nn.ModuleList()\n",
        "\n",
        "        for _f, _k, _d, _s, _br, _tr in zip(\n",
        "            self.config.filters,\n",
        "            self.config.kernel_sizes,\n",
        "            self.config.dilations,\n",
        "            self.config.strides,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if _tr:\n",
        "                segment_dim = self.config.segment_size // _br[1] - _br[0]\n",
        "            else:\n",
        "                segment_dim = _br[1] - _br[0]\n",
        "\n",
        "            self.discriminators.append(SBDBlock(\n",
        "                segment_dim=segment_dim,\n",
        "                filters=_f,\n",
        "                kernel_size=_k,\n",
        "                dilations=_d,\n",
        "                strides=_s,\n",
        "                use_spectral_norm=use_spectral_norm\n",
        "            ))\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        y_in = self.pqmf.analysis(y)\n",
        "        y_hat_in = self.pqmf.analysis(y_hat)\n",
        "        if self.f_pqmf is not None:\n",
        "            y_in_f = self.f_pqmf.analysis(y)\n",
        "            y_hat_in_f = self.f_pqmf.analysis(y_hat)\n",
        "\n",
        "        for d, br, tr in zip(\n",
        "            self.discriminators,\n",
        "            self.config.band_ranges,\n",
        "            self.config.transpose\n",
        "        ):\n",
        "            if tr:\n",
        "                _y_in = y_in_f[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in_f[:, br[0]:br[1], :]\n",
        "                _y_in = torch.transpose(_y_in, 1, 2)\n",
        "                _y_hat_in = torch.transpose(_y_hat_in, 1, 2)\n",
        "            else:\n",
        "                _y_in = y_in[:, br[0]:br[1], :]\n",
        "                _y_hat_in = y_hat_in[:, br[0]:br[1], :]\n",
        "            y_d_r, fmap_r = d(_y_in)\n",
        "            y_d_g, fmap_g = d(_y_hat_in)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttjqPrPh4sC"
      },
      "source": [
        "#generator.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HHcgmHJEo90g"
      },
      "outputs": [],
      "source": [
        "class ResBlock(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, 0.2)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, 0.2)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for _l in self.convs1:\n",
        "            remove_weight_norm(_l)\n",
        "        for _l in self.convs2:\n",
        "            remove_weight_norm(_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TUBqwfZgh3dx"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super(Generator, self).__init__()\n",
        "        self.h = h\n",
        "        self.resblock = h.resblock\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            _ups = nn.ModuleList()\n",
        "            for _i, (_u, _k) in enumerate(zip(u, k)):\n",
        "                in_channel = h.upsample_initial_channel // (2**i)\n",
        "                out_channel = h.upsample_initial_channel // (2**(i + 1))\n",
        "                _ups.append(weight_norm(\n",
        "                    ConvTranspose1d(in_channel, out_channel, _k, _u, padding=(_k - _u) // 2)))\n",
        "            self.ups.append(_ups)\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        self.conv_post = nn.ModuleList()\n",
        "        for i in range(self.num_upsamples):\n",
        "            ch = h.upsample_initial_channel // (2**(i + 1))\n",
        "            temp = nn.ModuleList()\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                temp.append(resblock(h, ch, k, d))\n",
        "            self.resblocks.append(temp)\n",
        "\n",
        "            if self.h.projection_filters[i] != 0:\n",
        "                self.conv_post.append(\n",
        "                    weight_norm(\n",
        "                        Conv1d(\n",
        "                            ch, self.h.projection_filters[i],\n",
        "                            self.h.projection_kernels[i], 1, padding=self.h.projection_kernels[i] // 2\n",
        "                        )))\n",
        "            else:\n",
        "                self.conv_post.append(torch.nn.Identity())\n",
        "\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        x = self.conv_pre(x)\n",
        "        for i, (ups, resblocks, conv_post) in enumerate(zip(self.ups, self.resblocks, self.conv_post)):\n",
        "            x = F.leaky_relu(x, 0.2)\n",
        "            for _ups in ups:\n",
        "                x = _ups(x)\n",
        "            xs = None\n",
        "            for j, resblock in enumerate(resblocks):\n",
        "                if xs is None:\n",
        "                    xs = resblock(x)\n",
        "                else:\n",
        "                    xs += resblock(x)\n",
        "            x = xs / self.num_kernels\n",
        "            if i >= (self.num_upsamples-3):\n",
        "                _x = F.leaky_relu(x)\n",
        "                _x = conv_post(_x)\n",
        "                _x = torch.tanh(_x)\n",
        "                outs.append(_x)\n",
        "            else:\n",
        "                x = conv_post(x)\n",
        "\n",
        "        return outs\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for ups in self.ups:\n",
        "            for _l in ups:\n",
        "                remove_weight_norm(_l)\n",
        "        for resblock in self.resblocks:\n",
        "            for _l in resblock:\n",
        "                _l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        for _l in self.conv_post:\n",
        "            if not isinstance(_l, torch.nn.Identity):\n",
        "                remove_weight_norm(_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUyxi8fxiHbD"
      },
      "source": [
        "#data_module.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2RVIsixRpGYI"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AvocodoDataConfig:\n",
        "    segment_size: int\n",
        "    num_mels: int\n",
        "    num_freq: int\n",
        "    sampling_rate: int\n",
        "    n_fft: int\n",
        "    hop_size: int\n",
        "    win_size: int\n",
        "    fmin: int\n",
        "    fmax: int\n",
        "    batch_size: int\n",
        "    num_workers: int\n",
        "\n",
        "    fine_tuning: bool\n",
        "    base_mels_path: str\n",
        "\n",
        "    input_wavs_dir: str\n",
        "    input_mels_dir: str\n",
        "    input_training_file: str\n",
        "    input_validation_file: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "R3ikwJZniJKo"
      },
      "outputs": [],
      "source": [
        "#원본\n",
        "# class AvocodoData(LightningDataModule):\n",
        "#     def __init__(self, h: AvocodoDataConfig):\n",
        "#         super().__init__()\n",
        "#         self.save_hyperparameters(h)\n",
        "\n",
        "#     def prepare_data(self):\n",
        "#         '''\n",
        "#             Download and prepare data\n",
        "#         '''\n",
        "#         self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "#             self.hparams.input_wavs_dir,\n",
        "#             self.hparams.input_training_file,\n",
        "#             self.hparams.input_validation_file\n",
        "#         )\n",
        "\n",
        "#     def setup(self, stage=None):\n",
        "#         '''\n",
        "#             Create datasets for training and validation\n",
        "#         '''\n",
        "#         self.trainset = MelDataset(\n",
        "#             self.training_filelist,\n",
        "#             self.hparams.segment_size,\n",
        "#             self.hparams.n_fft,\n",
        "#             self.hparams.num_mels,\n",
        "#             self.hparams.hop_size,\n",
        "#             self.hparams.win_size,\n",
        "#             self.hparams.sampling_rate,\n",
        "#             self.hparams.fmin,\n",
        "#             self.hparams.fmax,\n",
        "#             n_cache_reuse=0,\n",
        "#             fmax_loss=self.hparams.fmax_for_loss,\n",
        "#             fine_tuning=self.hparams.fine_tuning,\n",
        "#             base_mels_path=self.hparams.input_mels_dir\n",
        "#         )\n",
        "#         self.validset = MelDataset(\n",
        "#             self.validation_filelist,\n",
        "#             self.hparams.segment_size,\n",
        "#             self.hparams.n_fft,\n",
        "#             self.hparams.num_mels,\n",
        "#             self.hparams.hop_size,\n",
        "#             self.hparams.win_size,\n",
        "#             self.hparams.sampling_rate,\n",
        "#             self.hparams.fmin,\n",
        "#             self.hparams.fmax,\n",
        "#             n_cache_reuse=0,\n",
        "#             fmax_loss=self.hparams.fmax_for_loss,\n",
        "#             fine_tuning=self.hparams.fine_tuning,\n",
        "#             base_mels_path=self.hparams.input_mels_dir\n",
        "#         )\n",
        "\n",
        "#     def train_dataloader(self):\n",
        "#         max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "#         return DataLoader(\n",
        "#             self.trainset,\n",
        "#             num_workers=max_workers,\n",
        "#             shuffle=True,\n",
        "#             batch_size=self.hparams.batch_size,\n",
        "#             pin_memory=True,\n",
        "#             drop_last=True\n",
        "#         )\n",
        "\n",
        "#     #@rank_zero_only\n",
        "#     def val_dataloader(self):\n",
        "#         max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "#         return DataLoader(\n",
        "#             self.validset,\n",
        "#             num_workers=max_workers,\n",
        "#             shuffle=False,\n",
        "#             batch_size=1,\n",
        "#             pin_memory=True,\n",
        "#             drop_last=True\n",
        "#         )\n",
        "\n",
        "#     def full_dataloader(self):\n",
        "#         '''\n",
        "#             Combine training and validation datasets for inference\n",
        "#         '''\n",
        "#         full_dataset = torch.utils.data.ConcatDataset([self.trainset, self.validset])\n",
        "#         max_workers = os.cpu_count()  # 모든 CPU 코어 사용\n",
        "#         return DataLoader(\n",
        "#             full_dataset,\n",
        "#             num_workers=max_workers,\n",
        "#             shuffle=False,\n",
        "#             batch_size=1,  # Inference에서는 1개씩 처리\n",
        "#             pin_memory=True,\n",
        "#             drop_last=False\n",
        "#         )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RawAudioDataset(Dataset):\n",
        "    def __init__(self, filelist, segment_size, sampling_rate):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            filelist: List of paths to audio files.\n",
        "            segment_size: Size of audio segments to extract in samples.\n",
        "            sampling_rate: Target sampling rate.\n",
        "        \"\"\"\n",
        "        self.filelist = filelist\n",
        "        self.segment_size = segment_size\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filelist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Load a wav file, optionally resample and segment it.\n",
        "        \"\"\"\n",
        "        filepath = self.filelist[index]\n",
        "        audio, sr = torchaudio.load(filepath)  # Load audio\n",
        "        if sr != self.sampling_rate:\n",
        "            resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sampling_rate)\n",
        "            audio = resample(audio)\n",
        "\n",
        "        # Ensure audio is long enough for a segment\n",
        "        if audio.size(1) < self.segment_size:\n",
        "            pad_size = self.segment_size - audio.size(1)\n",
        "            audio = F.pad(audio, (0, pad_size), \"constant\", 0)\n",
        "        else:\n",
        "            # Randomly select a segment\n",
        "            start = torch.randint(0, audio.size(1) - self.segment_size + 1, (1,))\n",
        "            audio = audio[:, start:start + self.segment_size]\n",
        "\n",
        "        return audio\n"
      ],
      "metadata": {
        "id": "uLs_SPwW3DuK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AvocodoData(LightningDataModule):\n",
        "    def __init__(self, h: AvocodoDataConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"\n",
        "        Prepare data by loading file lists for training and validation.\n",
        "        \"\"\"\n",
        "        self.training_filelist, self.validation_filelist = get_dataset_filelist(\n",
        "            self.hparams.input_wavs_dir,\n",
        "            self.hparams.input_training_file,\n",
        "            self.hparams.input_validation_file\n",
        "        )\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"\n",
        "        Create datasets for training and validation using MelDataset.\n",
        "        Ensure that both datasets return data in the required format.\n",
        "        \"\"\"\n",
        "        self.trainset = MelDataset(\n",
        "            self.training_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "        self.validset = MelDataset(\n",
        "            self.validation_filelist,\n",
        "            self.hparams.segment_size,\n",
        "            self.hparams.n_fft,\n",
        "            self.hparams.num_mels,\n",
        "            self.hparams.hop_size,\n",
        "            self.hparams.win_size,\n",
        "            self.hparams.sampling_rate,\n",
        "            self.hparams.fmin,\n",
        "            self.hparams.fmax,\n",
        "            n_cache_reuse=0,\n",
        "            fmax_loss=self.hparams.fmax_for_loss,\n",
        "            fine_tuning=self.hparams.fine_tuning,\n",
        "            base_mels_path=self.hparams.input_mels_dir\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"\n",
        "        DataLoader for the training dataset.\n",
        "        \"\"\"\n",
        "        max_workers = os.cpu_count()  # Use all available CPU cores\n",
        "        return DataLoader(\n",
        "            self.trainset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=True,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"\n",
        "        DataLoader for the validation dataset.\n",
        "        \"\"\"\n",
        "        max_workers = os.cpu_count()  # Use all available CPU cores\n",
        "        return DataLoader(\n",
        "            self.validset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=1,  # Use batch size of 1 for validation\n",
        "            pin_memory=True,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "    def full_dataloader(self):\n",
        "        \"\"\"\n",
        "        Combine training and validation datasets for inference.\n",
        "        \"\"\"\n",
        "        full_dataset = torch.utils.data.ConcatDataset([self.trainset, self.validset])\n",
        "        max_workers = os.cpu_count()  # Use all available CPU cores\n",
        "        return DataLoader(\n",
        "            full_dataset,\n",
        "            num_workers=max_workers,\n",
        "            shuffle=False,\n",
        "            batch_size=1,  # Process one sample at a time during inference\n",
        "            pin_memory=True,\n",
        "            drop_last=False\n",
        "        )\n"
      ],
      "metadata": {
        "id": "7l2YVihyz8l2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#wavlm"
      ],
      "metadata": {
        "id": "cRVttldNdvFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransposeLast(nn.Module):\n",
        "    def __init__(self, deconstruct_idx=None):\n",
        "        super().__init__()\n",
        "        self.deconstruct_idx = deconstruct_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deconstruct_idx is not None:\n",
        "            x = x[self.deconstruct_idx]\n",
        "        return x.transpose(-2, -1)#마지막 차원 두개 바꿈\n",
        "\n",
        "\n",
        "class Fp32LayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.layer_norm(\n",
        "            input.float(),\n",
        "            self.normalized_shape,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input) #동일 형태\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None #동일 형태\n",
        "\n",
        "\n",
        "class SamePad(nn.Module): #패딩 추가 또는 빼기\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x): # [batch_size, channel, time]\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x #[batch_size, channel, time-padding]\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x) # 동일 차원\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module): #레이어 정규화\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x #동일 차원\n",
        "\n",
        "\n",
        "def gelu_accurate(x): #계산식\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor: #계산식\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size): #양자화 노이즈 추가\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training: #학습하는 경우\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else: #체크하는 경우\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim, #임베딩 차원\n",
        "            num_heads, #MULTIHEAD ATTENTION 헤드 수\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise( #노이즈 양자화\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): #파라미터 리셋\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional: #blstm\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length): #바이에스 계산\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None] #query_length-1까지\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :] #key_length-1까지\n",
        "        relative_position = memory_position - context_position #길이 차이 만큼의 상대적 위치 계산 5-3=2 식으로\n",
        "        relative_position_bucket = self._relative_positions_bucket( #범주화\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device) #상대적 위치 편향값\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1]) # [num_heads, query_length, key_length]\n",
        "        return values\n",
        "\n",
        "    def forward( #[time, batch, channel]\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights: #가중치 필요한 경우 확인\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size() #입력 데이터 분리\n",
        "        src_len = tgt_len #key 길이 설정\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None: #상대적 위치 편향 계산\n",
        "            position_bias = self.compute_bias(tgt_len, src_len) #원본과 타겟의 상대적 위치차이 계산\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "            #batch*heads, target_length, source_length\n",
        "\n",
        "        if (\n",
        "                not is_tpu  # don't use PyTorch version on TPUs\n",
        "                and incremental_state is None\n",
        "                and not static_kv\n",
        "                # A workaround for quantization to work. Otherwise JIT compilation\n",
        "                # treats bias in linear module as method.\n",
        "                and not torch.jit.is_scripting()\n",
        "                and self.q_head_dim == self.head_dim\n",
        "        ):\n",
        "            assert key is not None and value is not None\n",
        "            assert attn_mask is None\n",
        "\n",
        "            attn_mask_rel_pos = None\n",
        "            if position_bias is not None:\n",
        "                attn_mask_rel_pos = position_bias\n",
        "                if self.gru_rel_pos:\n",
        "                    query_layer = query.transpose(0, 1)\n",
        "                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n",
        "                    query_layer = query_layer.view(*new_x_shape)\n",
        "                    query_layer = query_layer.permute(0, 2, 1, 3)\n",
        "                    _B, _H, _L, __ = query_layer.size()\n",
        "\n",
        "                    gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                        _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n",
        "            k_proj_bias = self.k_proj.bias\n",
        "            if k_proj_bias is None:\n",
        "                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n",
        "\n",
        "            x, attn = F.multi_head_attention_forward( #멀티헤드 어텐션 계산\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                torch.empty([0]),\n",
        "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout_module.p,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                self.training,\n",
        "                # self.training or self.dropout_module.apply_during_inference,\n",
        "                key_padding_mask,\n",
        "                need_weights,\n",
        "                attn_mask_rel_pos,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj.weight,\n",
        "                k_proj_weight=self.k_proj.weight,\n",
        "                v_proj_weight=self.v_proj.weight,\n",
        "            )\n",
        "            return x, attn, position_bias\n",
        "\n",
        "        if incremental_state is not None: #증분상태 결합\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention: #프로젝션 계산, q,k,v 생성\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None: #저장 상태\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0: #패딩 추가의 경우\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "            position_bias = position_bias.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + position_bias\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights"
      ],
      "metadata": {
        "id": "cRdo0ajodwvi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --------------------------------------------------------\n",
        "# # WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# # Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# # Copyright (c) 2021 Microsoft\n",
        "# # Licensed under The MIT License [see LICENSE for details]\n",
        "# # Based on fairseq code bases\n",
        "# # https://github.com/pytorch/fairseq\n",
        "# # --------------------------------------------------------\n",
        "\n",
        "# import math\n",
        "# import logging\n",
        "# from typing import List, Optional, Tuple\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.nn import LayerNorm\n",
        "# # from modules import (\n",
        "# #     Fp32GroupNorm,\n",
        "# #     Fp32LayerNorm,\n",
        "# #     GradMultiply,\n",
        "# #     MultiheadAttention,\n",
        "# #     SamePad,\n",
        "# #     init_bert_params,\n",
        "# #     get_activation_fn,\n",
        "# #     TransposeLast,\n",
        "# #     GLU_Linear,\n",
        "# # )\n",
        "\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# def compute_mask_indices(\n",
        "#     shape: Tuple[int, int],\n",
        "#     padding_mask: Optional[torch.Tensor],\n",
        "#     mask_prob: float,\n",
        "#     mask_length: int,\n",
        "#     mask_type: str = \"static\",\n",
        "#     mask_other: float = 0.0,\n",
        "#     min_masks: int = 0,\n",
        "#     no_overlap: bool = False,\n",
        "#     min_space: int = 0,\n",
        "# ) -> np.ndarray:\n",
        "#     \"\"\"\n",
        "#     Computes random mask spans for a given shape\n",
        "\n",
        "#     Args:\n",
        "#         shape: the the shape for which to compute masks.\n",
        "#             should be of size 2 where first element is batch size and 2nd is timesteps\n",
        "#         padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements\n",
        "#         mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n",
        "#             number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n",
        "#             however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n",
        "#         mask_type: how to compute mask lengths\n",
        "#             static = fixed size\n",
        "#             uniform = sample from uniform distribution [mask_other, mask_length*2]\n",
        "#             normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element\n",
        "#             poisson = sample from possion distribution with lambda = mask length\n",
        "#         min_masks: minimum number of masked spans\n",
        "#         no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping\n",
        "#         min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans\n",
        "#     \"\"\"\n",
        "\n",
        "#     bsz, all_sz = shape\n",
        "#     mask = np.full((bsz, all_sz), False)\n",
        "\n",
        "#     all_num_mask = int(\n",
        "#         # add a random number for probabilistic rounding\n",
        "#         mask_prob * all_sz / float(mask_length)\n",
        "#         + np.random.rand()\n",
        "#     )\n",
        "\n",
        "#     all_num_mask = max(min_masks, all_num_mask)\n",
        "\n",
        "#     mask_idcs = []\n",
        "#     for i in range(bsz):\n",
        "#         if padding_mask is not None:\n",
        "#             sz = all_sz - padding_mask[i].long().sum().item()\n",
        "#             num_mask = int(\n",
        "#                 # add a random number for probabilistic rounding\n",
        "#                 mask_prob * sz / float(mask_length)\n",
        "#                 + np.random.rand()\n",
        "#             )\n",
        "#             num_mask = max(min_masks, num_mask)\n",
        "#         else:\n",
        "#             sz = all_sz\n",
        "#             num_mask = all_num_mask\n",
        "\n",
        "#         if mask_type == \"static\":\n",
        "#             lengths = np.full(num_mask, mask_length)\n",
        "#         elif mask_type == \"uniform\":\n",
        "#             lengths = np.random.randint(mask_other, mask_length * 2 + 1, size=num_mask)\n",
        "#         elif mask_type == \"normal\":\n",
        "#             lengths = np.random.normal(mask_length, mask_other, size=num_mask)\n",
        "#             lengths = [max(1, int(round(x))) for x in lengths]\n",
        "#         elif mask_type == \"poisson\":\n",
        "#             lengths = np.random.poisson(mask_length, size=num_mask)\n",
        "#             lengths = [int(round(x)) for x in lengths]\n",
        "#         else:\n",
        "#             raise Exception(\"unknown mask selection \" + mask_type)\n",
        "\n",
        "#         if sum(lengths) == 0:\n",
        "#             lengths[0] = min(mask_length, sz - 1)\n",
        "\n",
        "#         if no_overlap:\n",
        "#             mask_idc = []\n",
        "\n",
        "#             def arrange(s, e, length, keep_length):\n",
        "#                 span_start = np.random.randint(s, e - length)\n",
        "#                 mask_idc.extend(span_start + i for i in range(length))\n",
        "\n",
        "#                 new_parts = []\n",
        "#                 if span_start - s - min_space >= keep_length:\n",
        "#                     new_parts.append((s, span_start - min_space + 1))\n",
        "#                 if e - span_start - keep_length - min_space > keep_length:\n",
        "#                     new_parts.append((span_start + length + min_space, e))\n",
        "#                 return new_parts\n",
        "\n",
        "#             parts = [(0, sz)]\n",
        "#             min_length = min(lengths)\n",
        "#             for length in sorted(lengths, reverse=True):\n",
        "#                 lens = np.fromiter(\n",
        "#                     (e - s if e - s >= length + min_space else 0 for s, e in parts),\n",
        "#                     np.int,\n",
        "#                 )\n",
        "#                 l_sum = np.sum(lens)\n",
        "#                 if l_sum == 0:\n",
        "#                     break\n",
        "#                 probs = lens / np.sum(lens)\n",
        "#                 c = np.random.choice(len(parts), p=probs)\n",
        "#                 s, e = parts.pop(c)\n",
        "#                 parts.extend(arrange(s, e, length, min_length))\n",
        "#             mask_idc = np.asarray(mask_idc)\n",
        "#         else:\n",
        "#             min_len = min(lengths)\n",
        "#             if sz - min_len <= num_mask:\n",
        "#                 min_len = sz - num_mask - 1\n",
        "\n",
        "#             mask_idc = np.random.choice(sz - min_len, num_mask, replace=False)\n",
        "\n",
        "#             mask_idc = np.asarray(\n",
        "#                 [\n",
        "#                     mask_idc[j] + offset\n",
        "#                     for j in range(len(mask_idc))\n",
        "#                     for offset in range(lengths[j])\n",
        "#                 ]\n",
        "#             )\n",
        "\n",
        "#         mask_idcs.append(np.unique(mask_idc[mask_idc < sz]))\n",
        "\n",
        "#     min_len = min([len(m) for m in mask_idcs])\n",
        "#     for i, mask_idc in enumerate(mask_idcs):\n",
        "#         if len(mask_idc) > min_len:\n",
        "#             mask_idc = np.random.choice(mask_idc, min_len, replace=False)\n",
        "#         mask[i, mask_idc] = True\n",
        "\n",
        "#     return mask\n",
        "\n",
        "\n",
        "# class WavLMConfig:\n",
        "#     def __init__(self, cfg=None):\n",
        "#         self.extractor_mode: str = \"default\"     # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n",
        "#         self.encoder_layers: int = 12     # num encoder layers in the transformer\n",
        "\n",
        "#         self.encoder_embed_dim: int = 768     # encoder embedding dimension\n",
        "#         self.encoder_ffn_embed_dim: int = 3072     # encoder embedding dimension for FFN\n",
        "#         self.encoder_attention_heads: int = 12     # num encoder attention heads\n",
        "#         self.activation_fn: str = \"gelu\"     # activation function to use\n",
        "\n",
        "#         self.layer_norm_first: bool = False     # apply layernorm first in the transformer\n",
        "#         self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"     # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]\n",
        "#         self.conv_bias: bool = False     # include bias in conv encoder\n",
        "#         self.feature_grad_mult: float = 1.0     # multiply feature extractor var grads by this\n",
        "\n",
        "#         self.normalize: bool = False  # normalize input to have 0 mean and unit variance during training\n",
        "\n",
        "#         # dropouts\n",
        "#         self.dropout: float = 0.1     # dropout probability for the transformer\n",
        "#         self.attention_dropout: float = 0.1     # dropout probability for attention weights\n",
        "#         self.activation_dropout: float = 0.0     # dropout probability after activation in FFN\n",
        "#         self.encoder_layerdrop: float = 0.0     # probability of dropping a tarnsformer layer\n",
        "#         self.dropout_input: float = 0.0     # dropout to apply to the input (after feat extr)\n",
        "#         self.dropout_features: float = 0.0     # dropout to apply to the features (after feat extr)\n",
        "\n",
        "#         # masking\n",
        "#         self.mask_length: int = 10     # mask length\n",
        "#         self.mask_prob: float = 0.65     # probability of replacing a token with mask\n",
        "#         self.mask_selection: str = \"static\"     # how to choose mask length\n",
        "#         self.mask_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indicesh\n",
        "#         self.no_mask_overlap: bool = False     # whether to allow masks to overlap\n",
        "#         self.mask_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "#         # channel masking\n",
        "#         self.mask_channel_length: int = 10     # length of the mask for features (channels)\n",
        "#         self.mask_channel_prob: float = 0.0     # probability of replacing a feature with 0\n",
        "#         self.mask_channel_selection: str = \"static\"     # how to choose mask length for channel masking\n",
        "#         self.mask_channel_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indices\n",
        "#         self.no_mask_channel_overlap: bool = False     # whether to allow channel masks to overlap\n",
        "#         self.mask_channel_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "#         # positional embeddings\n",
        "#         self.conv_pos: int = 128     # number of filters for convolutional positional embeddings\n",
        "#         self.conv_pos_groups: int = 16     # number of groups for convolutional positional embedding\n",
        "\n",
        "#         # relative position embedding\n",
        "#         self.relative_position_embedding: bool = False     # apply relative position embedding\n",
        "#         self.num_buckets: int = 320     # number of buckets for relative position embedding\n",
        "#         self.max_distance: int = 1280     # maximum distance for relative position embedding\n",
        "#         self.gru_rel_pos: bool = False     # apply gated relative position embedding\n",
        "\n",
        "#         if cfg is not None:\n",
        "#             self.update(cfg)\n",
        "\n",
        "#     def update(self, cfg: dict):\n",
        "#         self.__dict__.update(cfg)\n",
        "\n",
        "\n",
        "# class WavLM(nn.Module):\n",
        "#     def __init__( #[batch_size, time]\n",
        "#         self,\n",
        "#         cfg: WavLMConfig,\n",
        "#     ) -> None:\n",
        "#         super().__init__()\n",
        "#         logger.info(f\"WavLM Config: {cfg.__dict__}\")\n",
        "\n",
        "#         self.cfg = cfg\n",
        "#         feature_enc_layers = eval(cfg.conv_feature_layers)\n",
        "#         self.embed = feature_enc_layers[-1][0]\n",
        "\n",
        "#         self.feature_extractor = ConvFeatureExtractionModel(\n",
        "#             conv_layers=feature_enc_layers,\n",
        "#             dropout=0.0,\n",
        "#             mode=cfg.extractor_mode,\n",
        "#             conv_bias=cfg.conv_bias,\n",
        "#         )\n",
        "\n",
        "#         self.post_extract_proj = (\n",
        "#             nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
        "#             if self.embed != cfg.encoder_embed_dim\n",
        "#             else None\n",
        "#         )\n",
        "\n",
        "#         self.mask_prob = cfg.mask_prob\n",
        "#         self.mask_selection = cfg.mask_selection\n",
        "#         self.mask_other = cfg.mask_other\n",
        "#         self.mask_length = cfg.mask_length\n",
        "#         self.no_mask_overlap = cfg.no_mask_overlap\n",
        "#         self.mask_min_space = cfg.mask_min_space\n",
        "\n",
        "#         self.mask_channel_prob = cfg.mask_channel_prob\n",
        "#         self.mask_channel_selection = cfg.mask_channel_selection\n",
        "#         self.mask_channel_other = cfg.mask_channel_other\n",
        "#         self.mask_channel_length = cfg.mask_channel_length\n",
        "#         self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
        "#         self.mask_channel_min_space = cfg.mask_channel_min_space\n",
        "\n",
        "#         self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
        "#         self.dropout_features = nn.Dropout(cfg.dropout_features)\n",
        "\n",
        "#         self.feature_grad_mult = cfg.feature_grad_mult\n",
        "\n",
        "#         self.mask_emb = nn.Parameter(\n",
        "#             torch.FloatTensor(cfg.encoder_embed_dim).uniform_()\n",
        "#         )\n",
        "\n",
        "#         self.encoder = TransformerEncoder(cfg)\n",
        "#         self.layer_norm = LayerNorm(self.embed)\n",
        "\n",
        "#     def apply_mask(self, x, padding_mask): #[batch_size, time, channel=feature]\n",
        "#         B, T, C = x.shape\n",
        "#         if self.mask_prob > 0:\n",
        "#             mask_indices = compute_mask_indices( #특정 시간 단계에 마스크 적용\n",
        "#                 (B, T),# [batch_size, time]\n",
        "#                 padding_mask,\n",
        "#                 self.mask_prob,\n",
        "#                 self.mask_length,\n",
        "#                 self.mask_selection,\n",
        "#                 self.mask_other,\n",
        "#                 min_masks=2,\n",
        "#                 no_overlap=self.no_mask_overlap,\n",
        "#                 min_space=self.mask_min_space,\n",
        "#             )\n",
        "#             mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
        "#             x[mask_indices] = self.mask_emb\n",
        "#         else:\n",
        "#             mask_indices = None\n",
        "\n",
        "#         if self.mask_channel_prob > 0:\n",
        "#             mask_channel_indices = compute_mask_indices(\n",
        "#                 (B, C),\n",
        "#                 None,\n",
        "#                 self.mask_channel_prob,\n",
        "#                 self.mask_channel_length,\n",
        "#                 self.mask_channel_selection,\n",
        "#                 self.mask_channel_other,\n",
        "#                 no_overlap=self.no_mask_channel_overlap,\n",
        "#                 min_space=self.mask_channel_min_space,\n",
        "#             )\n",
        "#             mask_channel_indices = (\n",
        "#                 torch.from_numpy(mask_channel_indices)\n",
        "#                 .to(x.device)\n",
        "#                 .unsqueeze(1)\n",
        "#                 .expand(-1, T, -1)\n",
        "#             )\n",
        "#             x[mask_channel_indices] = 0\n",
        "\n",
        "#         return x, mask_indices # [batch_size, time, channel=feature]\n",
        "\n",
        "#     def forward_padding_mask(\n",
        "#             self, features: torch.Tensor, padding_mask: torch.Tensor,\n",
        "#     ) -> torch.Tensor:\n",
        "#         extra = padding_mask.size(1) % features.size(1)\n",
        "#         if extra > 0:\n",
        "#             padding_mask = padding_mask[:, :-extra]\n",
        "#         padding_mask = padding_mask.view(\n",
        "#             padding_mask.size(0), features.size(1), -1\n",
        "#         )\n",
        "#         padding_mask = padding_mask.all(-1)\n",
        "#         return padding_mask\n",
        "\n",
        "#     def extract_features( #[batch_size, time, feature]\n",
        "#         self,\n",
        "#         source: torch.Tensor, #[batch_size, time]\n",
        "#         padding_mask: Optional[torch.Tensor] = None, #[batch_size, time]\n",
        "#         mask: bool = False,\n",
        "#         ret_conv: bool = False,\n",
        "#         output_layer: Optional[int] = None,\n",
        "#         ret_layer_results: bool =True,\n",
        "#     ):\n",
        "\n",
        "#         if self.feature_grad_mult > 0:\n",
        "#             features = self.feature_extractor(source)# [batch_size,time] -> [batch_size, time, feature]\n",
        "#             if self.feature_grad_mult != 1.0:\n",
        "#                 features = GradMultiply.apply(features, self.feature_grad_mult) #[batch_size, time, feature]\n",
        "#         else:\n",
        "#             with torch.no_grad():\n",
        "#                 features = self.feature_extractor(source)\n",
        "\n",
        "#         features = features.transpose(1, 2) #[batch_size, feature, time]\n",
        "#         features = self.layer_norm(features) #[batch_size, feature, time]\n",
        "\n",
        "#         if padding_mask is not None:\n",
        "#             padding_mask = self.forward_padding_mask(features, padding_mask) #[batch_size, time]\n",
        "\n",
        "#         if self.post_extract_proj is not None:\n",
        "#             features = self.post_extract_proj(features) #[batch_size, feature, time]\n",
        "\n",
        "#         features = self.dropout_input(features)\n",
        "\n",
        "#         if mask:\n",
        "#             x, mask_indices = self.apply_mask(\n",
        "#                 features, padding_mask\n",
        "#             )\n",
        "#         else:\n",
        "#             x = features\n",
        "\n",
        "#         # 중간단계 모양\n",
        "#         # feature: (B, T, D), float\n",
        "#         # target: (B, T), long\n",
        "#         # x: (B, T, D), float\n",
        "#         # padding_mask: (B, T), bool\n",
        "#         # mask_indices: (B, T), bool\n",
        "#         x, layer_results = self.encoder(\n",
        "#             x,\n",
        "#             padding_mask=padding_mask,\n",
        "#             layer=None if output_layer is None else output_layer - 1\n",
        "#         ) # [batch_size, channel, time]\n",
        "\n",
        "#         res = {\"x\": x, \"padding_mask\": padding_mask, \"features\": features, \"layer_results\": layer_results}\n",
        "\n",
        "#         feature = res[\"features\"] if ret_conv else res[\"x\"]\n",
        "#         if ret_layer_results:\n",
        "#             feature = (feature, res[\"layer_results\"])\n",
        "#         return feature, res[\"padding_mask\"]\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_pretrained(cls, cfg: WavLMConfig, pretrained_path: str):\n",
        "#         \"\"\"\n",
        "#         Load a pretrained WavLM model.\n",
        "#         Args:\n",
        "#             cfg (WavLMConfig): Configuration for WavLM.\n",
        "#             pretrained_path (str): Path to the pretrained model weights.\n",
        "#         Returns:\n",
        "#             WavLM: An instance of WavLM initialized with pretrained weights.\n",
        "#         \"\"\"\n",
        "#         model = cls(cfg)\n",
        "#         state_dict = torch.load(pretrained_path, map_location=\"cpu\")\n",
        "#         model.load_state_dict(state_dict)\n",
        "#         logger.info(f\"Loaded pretrained weights from {pretrained_path}\")\n",
        "#         return model\n",
        "\n",
        "\n",
        "# class ConvFeatureExtractionModel(nn.Module):\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             conv_layers: List[Tuple[int, int, int]],\n",
        "#             dropout: float = 0.0,\n",
        "#             mode: str = \"default\",\n",
        "#             conv_bias: bool = False,\n",
        "#             conv_type: str = \"default\"\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "\n",
        "#         assert mode in {\"default\", \"layer_norm\"}\n",
        "\n",
        "#         def block(\n",
        "#                 n_in,\n",
        "#                 n_out,\n",
        "#                 k,\n",
        "#                 stride,\n",
        "#                 is_layer_norm=False,\n",
        "#                 is_group_norm=False,\n",
        "#                 conv_bias=False,\n",
        "#         ):\n",
        "#             def make_conv():\n",
        "#                 conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias) #[batch_size, channel, time]\n",
        "#                 nn.init.kaiming_normal_(conv.weight)\n",
        "#                 return conv\n",
        "\n",
        "#             assert (\n",
        "#                            is_layer_norm and is_group_norm\n",
        "#                    ) == False, \"layer norm and group norm are exclusive\"\n",
        "\n",
        "#             if is_layer_norm:\n",
        "#                 return nn.Sequential(\n",
        "#                     make_conv(), #[batch_size, channel, time]->[batch_size,channel,time]\n",
        "#                     nn.Dropout(p=dropout),\n",
        "#                     nn.Sequential(\n",
        "#                         TransposeLast(),\n",
        "#                         Fp32LayerNorm(dim, elementwise_affine=True),\n",
        "#                         TransposeLast(),\n",
        "#                     ),\n",
        "#                     nn.GELU(),\n",
        "#                 )\n",
        "#             elif is_group_norm:\n",
        "#                 return nn.Sequential(\n",
        "#                     make_conv(),\n",
        "#                     nn.Dropout(p=dropout),\n",
        "#                     Fp32GroupNorm(dim, dim, affine=True),\n",
        "#                     nn.GELU(),\n",
        "#                 )\n",
        "#             else:\n",
        "#                 return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n",
        "\n",
        "#         self.conv_type = conv_type\n",
        "#         if self.conv_type == \"default\":\n",
        "#             in_d = 1\n",
        "#             self.conv_layers = nn.ModuleList() #모델 쌓기\n",
        "#             for i, cl in enumerate(conv_layers):\n",
        "#                 assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n",
        "#                 (dim, k, stride) = cl\n",
        "\n",
        "#                 self.conv_layers.append(\n",
        "#                     block(\n",
        "#                         in_d,\n",
        "#                         dim,\n",
        "#                         k,\n",
        "#                         stride,\n",
        "#                         is_layer_norm=mode == \"layer_norm\",\n",
        "#                         is_group_norm=mode == \"default\" and i == 0,\n",
        "#                         conv_bias=conv_bias,\n",
        "#                     )\n",
        "#                 )\n",
        "#                 in_d = dim\n",
        "#         elif self.conv_type == \"conv2d\":\n",
        "#             in_d = 1\n",
        "#             self.conv_layers = nn.ModuleList() #모델 쌓기\n",
        "#             for i, cl in enumerate(conv_layers):\n",
        "#                 assert len(cl) == 3\n",
        "#                 (dim, k, stride) = cl\n",
        "\n",
        "#                 self.conv_layers.append(\n",
        "#                     torch.nn.Conv2d(in_d, dim, k, stride)\n",
        "#                 )\n",
        "#                 self.conv_layers.append(torch.nn.ReLU())\n",
        "#                 in_d = dim\n",
        "#         elif self.conv_type == \"custom\":\n",
        "#             in_d = 1\n",
        "#             idim = 80\n",
        "#             self.conv_layers = nn.ModuleList()\n",
        "#             for i, cl in enumerate(conv_layers):\n",
        "#                 assert len(cl) == 3\n",
        "#                 (dim, k, stride) = cl\n",
        "#                 self.conv_layers.append(\n",
        "#                     torch.nn.Conv2d(in_d, dim, k, stride, padding=1)\n",
        "#                 )\n",
        "#                 self.conv_layers.append(\n",
        "#                     torch.nn.LayerNorm([dim, idim])\n",
        "#                 )\n",
        "#                 self.conv_layers.append(torch.nn.ReLU())\n",
        "#                 in_d = dim\n",
        "#                 if (i + 1) % 2 == 0:\n",
        "#                     self.conv_layers.append(\n",
        "#                         torch.nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "#                     )\n",
        "#                     idim = int(math.ceil(idim / 2))\n",
        "#         else:\n",
        "#             pass\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "\n",
        "#         # BxT -> BxCxT\n",
        "#         x = x.unsqueeze(1)\n",
        "#         if self.conv_type == \"custom\":\n",
        "#             for conv in self.conv_layers:\n",
        "#                 if isinstance(conv, nn.LayerNorm):\n",
        "#                     x = x.transpose(1, 2)\n",
        "#                     x = conv(x).transpose(1, 2)\n",
        "#                 else:\n",
        "#                     x = conv(x)\n",
        "#             x = x.transpose(2, 3).contiguous()\n",
        "#             x = x.view(x.size(0), -1, x.size(-1))\n",
        "#         else:\n",
        "#             for conv in self.conv_layers:\n",
        "#                 x = conv(x)\n",
        "#             if self.conv_type == \"conv2d\":\n",
        "#                 b, c, t, f = x.size()\n",
        "#                 x = x.transpose(2, 3).contiguous().view(b, c * f, t)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, args):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.dropout = args.dropout\n",
        "#         self.embedding_dim = args.encoder_embed_dim\n",
        "\n",
        "#         self.pos_conv = nn.Conv1d( #위치정보 학습\n",
        "#             self.embedding_dim,\n",
        "#             self.embedding_dim,\n",
        "#             kernel_size=args.conv_pos,\n",
        "#             padding=args.conv_pos // 2,\n",
        "#             groups=args.conv_pos_groups,\n",
        "#         )\n",
        "#         dropout = 0\n",
        "#         std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n",
        "#         nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
        "#         nn.init.constant_(self.pos_conv.bias, 0)\n",
        "\n",
        "#         self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
        "#         self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n",
        "\n",
        "#         if hasattr(args, \"relative_position_embedding\"):\n",
        "#             self.relative_position_embedding = args.relative_position_embedding\n",
        "#             self.num_buckets = args.num_buckets\n",
        "#             self.max_distance = args.max_distance\n",
        "#         else:\n",
        "#             self.relative_position_embedding = False\n",
        "#             self.num_buckets = 0\n",
        "#             self.max_distance = 0\n",
        "\n",
        "#         self.layers = nn.ModuleList(\n",
        "#             [\n",
        "#                 TransformerSentenceEncoderLayer(\n",
        "#                     embedding_dim=self.embedding_dim,\n",
        "#                     ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "#                     num_attention_heads=args.encoder_attention_heads,\n",
        "#                     dropout=self.dropout,\n",
        "#                     attention_dropout=args.attention_dropout,\n",
        "#                     activation_dropout=args.activation_dropout,\n",
        "#                     activation_fn=args.activation_fn,\n",
        "#                     layer_norm_first=args.layer_norm_first,\n",
        "#                     has_relative_attention_bias=(self.relative_position_embedding and i == 0),\n",
        "#                     num_buckets=self.num_buckets,\n",
        "#                     max_distance=self.max_distance,\n",
        "#                     gru_rel_pos=args.gru_rel_pos,\n",
        "#                 )\n",
        "#                 for i in range(args.encoder_layers)\n",
        "#             ]\n",
        "#         )\n",
        "\n",
        "#         self.layer_norm_first = args.layer_norm_first\n",
        "#         self.layer_norm = LayerNorm(self.embedding_dim)\n",
        "#         self.layerdrop = args.encoder_layerdrop\n",
        "\n",
        "#         self.apply(init_bert_params)\n",
        "\n",
        "#     def forward(self, x, padding_mask=None, streaming_mask=None, layer=None):\n",
        "#     # [batch_size, time, embedding_dim]\n",
        "#         x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)\n",
        "\n",
        "#         if self.layer_norm_first and layer is None:\n",
        "#             x = self.layer_norm(x)\n",
        "\n",
        "#         return x # [batch_size, time, embedding]\n",
        "\n",
        "#     def extract_features(self, x, padding_mask=None, streaming_mask=None, tgt_layer=None):\n",
        "#         # [batch_size, time, embedding_dim]\n",
        "#         if padding_mask is not None:\n",
        "#             x[padding_mask] = 0\n",
        "\n",
        "#         x_conv = self.pos_conv(x.transpose(1, 2)) #[batch_size, channel,time]\n",
        "#         x_conv = x_conv.transpose(1, 2)# [batch_size, channel,time] -> [batch_size, time, channel?]\n",
        "#         x = x + x_conv #특성값 더함\n",
        "\n",
        "#         if not self.layer_norm_first:\n",
        "#             x = self.layer_norm(x) #정규화\n",
        "\n",
        "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "#         # B x T x C -> T x B x C\n",
        "#         x = x.transpose(0, 1) #[batch_size, time, channel] -> [time, batch_size, channel]\n",
        "\n",
        "#         layer_results = []\n",
        "#         z = None\n",
        "#         if tgt_layer is not None:\n",
        "#             layer_results.append((x, z))\n",
        "#         r = None\n",
        "#         #print(\"데이터 추출 시작\")\n",
        "#         pos_bias = None\n",
        "#         for i, layer in enumerate(self.layers): #각 레이어별로 계산, 무작위로 스킵\n",
        "#             #print(i,\"번째 레이어 데이터 추가\")\n",
        "#             dropout_probability = np.random.random()\n",
        "#             if not self.training or (dropout_probability > self.layerdrop):\n",
        "#                 x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,\n",
        "#                                        self_attn_mask=streaming_mask, pos_bias=pos_bias)\n",
        "#             if tgt_layer is not None:\n",
        "#                 layer_results.append((x, z))\n",
        "#             if i == tgt_layer:\n",
        "#                 r = x\n",
        "#                 break\n",
        "\n",
        "#         if r is not None:\n",
        "#             x = r\n",
        "\n",
        "#         # T x B x C -> B x T x C\n",
        "#         x = x.transpose(0, 1) #[time, batch_size, channel] -> [batch_size, time, channel]\n",
        "#         #return x\n",
        "#         return x, layer_results #[batch_size, time, channel]\n",
        "\n",
        "\n",
        "# class TransformerSentenceEncoderLayer(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
        "#     models.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             embedding_dim: float = 768,\n",
        "#             ffn_embedding_dim: float = 3072,\n",
        "#             num_attention_heads: float = 8,\n",
        "#             dropout: float = 0.1,\n",
        "#             attention_dropout: float = 0.1,\n",
        "#             activation_dropout: float = 0.1,\n",
        "#             activation_fn: str = \"relu\",\n",
        "#             layer_norm_first: bool = False,\n",
        "#             has_relative_attention_bias: bool = False,\n",
        "#             num_buckets: int = 0,\n",
        "#             max_distance: int = 0,\n",
        "#             rescale_init: bool = False,\n",
        "#             gru_rel_pos: bool = False,\n",
        "#     ) -> None:\n",
        "\n",
        "#         super().__init__()\n",
        "#         # Initialize parameters\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.dropout = dropout\n",
        "#         self.activation_dropout = activation_dropout\n",
        "\n",
        "#         # Initialize blocks\n",
        "#         self.activation_name = activation_fn\n",
        "#         self.activation_fn = get_activation_fn(activation_fn)\n",
        "#         self.self_attn = MultiheadAttention(\n",
        "#             self.embedding_dim,\n",
        "#             num_attention_heads,\n",
        "#             dropout=attention_dropout,\n",
        "#             self_attention=True,\n",
        "#             has_relative_attention_bias=has_relative_attention_bias,\n",
        "#             num_buckets=num_buckets,\n",
        "#             max_distance=max_distance,\n",
        "#             rescale_init=rescale_init,\n",
        "#             gru_rel_pos=gru_rel_pos,\n",
        "#         )\n",
        "\n",
        "#         self.dropout1 = nn.Dropout(dropout)\n",
        "#         self.dropout2 = nn.Dropout(self.activation_dropout)\n",
        "#         self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "#         self.layer_norm_first = layer_norm_first\n",
        "\n",
        "#         # layer norm associated with the self attention layer\n",
        "#         self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "#         if self.activation_name == \"glu\":\n",
        "#             self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n",
        "#         else:\n",
        "#             self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "#         self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "#         # layer norm associated with the position wise feed-forward NN\n",
        "#         self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             x: torch.Tensor, # [seq_len, batch_size, embedding_dim]\n",
        "#             self_attn_mask: torch.Tensor = None,\n",
        "#             self_attn_padding_mask: torch.Tensor = None,\n",
        "#             need_weights: bool = False,\n",
        "#             pos_bias=None\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         LayerNorm is applied either before or after the self-attention/ffn\n",
        "#         modules similar to the original Transformer imlementation.\n",
        "#         \"\"\"\n",
        "#         residual = x #잔차 연결\n",
        "\n",
        "#         if self.layer_norm_first:\n",
        "#             x = self.self_attn_layer_norm(x) #[batch_size, time, embedding_dim]\n",
        "#             x, attn, pos_bias = self.self_attn(\n",
        "#                 query=x,\n",
        "#                 key=x,\n",
        "#                 value=x,\n",
        "#                 key_padding_mask=self_attn_padding_mask,\n",
        "#                 need_weights=False,\n",
        "#                 attn_mask=self_attn_mask,\n",
        "#                 position_bias=pos_bias\n",
        "#             )\n",
        "#             x = self.dropout1(x)\n",
        "#             x = residual + x\n",
        "\n",
        "#             residual = x\n",
        "#             x = self.final_layer_norm(x)\n",
        "#             if self.activation_name == \"glu\":\n",
        "#                 x = self.fc1(x)\n",
        "#             else:\n",
        "#                 x = self.activation_fn(self.fc1(x))\n",
        "#             x = self.dropout2(x)\n",
        "#             x = self.fc2(x)\n",
        "#             x = self.dropout3(x)\n",
        "#             x = residual + x\n",
        "#         else:\n",
        "#             x, attn, pos_bias = self.self_attn(\n",
        "#                 query=x,\n",
        "#                 key=x,\n",
        "#                 value=x,\n",
        "#                 key_padding_mask=self_attn_padding_mask,\n",
        "#                 need_weights=need_weights,\n",
        "#                 attn_mask=self_attn_mask,\n",
        "#                 position_bias=pos_bias\n",
        "#             )\n",
        "\n",
        "#             x = self.dropout1(x)\n",
        "#             x = residual + x\n",
        "\n",
        "#             x = self.self_attn_layer_norm(x)\n",
        "\n",
        "#             residual = x\n",
        "#             if self.activation_name == \"glu\":\n",
        "#                 x = self.fc1(x)\n",
        "#             else:\n",
        "#                 x = self.activation_fn(self.fc1(x))\n",
        "#             x = self.dropout2(x)\n",
        "#             x = self.fc2(x)\n",
        "#             x = self.dropout3(x)\n",
        "#             x = residual + x\n",
        "#             x = self.final_layer_norm(x)\n",
        "\n",
        "#         return x, attn, pos_bias #[seq_len, batch_size, embedding_dim]\n"
      ],
      "metadata": {
        "id": "DHXe6T70dx56"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import logging\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LayerNorm\n",
        "# from modules import (\n",
        "#     Fp32GroupNorm,\n",
        "#     Fp32LayerNorm,\n",
        "#     GradMultiply,\n",
        "#     MultiheadAttention,\n",
        "#     SamePad,\n",
        "#     init_bert_params,\n",
        "#     get_activation_fn,\n",
        "#     TransposeLast,\n",
        "#     GLU_Linear,\n",
        "# )\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def compute_mask_indices(\n",
        "    shape: Tuple[int, int],\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    mask_prob: float,\n",
        "    mask_length: int,\n",
        "    mask_type: str = \"static\",\n",
        "    mask_other: float = 0.0,\n",
        "    min_masks: int = 0,\n",
        "    no_overlap: bool = False,\n",
        "    min_space: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes random mask spans for a given shape\n",
        "\n",
        "    Args:\n",
        "        shape: the the shape for which to compute masks.\n",
        "            should be of size 2 where first element is batch size and 2nd is timesteps\n",
        "        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements\n",
        "        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n",
        "            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n",
        "            however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n",
        "        mask_type: how to compute mask lengths\n",
        "            static = fixed size\n",
        "            uniform = sample from uniform distribution [mask_other, mask_length*2]\n",
        "            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element\n",
        "            poisson = sample from possion distribution with lambda = mask length\n",
        "        min_masks: minimum number of masked spans\n",
        "        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping\n",
        "        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans\n",
        "    \"\"\"\n",
        "\n",
        "    bsz, all_sz = shape\n",
        "    mask = np.full((bsz, all_sz), False)\n",
        "\n",
        "    all_num_mask = int(\n",
        "        # add a random number for probabilistic rounding\n",
        "        mask_prob * all_sz / float(mask_length)\n",
        "        + np.random.rand()\n",
        "    )\n",
        "\n",
        "    all_num_mask = max(min_masks, all_num_mask)\n",
        "\n",
        "    mask_idcs = []\n",
        "    for i in range(bsz):\n",
        "        if padding_mask is not None:\n",
        "            sz = all_sz - padding_mask[i].long().sum().item()\n",
        "            num_mask = int(\n",
        "                # add a random number for probabilistic rounding\n",
        "                mask_prob * sz / float(mask_length)\n",
        "                + np.random.rand()\n",
        "            )\n",
        "            num_mask = max(min_masks, num_mask)\n",
        "        else:\n",
        "            sz = all_sz\n",
        "            num_mask = all_num_mask\n",
        "\n",
        "        if mask_type == \"static\":\n",
        "            lengths = np.full(num_mask, mask_length)\n",
        "        elif mask_type == \"uniform\":\n",
        "            lengths = np.random.randint(mask_other, mask_length * 2 + 1, size=num_mask)\n",
        "        elif mask_type == \"normal\":\n",
        "            lengths = np.random.normal(mask_length, mask_other, size=num_mask)\n",
        "            lengths = [max(1, int(round(x))) for x in lengths]\n",
        "        elif mask_type == \"poisson\":\n",
        "            lengths = np.random.poisson(mask_length, size=num_mask)\n",
        "            lengths = [int(round(x)) for x in lengths]\n",
        "        else:\n",
        "            raise Exception(\"unknown mask selection \" + mask_type)\n",
        "\n",
        "        if sum(lengths) == 0:\n",
        "            lengths[0] = min(mask_length, sz - 1)\n",
        "\n",
        "        if no_overlap:\n",
        "            mask_idc = []\n",
        "\n",
        "            def arrange(s, e, length, keep_length):\n",
        "                span_start = np.random.randint(s, e - length)\n",
        "                mask_idc.extend(span_start + i for i in range(length))\n",
        "\n",
        "                new_parts = []\n",
        "                if span_start - s - min_space >= keep_length:\n",
        "                    new_parts.append((s, span_start - min_space + 1))\n",
        "                if e - span_start - keep_length - min_space > keep_length:\n",
        "                    new_parts.append((span_start + length + min_space, e))\n",
        "                return new_parts\n",
        "\n",
        "            parts = [(0, sz)]\n",
        "            min_length = min(lengths)\n",
        "            for length in sorted(lengths, reverse=True):\n",
        "                lens = np.fromiter(\n",
        "                    (e - s if e - s >= length + min_space else 0 for s, e in parts),\n",
        "                    np.int,\n",
        "                )\n",
        "                l_sum = np.sum(lens)\n",
        "                if l_sum == 0:\n",
        "                    break\n",
        "                probs = lens / np.sum(lens)\n",
        "                c = np.random.choice(len(parts), p=probs)\n",
        "                s, e = parts.pop(c)\n",
        "                parts.extend(arrange(s, e, length, min_length))\n",
        "            mask_idc = np.asarray(mask_idc)\n",
        "        else:\n",
        "            min_len = min(lengths)\n",
        "            if sz - min_len <= num_mask:\n",
        "                min_len = sz - num_mask - 1\n",
        "\n",
        "            mask_idc = np.random.choice(sz - min_len, num_mask, replace=False)\n",
        "\n",
        "            mask_idc = np.asarray(\n",
        "                [\n",
        "                    mask_idc[j] + offset\n",
        "                    for j in range(len(mask_idc))\n",
        "                    for offset in range(lengths[j])\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        mask_idcs.append(np.unique(mask_idc[mask_idc < sz]))\n",
        "\n",
        "    min_len = min([len(m) for m in mask_idcs])\n",
        "    for i, mask_idc in enumerate(mask_idcs):\n",
        "        if len(mask_idc) > min_len:\n",
        "            mask_idc = np.random.choice(mask_idc, min_len, replace=False)\n",
        "        mask[i, mask_idc] = True\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "class WavLMConfig:\n",
        "    def __init__(self, cfg=None):\n",
        "        self.extractor_mode: str = \"default\"     # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n",
        "        self.encoder_layers: int = 12     # num encoder layers in the transformer\n",
        "\n",
        "        self.encoder_embed_dim: int = 768     # encoder embedding dimension\n",
        "        self.encoder_ffn_embed_dim: int = 3072     # encoder embedding dimension for FFN\n",
        "        self.encoder_attention_heads: int = 12     # num encoder attention heads\n",
        "        self.activation_fn: str = \"gelu\"     # activation function to use\n",
        "\n",
        "        self.layer_norm_first: bool = False     # apply layernorm first in the transformer\n",
        "        self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"     # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]\n",
        "        self.conv_bias: bool = False     # include bias in conv encoder\n",
        "        self.feature_grad_mult: float = 1.0     # multiply feature extractor var grads by this\n",
        "\n",
        "        self.normalize: bool = False  # normalize input to have 0 mean and unit variance during training\n",
        "\n",
        "        # dropouts\n",
        "        self.dropout: float = 0.1     # dropout probability for the transformer\n",
        "        self.attention_dropout: float = 0.1     # dropout probability for attention weights\n",
        "        self.activation_dropout: float = 0.0     # dropout probability after activation in FFN\n",
        "        self.encoder_layerdrop: float = 0.0     # probability of dropping a tarnsformer layer\n",
        "        self.dropout_input: float = 0.0     # dropout to apply to the input (after feat extr)\n",
        "        self.dropout_features: float = 0.0     # dropout to apply to the features (after feat extr)\n",
        "\n",
        "        # masking\n",
        "        self.mask_length: int = 10     # mask length\n",
        "        self.mask_prob: float = 0.65     # probability of replacing a token with mask\n",
        "        self.mask_selection: str = \"static\"     # how to choose mask length\n",
        "        self.mask_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indicesh\n",
        "        self.no_mask_overlap: bool = False     # whether to allow masks to overlap\n",
        "        self.mask_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # channel masking\n",
        "        self.mask_channel_length: int = 10     # length of the mask for features (channels)\n",
        "        self.mask_channel_prob: float = 0.0     # probability of replacing a feature with 0\n",
        "        self.mask_channel_selection: str = \"static\"     # how to choose mask length for channel masking\n",
        "        self.mask_channel_other: float = 0     # secondary mask argument (used for more complex distributions), see help in compute_mask_indices\n",
        "        self.no_mask_channel_overlap: bool = False     # whether to allow channel masks to overlap\n",
        "        self.mask_channel_min_space: int = 1     # min space between spans (if no overlap is enabled)\n",
        "\n",
        "        # positional embeddings\n",
        "        self.conv_pos: int = 128     # number of filters for convolutional positional embeddings\n",
        "        self.conv_pos_groups: int = 16     # number of groups for convolutional positional embedding\n",
        "\n",
        "        # relative position embedding\n",
        "        self.relative_position_embedding: bool = False     # apply relative position embedding\n",
        "        self.num_buckets: int = 320     # number of buckets for relative position embedding\n",
        "        self.max_distance: int = 1280     # maximum distance for relative position embedding\n",
        "        self.gru_rel_pos: bool = False     # apply gated relative position embedding\n",
        "\n",
        "        if cfg is not None:\n",
        "            self.update(cfg)\n",
        "\n",
        "    def update(self, cfg: dict):\n",
        "        self.__dict__.update(cfg)\n",
        "\n",
        "\n",
        "class WavLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg: WavLMConfig,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        logger.info(f\"WavLM Config: {cfg.__dict__}\")\n",
        "\n",
        "        self.cfg = cfg\n",
        "        feature_enc_layers = eval(cfg.conv_feature_layers)\n",
        "        self.embed = feature_enc_layers[-1][0]\n",
        "\n",
        "        self.feature_extractor = ConvFeatureExtractionModel(\n",
        "            conv_layers=feature_enc_layers,\n",
        "            dropout=0.0,\n",
        "            mode=cfg.extractor_mode,\n",
        "            conv_bias=cfg.conv_bias,\n",
        "        )\n",
        "\n",
        "        self.post_extract_proj = (\n",
        "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
        "            if self.embed != cfg.encoder_embed_dim\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.mask_prob = cfg.mask_prob\n",
        "        self.mask_selection = cfg.mask_selection\n",
        "        self.mask_other = cfg.mask_other\n",
        "        self.mask_length = cfg.mask_length\n",
        "        self.no_mask_overlap = cfg.no_mask_overlap\n",
        "        self.mask_min_space = cfg.mask_min_space\n",
        "\n",
        "        self.mask_channel_prob = cfg.mask_channel_prob\n",
        "        self.mask_channel_selection = cfg.mask_channel_selection\n",
        "        self.mask_channel_other = cfg.mask_channel_other\n",
        "        self.mask_channel_length = cfg.mask_channel_length\n",
        "        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
        "        self.mask_channel_min_space = cfg.mask_channel_min_space\n",
        "\n",
        "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
        "        self.dropout_features = nn.Dropout(cfg.dropout_features)\n",
        "\n",
        "        self.feature_grad_mult = cfg.feature_grad_mult\n",
        "\n",
        "        self.mask_emb = nn.Parameter(\n",
        "            torch.FloatTensor(cfg.encoder_embed_dim).uniform_()\n",
        "        )\n",
        "\n",
        "        self.encoder = TransformerEncoder(cfg)\n",
        "        self.layer_norm = LayerNorm(self.embed)\n",
        "\n",
        "    def apply_mask(self, x, padding_mask):\n",
        "        B, T, C = x.shape\n",
        "        if self.mask_prob > 0:\n",
        "            mask_indices = compute_mask_indices(\n",
        "                (B, T),\n",
        "                padding_mask,\n",
        "                self.mask_prob,\n",
        "                self.mask_length,\n",
        "                self.mask_selection,\n",
        "                self.mask_other,\n",
        "                min_masks=2,\n",
        "                no_overlap=self.no_mask_overlap,\n",
        "                min_space=self.mask_min_space,\n",
        "            )\n",
        "            mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
        "            x[mask_indices] = self.mask_emb\n",
        "        else:\n",
        "            mask_indices = None\n",
        "\n",
        "        if self.mask_channel_prob > 0:\n",
        "            mask_channel_indices = compute_mask_indices(\n",
        "                (B, C),\n",
        "                None,\n",
        "                self.mask_channel_prob,\n",
        "                self.mask_channel_length,\n",
        "                self.mask_channel_selection,\n",
        "                self.mask_channel_other,\n",
        "                no_overlap=self.no_mask_channel_overlap,\n",
        "                min_space=self.mask_channel_min_space,\n",
        "            )\n",
        "            mask_channel_indices = (\n",
        "                torch.from_numpy(mask_channel_indices)\n",
        "                .to(x.device)\n",
        "                .unsqueeze(1)\n",
        "                .expand(-1, T, -1)\n",
        "            )\n",
        "            x[mask_channel_indices] = 0\n",
        "\n",
        "        return x, mask_indices\n",
        "\n",
        "    def forward_padding_mask(\n",
        "            self, features: torch.Tensor, padding_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        extra = padding_mask.size(1) % features.size(1)\n",
        "        if extra > 0:\n",
        "            padding_mask = padding_mask[:, :-extra]\n",
        "        padding_mask = padding_mask.view(\n",
        "            padding_mask.size(0), features.size(1), -1\n",
        "        )\n",
        "        padding_mask = padding_mask.all(-1)\n",
        "        return padding_mask\n",
        "\n",
        "    def extract_features(\n",
        "        self,\n",
        "        source: torch.Tensor,\n",
        "        padding_mask: Optional[torch.Tensor] = None,\n",
        "        mask: bool = False,\n",
        "        ret_conv: bool = False,\n",
        "        output_layer: Optional[int] = None,\n",
        "        ret_layer_results: bool = False,\n",
        "    ):\n",
        "\n",
        "        if self.feature_grad_mult > 0:\n",
        "            features = self.feature_extractor(source)\n",
        "            if self.feature_grad_mult != 1.0:\n",
        "                features = GradMultiply.apply(features, self.feature_grad_mult)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                features = self.feature_extractor(source)\n",
        "\n",
        "        features = features.transpose(1, 2)\n",
        "        features = self.layer_norm(features)\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = self.forward_padding_mask(features, padding_mask)\n",
        "\n",
        "        if self.post_extract_proj is not None:\n",
        "            features = self.post_extract_proj(features)\n",
        "\n",
        "        features = self.dropout_input(features)\n",
        "\n",
        "        if mask:\n",
        "            x, mask_indices = self.apply_mask(\n",
        "                features, padding_mask\n",
        "            )\n",
        "        else:\n",
        "            x = features\n",
        "\n",
        "        # feature: (B, T, D), float\n",
        "        # target: (B, T), long\n",
        "        # x: (B, T, D), float\n",
        "        # padding_mask: (B, T), bool\n",
        "        # mask_indices: (B, T), bool\n",
        "        x, layer_results = self.encoder(\n",
        "            x,\n",
        "            padding_mask=padding_mask,\n",
        "            layer=None if output_layer is None else output_layer - 1\n",
        "        )\n",
        "\n",
        "        res = {\"x\": x, \"padding_mask\": padding_mask, \"features\": features, \"layer_results\": layer_results}\n",
        "\n",
        "        feature = res[\"features\"] if ret_conv else res[\"x\"]\n",
        "        if ret_layer_results:\n",
        "            feature = (feature, res[\"layer_results\"])\n",
        "        return feature, res[\"padding_mask\"]\n",
        "\n",
        "\n",
        "class ConvFeatureExtractionModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            conv_layers: List[Tuple[int, int, int]],\n",
        "            dropout: float = 0.0,\n",
        "            mode: str = \"default\",\n",
        "            conv_bias: bool = False,\n",
        "            conv_type: str = \"default\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert mode in {\"default\", \"layer_norm\"}\n",
        "\n",
        "        def block(\n",
        "                n_in,\n",
        "                n_out,\n",
        "                k,\n",
        "                stride,\n",
        "                is_layer_norm=False,\n",
        "                is_group_norm=False,\n",
        "                conv_bias=False,\n",
        "        ):\n",
        "            def make_conv():\n",
        "                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)\n",
        "                nn.init.kaiming_normal_(conv.weight)\n",
        "                return conv\n",
        "\n",
        "            assert (\n",
        "                           is_layer_norm and is_group_norm\n",
        "                   ) == False, \"layer norm and group norm are exclusive\"\n",
        "\n",
        "            if is_layer_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Sequential(\n",
        "                        TransposeLast(),\n",
        "                        Fp32LayerNorm(dim, elementwise_affine=True),\n",
        "                        TransposeLast(),\n",
        "                    ),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            elif is_group_norm:\n",
        "                return nn.Sequential(\n",
        "                    make_conv(),\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    Fp32GroupNorm(dim, dim, affine=True),\n",
        "                    nn.GELU(),\n",
        "                )\n",
        "            else:\n",
        "                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n",
        "\n",
        "        self.conv_type = conv_type\n",
        "        if self.conv_type == \"default\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    block(\n",
        "                        in_d,\n",
        "                        dim,\n",
        "                        k,\n",
        "                        stride,\n",
        "                        is_layer_norm=mode == \"layer_norm\",\n",
        "                        is_group_norm=mode == \"default\" and i == 0,\n",
        "                        conv_bias=conv_bias,\n",
        "                    )\n",
        "                )\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"conv2d\":\n",
        "            in_d = 1\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride)\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "        elif self.conv_type == \"custom\":\n",
        "            in_d = 1\n",
        "            idim = 80\n",
        "            self.conv_layers = nn.ModuleList()\n",
        "            for i, cl in enumerate(conv_layers):\n",
        "                assert len(cl) == 3\n",
        "                (dim, k, stride) = cl\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.Conv2d(in_d, dim, k, stride, padding=1)\n",
        "                )\n",
        "                self.conv_layers.append(\n",
        "                    torch.nn.LayerNorm([dim, idim])\n",
        "                )\n",
        "                self.conv_layers.append(torch.nn.ReLU())\n",
        "                in_d = dim\n",
        "                if (i + 1) % 2 == 0:\n",
        "                    self.conv_layers.append(\n",
        "                        torch.nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "                    )\n",
        "                    idim = int(math.ceil(idim / 2))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        # BxT -> BxCxT\n",
        "        x = x.unsqueeze(1)\n",
        "        if self.conv_type == \"custom\":\n",
        "            for conv in self.conv_layers:\n",
        "                if isinstance(conv, nn.LayerNorm):\n",
        "                    x = x.transpose(1, 2)\n",
        "                    x = conv(x).transpose(1, 2)\n",
        "                else:\n",
        "                    x = conv(x)\n",
        "            x = x.transpose(2, 3).contiguous()\n",
        "            x = x.view(x.size(0), -1, x.size(-1))\n",
        "        else:\n",
        "            for conv in self.conv_layers:\n",
        "                x = conv(x)\n",
        "            if self.conv_type == \"conv2d\":\n",
        "                b, c, t, f = x.size()\n",
        "                x = x.transpose(2, 3).contiguous().view(b, c * f, t)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.embedding_dim = args.encoder_embed_dim\n",
        "\n",
        "        self.pos_conv = nn.Conv1d(\n",
        "            self.embedding_dim,\n",
        "            self.embedding_dim,\n",
        "            kernel_size=args.conv_pos,\n",
        "            padding=args.conv_pos // 2,\n",
        "            groups=args.conv_pos_groups,\n",
        "        )\n",
        "        dropout = 0\n",
        "        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n",
        "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
        "        nn.init.constant_(self.pos_conv.bias, 0)\n",
        "\n",
        "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
        "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n",
        "\n",
        "        if hasattr(args, \"relative_position_embedding\"):\n",
        "            self.relative_position_embedding = args.relative_position_embedding\n",
        "            self.num_buckets = args.num_buckets\n",
        "            self.max_distance = args.max_distance\n",
        "        else:\n",
        "            self.relative_position_embedding = False\n",
        "            self.num_buckets = 0\n",
        "            self.max_distance = 0\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerSentenceEncoderLayer(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                    has_relative_attention_bias=(self.relative_position_embedding and i == 0),\n",
        "                    num_buckets=self.num_buckets,\n",
        "                    max_distance=self.max_distance,\n",
        "                    gru_rel_pos=args.gru_rel_pos,\n",
        "                )\n",
        "                for i in range(args.encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layer_norm_first = args.layer_norm_first\n",
        "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
        "        self.layerdrop = args.encoder_layerdrop\n",
        "\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "    def forward(self, x, padding_mask=None, streaming_mask=None, layer=None):\n",
        "        x, layer_results = self.extract_features(x, padding_mask, streaming_mask, layer)\n",
        "\n",
        "        if self.layer_norm_first and layer is None:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "    def extract_features(self, x, padding_mask=None, streaming_mask=None, tgt_layer=None):\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            x[padding_mask] = 0\n",
        "\n",
        "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "        x = x + x_conv\n",
        "\n",
        "        if not self.layer_norm_first:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        layer_results = []\n",
        "        z = None\n",
        "        if tgt_layer is not None:\n",
        "            layer_results.append((x, z))\n",
        "        r = None\n",
        "        pos_bias = None\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            dropout_probability = np.random.random()\n",
        "            if not self.training or (dropout_probability > self.layerdrop):\n",
        "                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False,\n",
        "                                       self_attn_mask=streaming_mask, pos_bias=pos_bias)\n",
        "            if tgt_layer is not None:\n",
        "                layer_results.append((x, z))\n",
        "            if i == tgt_layer:\n",
        "                r = x\n",
        "                break\n",
        "\n",
        "        if r is not None:\n",
        "            x = r\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        return x, layer_results\n",
        "\n",
        "\n",
        "class TransformerSentenceEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = \"relu\",\n",
        "            layer_norm_first: bool = False,\n",
        "            has_relative_attention_bias: bool = False,\n",
        "            num_buckets: int = 0,\n",
        "            max_distance: int = 0,\n",
        "            rescale_init: bool = False,\n",
        "            gru_rel_pos: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        # Initialize parameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        # Initialize blocks\n",
        "        self.activation_name = activation_fn\n",
        "        self.activation_fn = get_activation_fn(activation_fn)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=True,\n",
        "            has_relative_attention_bias=has_relative_attention_bias,\n",
        "            num_buckets=num_buckets,\n",
        "            max_distance=max_distance,\n",
        "            rescale_init=rescale_init,\n",
        "            gru_rel_pos=gru_rel_pos,\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm_first = layer_norm_first\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "        if self.activation_name == \"glu\":\n",
        "            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            self_attn_mask: torch.Tensor = None,\n",
        "            self_attn_padding_mask: torch.Tensor = None,\n",
        "            need_weights: bool = False,\n",
        "            pos_bias=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        LayerNorm is applied either before or after the self-attention/ffn\n",
        "        modules similar to the original Transformer imlementation.\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        if self.layer_norm_first:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=False,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            residual = x\n",
        "            x = self.final_layer_norm(x)\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "        else:\n",
        "            x, attn, pos_bias = self.self_attn(\n",
        "                query=x,\n",
        "                key=x,\n",
        "                value=x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=self_attn_mask,\n",
        "                position_bias=pos_bias\n",
        "            )\n",
        "\n",
        "            x = self.dropout1(x)\n",
        "            x = residual + x\n",
        "\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "            residual = x\n",
        "            if self.activation_name == \"glu\":\n",
        "                x = self.fc1(x)\n",
        "            else:\n",
        "                x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = residual + x\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        return x, attn, pos_bias\n"
      ],
      "metadata": {
        "id": "I3BR1_cSGaZ6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTk_rGDiPHB"
      },
      "source": [
        "#lightning_module.py + wavlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jK6AJpMZxaOC"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# WandB 프로젝트 초기화\n",
        "wandb_logger = WandbLogger(project=\"avocodo_train\", name=\"avocodo_training\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.transforms import Resample\n",
        "from pytorch_lightning import LightningModule\n",
        "from omegaconf import OmegaConf\n",
        "import torchaudio\n",
        "\n",
        "class Avocodo_WavLM(LightningModule):\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(h)\n",
        "\n",
        "        # WavLM 설정\n",
        "        wavlm_cfg_dict = OmegaConf.load(h.wavlm_config)\n",
        "        wavlm_cfg = WavLMConfig(wavlm_cfg_dict)\n",
        "        self.wavlm = WavLM(wavlm_cfg)\n",
        "\n",
        "        # WavLM 가중치 로드\n",
        "        checkpoint = torch.load(h.wavlm_checkpoint)\n",
        "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "        self.wavlm.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        # WavLM 가중치 동결\n",
        "        self.freeze_module(self.wavlm)\n",
        "\n",
        "        # Resample (22050Hz -> 16000Hz)\n",
        "        self.resample = Resample(orig_freq=22050, new_freq=16000)\n",
        "\n",
        "        # Model components\n",
        "        self.pqmf_lv2 = PQMF(*self.hparams.pqmf_config[\"lv2\"])\n",
        "        self.pqmf_lv1 = PQMF(*self.hparams.pqmf_config[\"lv1\"])\n",
        "\n",
        "        self.generator = Generator(self.hparams.generator)\n",
        "        self.combd = CoMBD(self.hparams.combd, [self.pqmf_lv2, self.pqmf_lv1])\n",
        "        self.sbd = SBD(self.hparams.sbd)\n",
        "\n",
        "        # Validation outputs storage\n",
        "        self.validation_outputs = []\n",
        "\n",
        "        # Manual optimization\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def freeze_module(self, module):\n",
        "        \"\"\"\n",
        "        특정 모듈의 가중치를 동결합니다.\n",
        "        \"\"\"\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)[-1]\n",
        "\n",
        "    def extract_layer_features(self, wav_input, layer=6):\n",
        "        \"\"\"\n",
        "        WavLM을 사용하여 특정 레이어의 특징값 추출.\n",
        "        \"\"\"\n",
        "        # WavLM에서 특징값 추출\n",
        "        rep, layer_results = self.wavlm.extract_features(\n",
        "            source=wav_input,\n",
        "            output_layer=6,\n",
        "            ret_layer_results=True\n",
        "        )\n",
        "\n",
        "        layer_tensor = rep[0]  # 튜플의 첫 번째 요소 추출\n",
        "        if isinstance(layer_tensor, torch.Tensor):\n",
        "            layer_features = layer_tensor.transpose(0, 1)  # (batch, time, feature_dim)\n",
        "            print(f\"Extracted layer {layer} features shape:\", layer_features.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Expected a Tensor but got: {type(layer_tensor)}\")\n",
        "        # Shape 디버깅용 출력\n",
        "        #print(f\"Extracted layer {layer} features shape: {layer_features.shape}\")\n",
        "\n",
        "        return layer_features\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        h = self.hparams.optimizer\n",
        "        opt_g = torch.optim.AdamW(\n",
        "            self.generator.parameters(),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        opt_d = torch.optim.AdamW(\n",
        "            itertools.chain(self.combd.parameters(), self.sbd.parameters()),\n",
        "            lr=h.learning_rate,\n",
        "            betas=(h.adam_b1, h.adam_b2)\n",
        "        )\n",
        "        return [opt_g, opt_d]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch  # x: 원본 wav, y: 타겟 wav\n",
        "\n",
        "        y_g_hat = self.generator(x)\n",
        "        y_g_hat2=self(x)\n",
        "        # Save generated wav for comparison\n",
        "        generated_wav = y_g_hat[-1] if isinstance(y_g_hat, list) else y_g_hat\n",
        "        generated_wav = generated_wav.squeeze(1)  # [batch, time]\n",
        "\n",
        "        # Normalize and resample inputs for WavLM\n",
        "        wav_input1_16khz = self.resample(y).squeeze(1)  # 원본 wav 리샘플링\n",
        "        wav_input2_16khz = self.resample(generated_wav)  # 생성된 wav 리샘플링\n",
        "\n",
        "        # Extract features from 6th layer\n",
        "        original_features = self.extract_layer_features(wav_input1_16khz, layer=6)\n",
        "        generated_features = self.extract_layer_features(wav_input2_16khz, layer=6)\n",
        "\n",
        "        # Calculate L1 Loss for WavLM features\n",
        "        wavlm_feature_loss = F.l1_loss(original_features, generated_features)\n",
        "\n",
        "        self.log(\"train/wavlm_feature_loss\", wavlm_feature_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Continue with existing training logic\n",
        "        opt_g, opt_d = self.optimizers()\n",
        "\n",
        "        # Train Generator\n",
        "        opt_g.zero_grad()\n",
        "        loss_feature_matching = self.calculate_feature_loss(x, y, layer=6)\n",
        "        self.log(\"train/loss_feature_matching\", loss_feature_matching, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            generated_wav,\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        loss_mel = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "        self.log(\"train/loss_mel\", loss_mel, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        g_loss = loss_feature_matching + loss_mel + wavlm_feature_loss\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_d.zero_grad()\n",
        "        detached_y_g_hats = [x.detach() for x in y_g_hats]\n",
        "\n",
        "        y_du_hat_r, y_du_hat_g, _, _ = self.combd([y], detached_y_g_hats)\n",
        "        loss_disc_u, _, _ = discriminator_loss(y_du_hat_r, y_du_hat_g)\n",
        "\n",
        "        y_ds_hat_r, y_ds_hat_g, _, _ = self.sbd(y, detached_y_g_hats[-1])\n",
        "        loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n",
        "\n",
        "        d_loss = loss_disc_s + loss_disc_u\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "        self.log(\"train/g_loss\", g_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"train/d_loss\", d_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _, y_mel = batch  # x: 원본 wav, y: 타겟 wav\n",
        "        #print(\"x (원본 멜스펙트로그램) 모양:\", x.shape)       # 원본 wav\n",
        "        #print(\"y (타겟 wav) 모양:\", y.shape)      # 타겟 wav\n",
        "        #print(\"_ (사용되지 않음) 값:\", _)          # 사용되지 않는 값\n",
        "        #print(\"y_mel (멜 스펙트로그램) 모양:\", y_mel.shape)\n",
        "        y = y.unsqueeze(1)  # 타겟 wav에 채널 추가\n",
        "        y_g_hat = self.generator(x)\n",
        "        y_g_hat2=self(x)\n",
        "        # Save generated wav for comparison\n",
        "        generated_wav = y_g_hat[-1] if isinstance(y_g_hat, list) else y_g_hat\n",
        "        generated_wav = generated_wav.squeeze(1)  # [batch, time]\n",
        "\n",
        "        # Normalize and resample inputs for WavLM\n",
        "        wav_input1_16khz = self.resample(y).squeeze(1)  # 원본 wav 리샘플링\n",
        "        wav_input2_16khz = self.resample(generated_wav)  # 생성된 wav 리샘플링\n",
        "\n",
        "        # Extract features from 6th layer\n",
        "        original_features = self.extract_layer_features(wav_input1_16khz, layer=6)\n",
        "        generated_features = self.extract_layer_features(wav_input2_16khz, layer=6)\n",
        "\n",
        "        # Calculate L1 Loss for WavLM features\n",
        "        wavlm_feature_loss = F.l1_loss(original_features, generated_features)\n",
        "        self.log(\"validation/wavlm_feature_loss\", wavlm_feature_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Mel-spectrogram loss\n",
        "        y_g_hat_mel = mel_spectrogram(\n",
        "            y_g_hat2.squeeze(1),\n",
        "            self.hparams.audio.n_fft,\n",
        "            self.hparams.audio.num_mels,\n",
        "            self.hparams.audio.sampling_rate,\n",
        "            self.hparams.audio.hop_size,\n",
        "            self.hparams.audio.win_size,\n",
        "            self.hparams.audio.fmin,\n",
        "            self.hparams.audio.fmax_for_loss\n",
        "        )\n",
        "        val_loss = F.l1_loss(y_mel, y_g_hat_mel)\n",
        "        self.validation_outputs.append(val_loss)\n",
        "\n",
        "        # Log validation loss\n",
        "        self.log(\"validation/loss_mel\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.validation_outputs:\n",
        "            avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
        "            self.log(\"validation/avg_loss\", avg_val_loss, prog_bar=True, logger=True)\n",
        "        self.validation_outputs.clear()\n"
      ],
      "metadata": {
        "id": "iexVoIUOyEqi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VohGwH8g_Dov"
      },
      "source": [
        "#train.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OHLRL85QEBkW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TBLogger(TensorBoardLogger):\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        metrics.pop('epoch', None)\n",
        "        return super().log_metrics(metrics, step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NY4uFuC9EEZY"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser() #세팅\n",
        "\n",
        "parser.add_argument('--group_name', default=None)\n",
        "parser.add_argument('--input_wavs_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs')\n",
        "parser.add_argument('--input_mels_dir', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram')\n",
        "parser.add_argument('--input_training_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt')\n",
        "parser.add_argument('--input_validation_file',\n",
        "                    default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt')\n",
        "parser.add_argument('--config', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json')\n",
        "parser.add_argument('--training_epochs', default=100, type=int)\n",
        "parser.add_argument('--fine_tuning', default=False, type=bool)\n",
        "parser.add_argument('--wavlm_config_pretrained',default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt')\n",
        "\n",
        "\n",
        "\n",
        "if \"ipykernel_launcher\" in sys.argv[0]:\n",
        "    sys.argv = [\n",
        "        'script_name',\n",
        "        '--group_name', 'default_group',\n",
        "        '--input_wavs_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs',\n",
        "        '--input_mels_dir', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/mel_spectrogram',\n",
        "        '--input_training_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/training.txt',\n",
        "        '--input_validation_file', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset_setting/validation.txt',\n",
        "        '--config', '/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/config/avocodo_v1_wavlm.json',\n",
        "        '--training_epochs', '100',\n",
        "        '--fine_tuning', 'False',\n",
        "        '--wavlm_config_pretrained','/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/WavLM-Base.pt'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu96Tbu2Twyi",
        "outputId": "86349378-bdd4-474b-f585-4506f4569e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([1, 80, 87])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import librosa\n",
        "\n",
        "# 샘플 Mel-spectrogram 생성 함수\n",
        "def generate_mel_spectrogram(sample_rate=22050, n_fft=1024, hop_length=256, n_mels=80, duration=1.0):\n",
        "    # 가상 오디오 데이터 생성 (White Noise)\n",
        "    raw_audio = torch.randn(int(sample_rate * duration))  # 1초 길이의 오디오\n",
        "\n",
        "    # Mel-spectrogram 변환\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(\n",
        "        y=raw_audio.numpy(),\n",
        "        sr=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    mel_tensor = torch.tensor(mel_spectrogram).unsqueeze(0)  # 배치 차원 추가\n",
        "    return mel_tensor\n",
        "\n",
        "# 예시 데이터 준비\n",
        "mel_input = generate_mel_spectrogram(duration=1.0)  # 1초 길이의 Mel-spectrogram\n",
        "mel_input = mel_input.unsqueeze(0)  # 배치 차원 추가 (1, 80, Frames)\n",
        "mel_input = mel_input.squeeze(1)\n",
        "print(f\"Input Shape: {mel_input.shape}\")  # (1, 80, Frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nJuJSVJR_DRA"
      },
      "outputs": [],
      "source": [
        "# Parse Arguments\n",
        "a, unknown = parser.parse_known_args()\n",
        "\n",
        "# OmegaConf 설정\n",
        "OmegaConf.register_new_resolver(\"from_args\", lambda x: getattr(a, x))\n",
        "OmegaConf.register_new_resolver(\"dir\", lambda base_dir, string: os.path.join(base_dir, string))\n",
        "conf = OmegaConf.load(a.config)\n",
        "OmegaConf.resolve(conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQYocu51VK1C",
        "outputId": "9318ce20-53d7-4b43-ebbe-6570ce42c972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "<ipython-input-35-7475c4988d61>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(h.wavlm_checkpoint)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 데이터 및 모델 초기화\n",
        "dm = AvocodoData(conf.data)\n",
        "model = Avocodo_WavLM(conf.model)\n",
        "\n",
        "# 모델 요약 출력\n",
        "summary(\n",
        "    model,\n",
        "    input_data=mel_input,  # 입력 데이터 전달\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\"],  # 표시할 열 선택\n",
        "    col_width=20,\n",
        "    depth=5  # 레이어 깊이 제한\n",
        ")\n",
        "\n",
        "limit_train_batches = 1.0\n",
        "limit_val_batches = 1.0\n",
        "log_every_n_steps = 50\n",
        "max_epochs = conf.model.train.training_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1hmp4fZrEasS"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',               # Validation 손실 기준\n",
        "    dirpath='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint_wavlm',  # 체크포인트 저장 디렉토리\n",
        "    filename='best-checkpoint-{epoch:02d}-{val_loss:.2f}',  # 파일 이름 패턴\n",
        "    save_top_k=1,                     # 가장 좋은 k개의 모델만 저장\n",
        "    mode='min',                       # 손실 기준으로 최소값 저장\n",
        ")\n",
        "\n",
        "# 조기 종료 콜백: Validation 손실 개선이 없으면 중단\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',              # Validation 손실 기준\n",
        "    patience=10000000,                     # 몇 에포크 동안 개선 없으면 중단\n",
        "    verbose=True,                    # 중단 시 메시지 출력\n",
        "    mode='min'                       # 손실 기준으로 최소값 기준\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUWFRnb9EJ33",
        "outputId": "8b6a6225-32d6-479f-fe0e-5c7f7a9cfea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=\"auto\",\n",
        "    max_epochs=max_epochs,\n",
        "    callbacks=[\n",
        "        checkpoint_callback,\n",
        "        RichProgressBar(\n",
        "            refresh_rate=1,\n",
        "            theme=RichProgressBarTheme(\n",
        "                description=\"#AF81EB\",\n",
        "                progress_bar=\"#8BE9FE\",\n",
        "                progress_bar_finished=\"#8BE9FE\",\n",
        "                progress_bar_pulse=\"#1363DF\",\n",
        "                batch_progress=\"#AF81EB\",\n",
        "                time=\"#1363DF\",\n",
        "                processing_speed=\"#1363DF\",\n",
        "                metrics=\"#9BF9FE\",\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    # logger=TensorBoardLogger(\"logs\", name=\"Avocodo\"),\n",
        "    logger=wandb_logger,\n",
        "    limit_train_batches=limit_train_batches,\n",
        "    limit_val_batches=limit_val_batches,\n",
        "    log_every_n_steps=log_every_n_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#학습 본편\n"
      ],
      "metadata": {
        "id": "Ij4qLnLHSO6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aq4FBO9OArgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88e1bfae2f88457f95230f26371ad31b",
            "41e54829bc714db381d4746323945579"
          ]
        },
        "outputId": "ff525041-582b-4e5e-bb90-833a848f6d53"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20250104_164109-m1a1udm5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train/runs/m1a1udm5' target=\"_blank\">avocodo_training</a></strong> to <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train/runs/m1a1udm5' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/avocodo_train/runs/m1a1udm5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ wavlm     │ WavLM     │ 94.4 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ resample  │ Resample  │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ pqmf_lv2  │ PQMF      │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ pqmf_lv1  │ PQMF      │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ generator │ Generator │ 13.9 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ combd     │ CoMBD     │ 16.5 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ sbd       │ SBD       │ 10.6 M │ train │\n",
              "└───┴───────────┴───────────┴────────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ wavlm     │ WavLM     │ 94.4 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ resample  │ Resample  │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ pqmf_lv2  │ PQMF      │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ pqmf_lv1  │ PQMF      │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ generator │ Generator │ 13.9 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ combd     │ CoMBD     │ 16.5 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ sbd       │ SBD       │ 10.6 M │ train │\n",
              "└───┴───────────┴───────────┴────────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 41.0 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 94.4 M                                                                                       \n",
              "\u001b[1mTotal params\u001b[0m: 135 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 541                                                                        \n",
              "\u001b[1mModules in train mode\u001b[0m: 532                                                                                         \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 41.0 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 94.4 M                                                                                       \n",
              "<span style=\"font-weight: bold\">Total params</span>: 135 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 541                                                                        \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 532                                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88e1bfae2f88457f95230f26371ad31b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([327, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([327, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([327, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([327, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([327, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([327, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([327, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([327, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` \n",
              "from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., \n",
              "batch_size=batch_size)`.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` \n",
              "from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., \n",
              "batch_size=batch_size)`.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([187, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([187, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([187, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([187, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([18, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([18, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracted layer 6 features shape: torch.Size([18, 1, 768])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted layer 6 features shape: torch.Size([18, 1, 768])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<ipython-input-35-7475c4988d61>:180: UserWarning: Using a target size (torch.Size([18, 1, 768])) that is different \n",
              "to the input size (torch.Size([187, 1, 768])). This will likely lead to incorrect results due to broadcasting. \n",
              "Please ensure they have the same size.\n",
              "  wavlm_feature_loss = F.l1_loss(original_features, generated_features)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;ipython-input-35-7475c4988d61&gt;:180: UserWarning: Using a target size (torch.Size([18, 1, 768])) that is different \n",
              "to the input size (torch.Size([187, 1, 768])). This will likely lead to incorrect results due to broadcasting. \n",
              "Please ensure they have the same size.\n",
              "  wavlm_feature_loss = F.l1_loss(original_features, generated_features)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (187) must match the size of tensor b (18) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-d47d1b932123>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         )\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7475c4988d61>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Calculate L1 Loss for WavLM features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mwavlm_feature_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation/wavlm_feature_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavlm_feature_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3753\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3754\u001b[0m     return torch._C._nn.l1_loss(\n\u001b[1;32m   3755\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (187) must match the size of tensor b (18) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32FgGv8ihNR"
      },
      "source": [
        "#inference.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6baN8L8CmXVM"
      },
      "outputs": [],
      "source": [
        "\n",
        "h = None\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def get_mel(x):\n",
        "    return mel_spectrogram(\n",
        "        x,\n",
        "        1024,\n",
        "        80,\n",
        "        22050,\n",
        "        256,\n",
        "        1024,\n",
        "        0,\n",
        "        8000\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6rbIWAomjLc"
      },
      "outputs": [],
      "source": [
        "def inference(a, conf):\n",
        "    \"\"\"전체 데이터셋에 대한 추론 실행\"\"\"\n",
        "\n",
        "    # 모델 로드\n",
        "    checkpoint_full_path = os.path.join(a.checkpoint_path, a.checkpoint_file_id)\n",
        "    avocodo = Avocodo.load_from_checkpoint(\n",
        "        checkpoint_full_path,\n",
        "        map_location=torch.device(a.device)  # GPU 설정\n",
        "    )\n",
        "\n",
        "    # 모델 준비\n",
        "    avocodo.to(a.device)\n",
        "    avocodo.generator.to(a.device)\n",
        "    avocodo.generator.remove_weight_norm()\n",
        "\n",
        "    # 데이터 준비\n",
        "    avocodo_data = AvocodoData(conf.audio)\n",
        "    avocodo_data.prepare_data()\n",
        "    avocodo_data.setup()\n",
        "    full_dataloader = avocodo_data.full_dataloader()\n",
        "\n",
        "    # 결과 저장 경로\n",
        "    output_path = os.path.join(a.output_dir, f\"version_{a.version}\")\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # TorchScript 저장\n",
        "    scripted_model = torch.jit.script(avocodo.generator)\n",
        "    torch.jit.save(scripted_model, os.path.join(output_path, \"scripted.pt\"))\n",
        "\n",
        "    # 추론 실행\n",
        "    with torch.no_grad():\n",
        "        for batch in full_dataloader:\n",
        "            mels, _, file_ids, _ = batch\n",
        "\n",
        "            # 데이터를 GPU로 이동\n",
        "            mels = mels.to(a.device)\n",
        "\n",
        "            # 모델로 음성 생성\n",
        "            y_g_hat = avocodo(mels)\n",
        "\n",
        "            for _y_g_hat, file_id in zip(y_g_hat, file_ids):\n",
        "                # 생성된 오디오 후처리\n",
        "                audio = _y_g_hat.squeeze(0)\n",
        "                audio = (audio * MAX_WAV_VALUE).cpu().numpy().astype('int16')\n",
        "\n",
        "                # 파일 저장\n",
        "                output_file = os.path.join(output_path, file_id.split('/')[-1])\n",
        "                os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "                write(output_file, conf.audio.sampling_rate, audio)\n",
        "                print(f\"Saved: {output_file}\")\n",
        "\n",
        "    print(\"Inference completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69y4YDJZiilp"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    # Argument parsing\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--checkpoint_path', default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/checkpoint')\n",
        "    parser.add_argument('--version', type=int,default=1, required=True)\n",
        "    parser.add_argument('--checkpoint_file_id', type=str, default=\"best-checkpoint-epoch=80-validation/avg_loss=0.27.ckpt\", required=True)\n",
        "    parser.add_argument('--output_dir', type=str, default='/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result_all')\n",
        "    parser.add_argument('--script', type=bool, default=True)\n",
        "    parser.add_argument('--device', type=str, default='cuda')  # 디바이스 기본값은 GPU\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    # 체크포인트에서 하이퍼파라미터 로드\n",
        "    checkpoint_full_path = os.path.join(a.checkpoint_path, a.checkpoint_file_id)\n",
        "    checkpoint = torch.load(checkpoint_full_path, map_location=torch.device('cpu'))  # GPU는 나중에 이동\n",
        "\n",
        "    if \"hyper_parameters\" in checkpoint:\n",
        "        conf = checkpoint[\"hyper_parameters\"]\n",
        "    else:\n",
        "        # hparams.yaml 파일 로드\n",
        "        yaml_path = os.path.join(a.checkpoint_path, f\"version_{a.version}\", \"hparams.yaml\")\n",
        "        if os.path.exists(yaml_path):\n",
        "            conf = OmegaConf.load(yaml_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(\"hparams.yaml not found and no hparams in the checkpoint.\")\n",
        "\n",
        "    # 추론 실행\n",
        "    inference(a, conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn0baVSmmlsz"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    import sys\n",
        "    sys.argv = [\n",
        "        \"colab_kernel_launcher.py\",\n",
        "        \"--version\", \"2\",\n",
        "        \"--checkpoint_file_id\", \"best-checkpoint-epoch=80-validation/avg_loss=0.27.ckpt\",\n",
        "        \"--output_dir\", \"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result\",\n",
        "        \"--device\", \"cuda\"\n",
        "    ]\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlflVpgd2Vr5"
      },
      "source": [
        "#원본, 생성된 데이터 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWnbWZlt3cUk"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import librosa.display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMD0dAu56j_n"
      },
      "outputs": [],
      "source": [
        "# # 멜 스펙트로그램 계산 함수\n",
        "# def compute_mel_spectrogram(file_path, sr=22050, n_fft=1024, hop_length=512, n_mels=128):\n",
        "#     y, sr = librosa.load(file_path, sr=sr)\n",
        "#     mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "#     return mel_spec_db\n",
        "\n",
        "# # 멜 스펙트로그램 시각화 함수\n",
        "# def plot_mel_spectrogram(mel_spectrogram, title=\"Mel-Spectrogram\", cmap=\"inferno\", figsize=(10, 4), db_min=-40, db_max=40):\n",
        "\n",
        "#     mel_spectrogram_db = np.clip(mel_spectrogram, db_min, db_max)  # dB 범위 제한\n",
        "\n",
        "#     plt.figure(figsize=figsize)\n",
        "#     plt.imshow(mel_spectrogram_db.T, aspect='auto', origin='lower', interpolation='none', cmap=cmap)\n",
        "#     plt.colorbar(format=\"%+2.0f dB\")\n",
        "#     plt.title(title)\n",
        "#     plt.xlabel(\"Time\")\n",
        "#     plt.ylabel(\"Mel Frequency\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # 원본 및 생성된 파일 비교 시각화\n",
        "# def compare_mel_spectrograms(origin_file, result_file):\n",
        "#     origin_mel = compute_mel_spectrogram(origin_file)\n",
        "#     result_mel = compute_mel_spectrogram(result_file)\n",
        "\n",
        "#     # 원본 멜 스펙트로그램 시각화\n",
        "#     plot_mel_spectrogram(origin_mel, title=\"Original Mel-Spectrogram\")\n",
        "\n",
        "#     # 생성된 멜 스펙트로그램 시각화\n",
        "#     #plot_mel_spectrogram(result_mel, title=\"Generated Mel-Spectrogram\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7-v699MxyXA"
      },
      "outputs": [],
      "source": [
        "# result_path = \"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/result/version_1\"\n",
        "# origin_path = \"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs\"\n",
        "# common_files = set(os.listdir(result_path)) & set(os.listdir(origin_path))\n",
        "\n",
        "# # 공통 파일 중 하나 비교\n",
        "# file_names = list(common_files)[:2]  # 첫 번째 공통 파일\n",
        "# for file_name in file_names:\n",
        "#   origin_file = os.path.join(origin_path, file_name)\n",
        "#   result_file = os.path.join(result_path, file_name)\n",
        "\n",
        "#   print(f\"Comparing {file_name}...\")\n",
        "#   compare_mel_spectrograms(origin_file, result_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQE58VYQ72hb"
      },
      "outputs": [],
      "source": [
        "# import librosa\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# def compute_mel_spectrogram(file_path, sr=22050, n_fft=2048, hop_length=512, n_mels=128):\n",
        "#     \"\"\"\n",
        "#     Compute the mel-spectrogram for a given WAV file.\n",
        "\n",
        "#     Args:\n",
        "#         file_path (str): Path to the WAV file.\n",
        "#         sr (int): Sampling rate for the audio.\n",
        "#         n_fft (int): Number of FFT components.\n",
        "#         hop_length (int): Hop length for STFT.\n",
        "#         n_mels (int): Number of Mel bands.\n",
        "\n",
        "#     Returns:\n",
        "#         np.ndarray: Mel-spectrogram in decibel scale.\n",
        "#     \"\"\"\n",
        "#     # Load the WAV file\n",
        "#     y, sr = librosa.load(file_path, sr=sr)\n",
        "#     print(f\"Loaded file: {file_path}\")\n",
        "#     print(f\"Audio signal shape: {y.shape}, Sampling rate: {sr}\")\n",
        "\n",
        "#     # Compute Mel-Spectrogram\n",
        "#     mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "\n",
        "#     # Convert to dB scale\n",
        "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "#     print(f\"Mel-Spectrogram shape: {mel_spec_db.shape}\")\n",
        "\n",
        "#     return mel_spec_db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coSmxdz-8JKc"
      },
      "outputs": [],
      "source": [
        "# file_path = \"/content/drive/MyDrive/코딩공부/deep_daiv/daiv_fall/Avocodo/dataset/LJSpeech-1.1/wavs/LJ001-0001.wav\"  # WAV 파일 경로\n",
        "# mel_spectrogram = compute_mel_spectrogram(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XUlBEin8OLr"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import librosa.display\n",
        "\n",
        "# def plot_mel_spectrogram(mel_spectrogram, sr=22050, hop_length=512, title=\"Mel-Spectrogram\"):\n",
        "#     \"\"\"\n",
        "#     Plot the given mel-spectrogram.\n",
        "\n",
        "#     Args:\n",
        "#         mel_spectrogram (np.ndarray): Mel-spectrogram in decibel scale.\n",
        "#         sr (int): Sampling rate of the audio.\n",
        "#         hop_length (int): Hop length for STFT.\n",
        "#         title (str): Title for the plot.\n",
        "#     \"\"\"\n",
        "#     plt.figure(figsize=(10, 4))\n",
        "#     librosa.display.specshow(mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='inferno')\n",
        "#     plt.colorbar(format=\"%+2.0f dB\")\n",
        "#     plt.title(title)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # 시각화 실행\n",
        "# plot_mel_spectrogram(mel_spectrogram)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LuKV0hhkiCFp",
        "9owc_vcWhiAC",
        "98DxXGMdhmT5",
        "MfcMzi-hiajZ",
        "zSQ3Q0mqht0K",
        "NPeFVq4_hzq6",
        "4ttjqPrPh4sC",
        "BUyxi8fxiHbD",
        "_32FgGv8ihNR",
        "RlflVpgd2Vr5"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMHo3GfVw9S1ia76hidlCV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88e1bfae2f88457f95230f26371ad31b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_41e54829bc714db381d4746323945579",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;175;129;235mSanity Checking\u001b[0m \u001b[38;2;139;233;254m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[38;2;175;129;235m1/2\u001b[0m \u001b[38;2;19;99;223m0:00:05 • -:--:--\u001b[0m \u001b[38;2;19;99;223m0.00it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #af81eb; text-decoration-color: #af81eb\">Sanity Checking</span> <span style=\"color: #8be9fe; text-decoration-color: #8be9fe\">━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #af81eb; text-decoration-color: #af81eb\">1/2</span> <span style=\"color: #1363df; text-decoration-color: #1363df\">0:00:05 • -:--:--</span> <span style=\"color: #1363df; text-decoration-color: #1363df\">0.00it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "41e54829bc714db381d4746323945579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}